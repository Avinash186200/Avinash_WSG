{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e050b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "Trailing Stop-Loss (CAPPED) — OOS Full-Year Simulation (Step8 + Step9) — Next-Day Only\n",
    "================================================================================\n",
    "\n",
    "PURPOSE\n",
    "-------\n",
    "This script evaluates a trailing stop-loss overlay on existing trade records using\n",
    "15-minute resampled bars built from raw M5 data.\n",
    "\n",
    "Key design:\n",
    "- Trades are assumed to be entered at/near end-of-day (EOD).\n",
    "- An INITIAL stop-loss is placed at the entry close (>= 1% distance by rule).\n",
    "- Stop monitoring / trailing begins from the NEXT trading day only (no same-day trailing).\n",
    "\n",
    "The output compares:\n",
    "- Original exit (as per trade file)\n",
    "vs\n",
    "- Exit produced by Initial SL / Trailing SL logic\n",
    "\n",
    "and reports both trade-level results and summary metrics.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "INPUTS\n",
    "------\n",
    "1) TRADE_CSV_PATH\n",
    "   Trade-level data with columns (case-sensitive as used in code):\n",
    "     - TradeID\n",
    "     - Currency\n",
    "     - Direction\n",
    "     - date\n",
    "     - price\n",
    "     - Opening Date\n",
    "     - Closing Date\n",
    "   Notes:\n",
    "   - Script filters trades to YEAR_FILTER based on the 'date' column year.\n",
    "   - TradeID may have multiple rows; script aggregates to 1 row per TradeID.\n",
    "\n",
    "2) M5 raw price data files (CSV)\n",
    "   Each ticker file must contain columns:\n",
    "     - date, open, high, low, close\n",
    "   The script:\n",
    "     - parses 'date' to datetime\n",
    "     - filters market hours (MARKET_START to MARKET_END)\n",
    "     - resamples to 15-minute OHLC (RESAMPLE_RULE)\n",
    "     - computes ATR(ATR_PERIOD) on resampled bars\n",
    "\n",
    "3) M5 file list (M5_FILE_LIST_PATH)\n",
    "   CSV with columns:\n",
    "     - ticker, filename\n",
    "   Used to locate each ticker's raw M5 data file under M5_DIR.\n",
    "\n",
    "4) Instrument mapping (MAPPING_XLSX_PATH)\n",
    "   Excel with columns:\n",
    "     - org_symbol  (ticker name used in M5 files)\n",
    "     - Symbol      (trade symbol / currency, e.g., \"EUR/USD\")\n",
    "   Mapping logic:\n",
    "     - TradeBase = base symbol from Currency/Symbol (before \"/\")\n",
    "     - TickerNorm = mapped to M5 ticker via org_symbol\n",
    "\n",
    "5) Step8 / Step9 best configs\n",
    "   - best_config_step8.csv (expects first row keys):\n",
    "       ATAN_LONG, ATAN_SHORT, MAX_STOP_LONG, MAX_STOP_SHORT\n",
    "   - best_config_step9.csv (expects first row keys, defaults to 1.0 if missing):\n",
    "       SENS_LONG, SENS_SHORT\n",
    "   These are merged into one config dict per run.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "CORE LOGIC (HIGH LEVEL)\n",
    "----------------------\n",
    "A) Trade aggregation\n",
    "   For each TradeID (within YEAR_FILTER):\n",
    "   - EntryDate  = first non-null Opening Date\n",
    "   - ExitDate   = first non-null Closing Date\n",
    "   - EntryPrice = last available 'price' row on EntryDate\n",
    "   - ExitPrice  = last available 'price' row on ExitDate\n",
    "   Eligibility filters:\n",
    "   - Must have mapped ticker available in M5 file list\n",
    "   - Direction must be LONG or SHORT\n",
    "   - Entry/Exit dates and prices must exist\n",
    "   - ExitDate must be strictly after EntryDate (at least 1-day window)\n",
    "\n",
    "B) Price series preparation per ticker (cached)\n",
    "   For each eligible ticker:\n",
    "   - load raw M5 CSV\n",
    "   - filter to market hours [MARKET_START, MARKET_END]\n",
    "   - resample to 15-min OHLC\n",
    "   - compute ATR(14)\n",
    "   The resampled series is cached so multiple trades reuse it.\n",
    "\n",
    "C) Initial Stop-Loss (placed at entry close)\n",
    "   - Initial stop distance percentage must be >= INITIAL_MIN_STOP_PCT (>= 1%).\n",
    "   - Attempt priority:\n",
    "       1) adaptive_stop_distance_pct() computed from historical bars up to entry close\n",
    "       2) ATR% * ATR_FALLBACK_MULT\n",
    "       3) INITIAL_MIN_STOP_PCT\n",
    "   - Always capped by MAX_STOP_LONG/MAX_STOP_SHORT from Step8.\n",
    "\n",
    "D) Trailing Stop-Loss (NEXT DAY ONLY)\n",
    "   - Monitoring starts at: EntryDate + 1 day @ MARKET_START\n",
    "   - For each 15-min bar in the monitoring window:\n",
    "       1) Check stop hit FIRST:\n",
    "          * If TRIGGER_ON_CLOSE=True -> stop is evaluated on bar close\n",
    "          * Else -> evaluated on low (long) / high (short)\n",
    "          If hit before any trailing update -> ExitReason=\"InitialSL\"\n",
    "          If hit after trailing updates -> ExitReason=\"TrailingSL\"\n",
    "       2) Tighten stop using adaptive_stop_distance_pct():\n",
    "          - Uses last REG_WINDOW closes (quadratic fit) to detect slope regime.\n",
    "          - If slope regime strong: uses slope-based stop distance (scaled by sensitivity).\n",
    "          - Else uses ATR fallback (ATR% * ATR_FALLBACK_MULT).\n",
    "          - Trailing stop minimum distance is MIN_STOP_PCT (can be tighter than 1%).\n",
    "   - If no stop hit, exit remains the original exit (\"OriginalExit\").\n",
    "\n",
    "E) Outputs\n",
    "   - trades_full_{YEAR_FILTER}.csv\n",
    "       Trade-by-trade results: initial stop, exit reason, trailing exit, improvement metrics\n",
    "   - summary_full_{YEAR_FILTER}.csv\n",
    "       Aggregated metrics for ALL trades and by Direction:\n",
    "         Trades, SLTriggered, PctTriggered, Avg impact, etc.\n",
    "   - debug_reasons_full_{YEAR_FILTER}.csv + reason_counts_full_{YEAR_FILTER}.csv\n",
    "       Diagnostics / exit reason counts\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "IMPORTANT SETTINGS / ASSUMPTIONS\n",
    "--------------------------------\n",
    "- MARKET_START / MARKET_END define the intraday window used for resampling and simulation.\n",
    "- RESAMPLE_RULE is set to 15T (15-minute candles).\n",
    "- APPLY_FROM_NEXT_DAY_ONLY=True means there is NO same-day monitoring or trailing.\n",
    "- TRIGGER_ON_CLOSE=True means stop triggers on 15-min CLOSE (not intrabar extremes).\n",
    "  Set to False to trigger on LOW/HIGH (more realistic for stop execution).\n",
    "- Timezone: the script assumes the timestamps in M5 'date' column are already aligned\n",
    "  with the intended MARKET_START / MARKET_END clock times.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "HOW TO RUN (MINIMUM STEPS)\n",
    "-------------------------\n",
    "1) Update CONFIG paths:\n",
    "   - TRADE_CSV_PATH\n",
    "   - M5_DIR, M5_FILE_LIST_PATH\n",
    "   - MAPPING_XLSX_PATH\n",
    "   - STEP8_DIR / STEP9_DIR (best_config files)\n",
    "   - YEAR_FILTER (or remove year filter if needed)\n",
    "\n",
    "2) Ensure required columns exist in the trade CSV and M5 raw files.\n",
    "\n",
    "3) Run:\n",
    "   python your_script.py\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1863aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT THESE PATHS)\n",
    "# ============================================================\n",
    "\n",
    "TRADE_CSV_PATH = r\"D:/work/Client/Maatra/Trade Level Data/Equities 14 vol trade data (Jan 2021 to Nov 2025).csv\"\n",
    "\n",
    "M5_DIR            = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "M5_FILE_LIST_PATH = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data/m5_file_list.csv\"\n",
    "MAPPING_XLSX_PATH = r\"D:/work/Client/Maatra/Trade Level Data/Instrument Mapping.xlsx\"\n",
    "\n",
    "# Step 8 base params (dir-specific)\n",
    "STEP8_DIR = r\"D:/work/Client/Maatra/Trade Level Data/TrailingSL_CAPPED_MKT1430_2100_SME_15T_STEP8_DIRSPEC\"\n",
    "BEST_STEP8_PATH = os.path.join(STEP8_DIR, \"best_config_step8.csv\")\n",
    "\n",
    "# Step 9 sensitivity\n",
    "STEP9_DIR = r\"D:/work/Client/Maatra/Trade Level Data/TrailingSL_CAPPED_MKT1430_2100_SME_15T_STEP9_SENSITIVITY\"\n",
    "BEST_STEP9_PATH = os.path.join(STEP9_DIR, \"best_config_step9.csv\")\n",
    "\n",
    "YEAR_FILTER = 2025\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(TRADE_CSV_PATH), f\"TrailingSL_OOS_{YEAR_FILTER}_FULLYEAR_STEP8STEP9_NEXTDAY_ONLY_INITIALSL\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# Locked settings (same as pilot)\n",
    "# ============================================================\n",
    "MARKET_START = \"14:30\"\n",
    "MARKET_END   = \"21:00\"\n",
    "RESAMPLE_RULE = \"15T\"\n",
    "\n",
    "ATR_PERIOD = 14\n",
    "REG_WINDOW = 16\n",
    "DELAY_MIN = 0\n",
    "\n",
    "TRIGGER_ON_CLOSE = True\n",
    "ATR_FALLBACK_MULT = 3.0\n",
    "\n",
    "# --- Your new requirement: Initial SL must be >= 1% ---\n",
    "INITIAL_MIN_STOP_PCT = 0.01   # 1%\n",
    "# Keep trailing minimum as before (can be tighter later)\n",
    "MIN_STOP_PCT = 0.0010         # 0.10%\n",
    "\n",
    "# YOUR RULE\n",
    "APPLY_FROM_NEXT_DAY_ONLY = True\n",
    "\n",
    "# Progress printing frequency\n",
    "PRINT_EVERY = 500\n",
    "\n",
    "# ============================================================\n",
    "# Robust CSV reader\n",
    "# ============================================================\n",
    "def read_trade_csv(path: str) -> pd.DataFrame:\n",
    "    for enc in [\"utf-8\", \"cp1252\", \"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, encoding=\"utf-8\", encoding_errors=\"ignore\", low_memory=False)\n",
    "\n",
    "# ============================================================\n",
    "# Helpers\n",
    "# ============================================================\n",
    "def normalize_ticker(x: str) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    x = str(x).strip().upper()\n",
    "    return \"\".join([ch for ch in x if ch.isalnum()])\n",
    "\n",
    "def trade_base_from_currency(currency: str) -> str:\n",
    "    if currency is None or (isinstance(currency, float) and np.isnan(currency)):\n",
    "        return \"\"\n",
    "    s = str(currency).strip().upper()\n",
    "    base = s.split(\"/\")[0].strip() if \"/\" in s else s\n",
    "    return normalize_ticker(base)\n",
    "\n",
    "def normalize_direction(x) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"UNKNOWN\"\n",
    "    s = str(x).strip().upper()\n",
    "    if s in [\"LONG\", \"BUY\", \"B\", \"1\", \"1.0\", \"+1\", \"+1.0\"]:\n",
    "        return \"LONG\"\n",
    "    if s in [\"SHORT\", \"SELL\", \"S\", \"-1\", \"-1.0\"]:\n",
    "        return \"SHORT\"\n",
    "    try:\n",
    "        v = float(s)\n",
    "        if v > 0: return \"LONG\"\n",
    "        if v < 0: return \"SHORT\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def _todelta_hhmm(hhmm: str) -> pd.Timedelta:\n",
    "    return pd.to_timedelta(hhmm + \":00\")\n",
    "\n",
    "# ============================================================\n",
    "# Load mapping file\n",
    "# ============================================================\n",
    "def load_trade_to_m5_mapping(mapping_xlsx_path: str) -> dict:\n",
    "    mp = pd.read_excel(mapping_xlsx_path)\n",
    "    mp.columns = [c.strip() for c in mp.columns]\n",
    "    if \"org_symbol\" not in mp.columns or \"Symbol\" not in mp.columns:\n",
    "        raise ValueError(\"Mapping file must contain columns: org_symbol, Symbol\")\n",
    "    mp[\"m5_ticker\"] = mp[\"org_symbol\"].apply(normalize_ticker)\n",
    "    mp[\"trade_base\"] = mp[\"Symbol\"].astype(str).str.upper().str.strip().str.split(\"/\").str[0].apply(normalize_ticker)\n",
    "    mp = mp[(mp[\"trade_base\"] != \"\") & (mp[\"m5_ticker\"] != \"\")]\n",
    "    mp = mp.drop_duplicates(subset=[\"trade_base\"], keep=\"first\")\n",
    "    return dict(zip(mp[\"trade_base\"], mp[\"m5_ticker\"]))\n",
    "\n",
    "TRADE_TO_M5 = load_trade_to_m5_mapping(MAPPING_XLSX_PATH)\n",
    "\n",
    "def map_trade_base_to_m5(trade_base: str) -> str:\n",
    "    t = normalize_ticker(trade_base)\n",
    "    return TRADE_TO_M5.get(t, t)\n",
    "\n",
    "# ============================================================\n",
    "# Load M5 file list\n",
    "# ============================================================\n",
    "def load_m5_file_map(m5_dir: str, file_list_path: str) -> dict:\n",
    "    fl = pd.read_csv(file_list_path)\n",
    "    fl.columns = [c.strip().lower() for c in fl.columns]\n",
    "    if \"ticker\" not in fl.columns or \"filename\" not in fl.columns:\n",
    "        raise ValueError(\"m5_file_list.csv must have columns: ticker, filename\")\n",
    "    fl[\"ticker_norm\"] = fl[\"ticker\"].apply(normalize_ticker)\n",
    "    fl[\"path\"] = fl[\"filename\"].apply(lambda f: os.path.join(m5_dir, f))\n",
    "    return dict(zip(fl[\"ticker_norm\"], fl[\"path\"]))\n",
    "\n",
    "M5_MAP = load_m5_file_map(M5_DIR, M5_FILE_LIST_PATH)\n",
    "M5_TICKERS = set(M5_MAP.keys())\n",
    "\n",
    "# ============================================================\n",
    "# M5 preparation (market hours + resample + ATR) with caching\n",
    "# ============================================================\n",
    "def filter_market_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    dfi = df.set_index(\"_dt\", drop=False).sort_index()\n",
    "    dfi = dfi.between_time(MARKET_START, MARKET_END, inclusive=\"both\")\n",
    "    return dfi.reset_index(drop=True)\n",
    "\n",
    "def apply_delay_filter(window: pd.DataFrame, delay_minutes: int) -> pd.DataFrame:\n",
    "    if delay_minutes <= 0 or window.empty:\n",
    "        return window\n",
    "    w = window.copy()\n",
    "    w[\"d\"] = w[\"_dt\"].dt.normalize()\n",
    "    thr = w[\"d\"] + pd.to_timedelta(MARKET_START + \":00\")\n",
    "    w = w[w[\"_dt\"] >= (thr + pd.to_timedelta(delay_minutes, unit=\"m\"))]\n",
    "    return w.drop(columns=[\"d\"])\n",
    "\n",
    "def resample_ohlc(df: pd.DataFrame, rule: str) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    dfi = df.set_index(\"_dt\").sort_index()\n",
    "    ohlc = dfi[[\"open\",\"high\",\"low\",\"close\"]].resample(rule).agg({\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "    }).dropna()\n",
    "    return ohlc.reset_index()\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, period: int) -> pd.Series:\n",
    "    high = df[\"high\"]\n",
    "    low = df[\"low\"]\n",
    "    close = df[\"close\"]\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr.rolling(period, min_periods=period).mean()\n",
    "\n",
    "_RS_CACHE = {}\n",
    "\n",
    "def prepare_rs_for_ticker(ticker: str):\n",
    "    if ticker in _RS_CACHE:\n",
    "        return _RS_CACHE[ticker]\n",
    "\n",
    "    fp = M5_MAP.get(ticker)\n",
    "    if not fp or not os.path.exists(fp):\n",
    "        _RS_CACHE[ticker] = None\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    needed = [\"date\", \"open\", \"high\", \"low\", \"close\"]\n",
    "    if not all(c in df.columns for c in needed):\n",
    "        _RS_CACHE[ticker] = None\n",
    "        return None\n",
    "\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"_dt\"]).sort_values(\"_dt\").reset_index(drop=True)\n",
    "\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"open\", \"high\", \"low\", \"close\"])\n",
    "\n",
    "    df = filter_market_hours(df)\n",
    "    if df.empty:\n",
    "        _RS_CACHE[ticker] = None\n",
    "        return None\n",
    "\n",
    "    rs = resample_ohlc(df, RESAMPLE_RULE)\n",
    "    if rs.empty:\n",
    "        _RS_CACHE[ticker] = None\n",
    "        return None\n",
    "\n",
    "    rs = apply_delay_filter(rs, DELAY_MIN)\n",
    "    if rs.empty:\n",
    "        _RS_CACHE[ticker] = None\n",
    "        return None\n",
    "\n",
    "    rs[\"ATR\"] = compute_atr(rs, ATR_PERIOD)\n",
    "\n",
    "    _RS_CACHE[ticker] = rs\n",
    "    return rs\n",
    "\n",
    "# ============================================================\n",
    "# Load & merge config (Step8 base + Step9 sensitivity)\n",
    "# ============================================================\n",
    "def load_best_step8(path: str) -> dict:\n",
    "    b8 = pd.read_csv(path).iloc[0].to_dict()\n",
    "    need = [\"ATAN_LONG\",\"ATAN_SHORT\",\"MAX_STOP_LONG\",\"MAX_STOP_SHORT\"]\n",
    "    miss = [k for k in need if k not in b8 or pd.isna(b8.get(k))]\n",
    "    if miss:\n",
    "        raise ValueError(f\"best_config_step8.csv missing/NaN keys: {miss}\")\n",
    "    return {k: float(b8[k]) for k in need}\n",
    "\n",
    "def load_best_step9(path: str) -> dict:\n",
    "    b9 = pd.read_csv(path).iloc[0].to_dict()\n",
    "    return {\n",
    "        \"SENS_LONG\": float(b9.get(\"SENS_LONG\", 1.0)),\n",
    "        \"SENS_SHORT\": float(b9.get(\"SENS_SHORT\", 1.0)),\n",
    "    }\n",
    "\n",
    "def load_merged_config() -> dict:\n",
    "    base = load_best_step8(BEST_STEP8_PATH)\n",
    "    sens = load_best_step9(BEST_STEP9_PATH)\n",
    "    cfg = {**base, **sens}\n",
    "    print(\"Merged config:\", cfg)\n",
    "    return cfg\n",
    "\n",
    "# ============================================================\n",
    "# Trade extraction (NEW CSV schema)\n",
    "# ============================================================\n",
    "def build_trade_summary(raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = raw.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required = [\"TradeID\",\"Currency\",\"Direction\",\"date\",\"price\",\"Opening Date\",\"Closing Date\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Trade file missing required columns: {missing}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"Opening Date\"] = pd.to_datetime(df[\"Opening Date\"], errors=\"coerce\")\n",
    "    df[\"Closing Date\"] = pd.to_datetime(df[\"Closing Date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    df = df[df[\"date\"].dt.year == YEAR_FILTER].copy()\n",
    "\n",
    "    df[\"TradeBase\"] = df[\"Currency\"].apply(trade_base_from_currency)\n",
    "    df[\"TickerNorm\"] = df[\"TradeBase\"].apply(map_trade_base_to_m5)\n",
    "    df[\"DirectionNorm\"] = df[\"Direction\"].apply(normalize_direction)\n",
    "\n",
    "    rows = []\n",
    "    for tid, g in df.groupby(\"TradeID\", dropna=True):\n",
    "        g = g.sort_values(\"date\")\n",
    "\n",
    "        entry_date = g[\"Opening Date\"].dropna().iloc[0] if g[\"Opening Date\"].notna().any() else pd.NaT\n",
    "        exit_date  = g[\"Closing Date\"].dropna().iloc[0] if g[\"Closing Date\"].notna().any() else pd.NaT\n",
    "\n",
    "        direction = g[\"DirectionNorm\"].dropna().iloc[0] if g[\"DirectionNorm\"].notna().any() else \"UNKNOWN\"\n",
    "        ticker = g[\"TickerNorm\"].dropna().iloc[0] if g[\"TickerNorm\"].notna().any() else \"\"\n",
    "\n",
    "        entry_px = np.nan\n",
    "        if pd.notna(entry_date):\n",
    "            day_rows = g[g[\"date\"].dt.normalize() == entry_date.normalize()]\n",
    "            if day_rows[\"price\"].notna().any():\n",
    "                entry_px = float(day_rows[\"price\"].dropna().iloc[-1])\n",
    "\n",
    "        exit_px = np.nan\n",
    "        if pd.notna(exit_date):\n",
    "            day_rows = g[g[\"date\"].dt.normalize() == exit_date.normalize()]\n",
    "            if day_rows[\"price\"].notna().any():\n",
    "                exit_px = float(day_rows[\"price\"].dropna().iloc[-1])\n",
    "\n",
    "        valid_window = pd.notna(entry_date) and pd.notna(exit_date) and (exit_date.normalize() > entry_date.normalize())\n",
    "        core_ok = (ticker != \"\" and ticker in M5_TICKERS and direction != \"UNKNOWN\" and\n",
    "                   pd.notna(entry_date) and pd.notna(exit_date) and pd.notna(entry_px) and pd.notna(exit_px))\n",
    "\n",
    "        rows.append({\n",
    "            \"TradeID\": tid,\n",
    "            \"TickerNorm\": ticker,\n",
    "            \"Direction\": direction,\n",
    "            \"EntryDate\": entry_date,\n",
    "            \"ExitDate_Original\": exit_date,\n",
    "            \"EntryPrice\": entry_px,\n",
    "            \"ExitPrice_Original\": exit_px,\n",
    "            \"HasAllCoreFields\": bool(core_ok),\n",
    "            \"HasValidTrailingWindow\": bool(valid_window)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# Trailing simulation\n",
    "# ============================================================\n",
    "def adaptive_stop_distance_pct(bars: pd.DataFrame,\n",
    "                               atan_threshold: float,\n",
    "                               atr_fallback_mult: float,\n",
    "                               min_stop_pct: float,\n",
    "                               max_stop_pct: float,\n",
    "                               sensitivity: float) -> float:\n",
    "    if len(bars) < REG_WINDOW or bars[\"ATR\"].dropna().empty:\n",
    "        return np.nan\n",
    "\n",
    "    recent = bars[\"close\"].iloc[-REG_WINDOW:].astype(float)\n",
    "    if len(recent) < REG_WINDOW:\n",
    "        return np.nan\n",
    "\n",
    "    x = np.arange(len(recent), dtype=float)\n",
    "    a, b, c = np.polyfit(x, recent.values, 2)\n",
    "\n",
    "    slope_norm = (2 * a * x[-1] + b) / recent.iloc[-1]\n",
    "    accel = (2 * a)\n",
    "    atan_slope = float(np.arctan(slope_norm))\n",
    "\n",
    "    atr = bars[\"ATR\"].iloc[-1]\n",
    "    px = bars[\"close\"].iloc[-1]\n",
    "    atr_pct = float(atr / px) if (pd.notna(atr) and atr > 0 and px > 0) else np.nan\n",
    "    if pd.isna(atr_pct):\n",
    "        return np.nan\n",
    "\n",
    "    if abs(atan_slope) > atan_threshold:\n",
    "        stop_pct = abs(slope_norm) * sensitivity * (1.0 + abs(accel))\n",
    "        return float(np.clip(stop_pct, min_stop_pct, max_stop_pct))\n",
    "\n",
    "    stop_pct = atr_pct * atr_fallback_mult\n",
    "    return float(np.clip(stop_pct, min_stop_pct, max_stop_pct))\n",
    "\n",
    "def compute_initial_stop_pct(rs: pd.DataFrame,\n",
    "                             entry_date: pd.Timestamp,\n",
    "                             entry_px: float,\n",
    "                             direction: str,\n",
    "                             cfg: dict) -> float:\n",
    "    \"\"\"\n",
    "    Initial SL at ENTRY CLOSE:\n",
    "    - Must be >= INITIAL_MIN_STOP_PCT (>=1%)\n",
    "    - Can be wider, but capped by MAX_STOP_* for direction.\n",
    "    Priority:\n",
    "      1) adaptive_stop_distance_pct on history up to entry close (if available)\n",
    "      2) ATR% * ATR_FALLBACK_MULT (if ATR available)\n",
    "      3) INITIAL_MIN_STOP_PCT\n",
    "    \"\"\"\n",
    "    direction = normalize_direction(direction)\n",
    "    is_long = (direction == \"LONG\")\n",
    "\n",
    "    atan_thr = cfg[\"ATAN_LONG\"] if is_long else cfg[\"ATAN_SHORT\"]\n",
    "    max_stop = cfg[\"MAX_STOP_LONG\"] if is_long else cfg[\"MAX_STOP_SHORT\"]\n",
    "    sens     = cfg[\"SENS_LONG\"] if is_long else cfg[\"SENS_SHORT\"]\n",
    "\n",
    "    entry_close_dt = entry_date.normalize() + _todelta_hhmm(MARKET_END)\n",
    "\n",
    "    hist = rs[rs[\"_dt\"] <= entry_close_dt].copy()\n",
    "    if hist.empty:\n",
    "        return float(np.clip(INITIAL_MIN_STOP_PCT, INITIAL_MIN_STOP_PCT, max_stop))\n",
    "\n",
    "    stop_pct = adaptive_stop_distance_pct(\n",
    "        hist, atan_thr, ATR_FALLBACK_MULT, INITIAL_MIN_STOP_PCT, max_stop, sens\n",
    "    )\n",
    "    if pd.notna(stop_pct):\n",
    "        return float(stop_pct)\n",
    "\n",
    "    last = hist.iloc[-1]\n",
    "    atr = last.get(\"ATR\", np.nan)\n",
    "    px  = last.get(\"close\", np.nan)\n",
    "    if pd.notna(atr) and atr > 0 and pd.notna(px) and px > 0:\n",
    "        atr_pct = float(atr / px)\n",
    "        stop_pct = atr_pct * ATR_FALLBACK_MULT\n",
    "        return float(np.clip(stop_pct, INITIAL_MIN_STOP_PCT, max_stop))\n",
    "\n",
    "    return float(np.clip(INITIAL_MIN_STOP_PCT, INITIAL_MIN_STOP_PCT, max_stop))\n",
    "\n",
    "@dataclass\n",
    "class TrailResult:\n",
    "    exit_time: pd.Timestamp\n",
    "    exit_price: float\n",
    "    exit_reason: str\n",
    "    initial_stop_pct: float\n",
    "    initial_stop_price: float\n",
    "\n",
    "def price_improvement(direction: str, new_exit: float, old_exit: float) -> float:\n",
    "    d = normalize_direction(direction)\n",
    "    return (new_exit - old_exit) if d == \"LONG\" else (old_exit - new_exit)\n",
    "\n",
    "def simulate_trade(tr: pd.Series, rs: pd.DataFrame, cfg: dict) -> TrailResult:\n",
    "    entry_date = pd.to_datetime(tr[\"EntryDate\"])\n",
    "    exit_date  = pd.to_datetime(tr[\"ExitDate_Original\"])\n",
    "    entry_px   = float(tr[\"EntryPrice\"])\n",
    "    orig_exit_px = float(tr[\"ExitPrice_Original\"])\n",
    "\n",
    "    if exit_date.normalize() <= entry_date.normalize():\n",
    "        return TrailResult(exit_date, orig_exit_px, \"SameDayOrInvalidWindow\",\n",
    "                           np.nan, np.nan)\n",
    "\n",
    "    direction = normalize_direction(tr[\"Direction\"])\n",
    "    is_long = (direction == \"LONG\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Initial SL placed at entry close (>= 1%)\n",
    "    # ----------------------------\n",
    "    init_stop_pct = compute_initial_stop_pct(rs, entry_date, entry_px, direction, cfg)\n",
    "    if is_long:\n",
    "        stop = entry_px * (1.0 - init_stop_pct)\n",
    "    else:\n",
    "        stop = entry_px * (1.0 + init_stop_pct)\n",
    "\n",
    "    initial_stop_price = float(stop)\n",
    "    best_close = entry_px\n",
    "    stop_source = \"Initial\"  # becomes \"Trailing\" once adaptive updates\n",
    "\n",
    "    # Monitoring starts from next day market open\n",
    "    start_dt = entry_date.normalize() + timedelta(days=1) + _todelta_hhmm(MARKET_START)\n",
    "    end_dt   = exit_date + timedelta(days=1)\n",
    "\n",
    "    w = rs[(rs[\"_dt\"] >= start_dt) & (rs[\"_dt\"] < end_dt)].copy()\n",
    "    if w.empty:\n",
    "        return TrailResult(exit_date, orig_exit_px, \"NoRSDataInWindow\",\n",
    "                           float(init_stop_pct), float(initial_stop_price))\n",
    "\n",
    "    for i in range(len(w)):\n",
    "        sub = w.iloc[:i+1]\n",
    "        row = w.iloc[i]\n",
    "        dt = row[\"_dt\"]\n",
    "\n",
    "        close = float(row[\"close\"])\n",
    "        high = float(row[\"high\"])\n",
    "        low  = float(row[\"low\"])\n",
    "\n",
    "        if is_long:\n",
    "            best_close = max(best_close, close)\n",
    "        else:\n",
    "            best_close = min(best_close, close)\n",
    "\n",
    "        # 1) Check current stop first (so Initial SL can trigger immediately)\n",
    "        if is_long:\n",
    "            hit_val = close if TRIGGER_ON_CLOSE else low\n",
    "            if hit_val <= stop:\n",
    "                reason = \"InitialSL\" if stop_source == \"Initial\" else \"TrailingSL\"\n",
    "                return TrailResult(dt, float(stop), reason,\n",
    "                                   float(init_stop_pct), float(initial_stop_price))\n",
    "        else:\n",
    "            hit_val = close if TRIGGER_ON_CLOSE else high\n",
    "            if hit_val >= stop:\n",
    "                reason = \"InitialSL\" if stop_source == \"Initial\" else \"TrailingSL\"\n",
    "                return TrailResult(dt, float(stop), reason,\n",
    "                                   float(init_stop_pct), float(initial_stop_price))\n",
    "\n",
    "        # 2) Then try to tighten via adaptive logic (may be NaN early)\n",
    "        atan_thr = cfg[\"ATAN_LONG\"] if is_long else cfg[\"ATAN_SHORT\"]\n",
    "        max_stop = cfg[\"MAX_STOP_LONG\"] if is_long else cfg[\"MAX_STOP_SHORT\"]\n",
    "        sens     = cfg[\"SENS_LONG\"] if is_long else cfg[\"SENS_SHORT\"]\n",
    "\n",
    "        stop_pct = adaptive_stop_distance_pct(\n",
    "            sub, atan_thr, ATR_FALLBACK_MULT, MIN_STOP_PCT, max_stop, sens\n",
    "        )\n",
    "        if pd.isna(stop_pct):\n",
    "            continue\n",
    "\n",
    "        if is_long:\n",
    "            stop = max(stop, best_close * (1.0 - stop_pct))\n",
    "        else:\n",
    "            stop = min(stop, best_close * (1.0 + stop_pct))\n",
    "\n",
    "        stop_source = \"Trailing\"\n",
    "\n",
    "    return TrailResult(exit_date, orig_exit_px, \"OriginalExit\",\n",
    "                       float(init_stop_pct), float(initial_stop_price))\n",
    "\n",
    "def summarize(out: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = out.copy()\n",
    "    df[\"TriggeredFlag\"] = df[\"ExitReason\"].isin([\"TrailingSL\", \"InitialSL\"])\n",
    "    df[\"ImprovedFlag\"] = df[\"PriceImprovement\"] > 0\n",
    "\n",
    "    def agg(g):\n",
    "        n = len(g)\n",
    "        trig = int(g[\"TriggeredFlag\"].sum())\n",
    "        return {\n",
    "            \"Trades\": n,\n",
    "            \"SLTriggered\": trig,\n",
    "            \"PctTriggered\": trig / n if n else np.nan,\n",
    "            \"Triggered_PctImproved\": float(g.loc[g[\"TriggeredFlag\"], \"ImprovedFlag\"].mean()) if trig else np.nan,\n",
    "            \"AvgPctImpact_All\": float(g[\"PctImprovement_vs_OriginalExit\"].mean(skipna=True)),\n",
    "            \"AvgPctImpact_Triggered\": float(g.loc[g[\"TriggeredFlag\"], \"PctImprovement_vs_OriginalExit\"].mean(skipna=True)) if trig else np.nan,\n",
    "        }\n",
    "\n",
    "    rows = [{\"Group\":\"ALL\", **agg(df)}]\n",
    "    for d, g in df.groupby(df[\"Direction\"].astype(str).str.upper().str.strip()):\n",
    "        rows.append({\"Group\": f\"Direction={d}\", **agg(g)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN (FULL YEAR)\n",
    "# ============================================================\n",
    "def main():\n",
    "    cfg = load_merged_config()\n",
    "\n",
    "    raw = read_trade_csv(TRADE_CSV_PATH)\n",
    "    trades = build_trade_summary(raw)\n",
    "\n",
    "    print(\"\\nTradeIDs in year (pre-filter):\", trades[\"TradeID\"].nunique())\n",
    "    print(\"Trades with core fields OK:\", int(trades[\"HasAllCoreFields\"].sum()))\n",
    "    print(\"Trades with valid trailing window:\", int(trades[\"HasValidTrailingWindow\"].sum()))\n",
    "\n",
    "    eligible = trades[(trades[\"HasAllCoreFields\"]) & (trades[\"HasValidTrailingWindow\"])].copy()\n",
    "    if eligible.empty:\n",
    "        raise RuntimeError(\"No eligible trades after filtering (core fields + exit>entry).\")\n",
    "\n",
    "    eligible = eligible.sort_values([\"TickerNorm\", \"EntryDate\", \"TradeID\"]).reset_index(drop=True)\n",
    "    print(\"\\nEligible trades to process:\", len(eligible))\n",
    "    print(\"Unique tickers in eligible:\", eligible[\"TickerNorm\"].nunique())\n",
    "\n",
    "    # Preload RS for all tickers\n",
    "    tickers = sorted(eligible[\"TickerNorm\"].unique())\n",
    "    print(\"\\nPreloading resampled market-hours data for tickers:\", len(tickers))\n",
    "    for i, tkr in enumerate(tickers, 1):\n",
    "        prepare_rs_for_ticker(tkr)\n",
    "        if i % 25 == 0:\n",
    "            print(f\"  Loaded {i}/{len(tickers)} tickers...\")\n",
    "\n",
    "    out_rows = []\n",
    "    dbg_rows = []\n",
    "\n",
    "    for i, tr in eligible.iterrows():\n",
    "        rs = prepare_rs_for_ticker(tr[\"TickerNorm\"])\n",
    "        if rs is None or rs.empty:\n",
    "            dbg_rows.append({\"TradeID\": tr[\"TradeID\"], \"Reason\": \"MissingRSData\"})\n",
    "            continue\n",
    "\n",
    "        res = simulate_trade(tr, rs, cfg)\n",
    "\n",
    "        old_exit = float(tr[\"ExitPrice_Original\"])\n",
    "        new_exit = float(res.exit_price)\n",
    "        impr = price_improvement(tr[\"Direction\"], new_exit, old_exit)\n",
    "\n",
    "        out_rows.append({\n",
    "            \"TradeID\": tr[\"TradeID\"],\n",
    "            \"TickerNorm\": tr[\"TickerNorm\"],\n",
    "            \"Direction\": tr[\"Direction\"],\n",
    "            \"EntryDate\": tr[\"EntryDate\"],\n",
    "            \"ExitDate_Original\": tr[\"ExitDate_Original\"],\n",
    "            \"EntryPrice\": float(tr[\"EntryPrice\"]),\n",
    "            \"InitialStopPct\": res.initial_stop_pct,\n",
    "            \"InitialStopPrice\": res.initial_stop_price,\n",
    "            \"ExitPrice_Original\": old_exit,\n",
    "            \"ExitTime_Trailing\": res.exit_time,\n",
    "            \"ExitPrice_Trailing\": new_exit,\n",
    "            \"ExitReason\": res.exit_reason,\n",
    "            \"PriceImprovement\": impr,\n",
    "            \"PctImprovement_vs_OriginalExit\": (impr / old_exit) if old_exit else np.nan\n",
    "        })\n",
    "        dbg_rows.append({\"TradeID\": tr[\"TradeID\"], \"Reason\": res.exit_reason})\n",
    "\n",
    "        if (i + 1) % PRINT_EVERY == 0:\n",
    "            print(f\"Processed {i+1}/{len(eligible)} trades...\")\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "    summ_df = summarize(out_df)\n",
    "    dbg_df = pd.DataFrame(dbg_rows)\n",
    "\n",
    "    reason_counts = dbg_df[\"Reason\"].value_counts(dropna=False)\n",
    "\n",
    "    out_path = os.path.join(OUT_DIR, f\"trades_full_{YEAR_FILTER}.csv\")\n",
    "    summ_path = os.path.join(OUT_DIR, f\"summary_full_{YEAR_FILTER}.csv\")\n",
    "    dbg_path = os.path.join(OUT_DIR, f\"debug_reasons_full_{YEAR_FILTER}.csv\")\n",
    "    reason_path = os.path.join(OUT_DIR, f\"reason_counts_full_{YEAR_FILTER}.csv\")\n",
    "\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "    summ_df.to_csv(summ_path, index=False)\n",
    "    dbg_df.to_csv(dbg_path, index=False)\n",
    "    reason_counts.rename_axis(\"Reason\").reset_index(name=\"Count\").to_csv(reason_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\", out_path)\n",
    "    print(\"Saved:\", summ_path)\n",
    "    print(\"Saved:\", dbg_path)\n",
    "    print(\"Saved:\", reason_path)\n",
    "\n",
    "    print(\"\\n=== FULL YEAR SUMMARY ===\")\n",
    "    print(summ_df.to_string(index=False))\n",
    "\n",
    "    print(\"\\n=== EXIT REASONS (full year) ===\")\n",
    "    print(reason_counts.to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
