{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab44e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d992d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "5-Minute → 15-Minute OHLC Builder (Polygon Raw Data)\n",
    "===============================================================================\n",
    "\n",
    "PURPOSE\n",
    "-------\n",
    "This script converts raw 5-minute price data into 15-minute OHLC bars and\n",
    "stores them as parquet files for faster downstream use (e.g., trailing stop-loss\n",
    "simulations).\n",
    "\n",
    "Instead of resampling 5-minute data every time inside the stop-loss logic,\n",
    "this script builds and caches 15-minute bars once (or incrementally).\n",
    "\n",
    "This significantly reduces runtime for repeated backtests.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "INPUT DATA STRUCTURE\n",
    "-------------------------------------------------------------------------------\n",
    "Expected raw structure:\n",
    "\n",
    "RAW_5M_ROOT/\n",
    "    AAOI/\n",
    "        <5-min csv or parquet files>\n",
    "    MSFT/\n",
    "        <5-min files>\n",
    "    ...\n",
    "    (one folder per symbol)\n",
    "\n",
    "Each raw file should contain columns (case-insensitive detection supported):\n",
    "    - date / trade_date / datetime / timestamp\n",
    "    - open (or open_close)\n",
    "    - high (or high_close)\n",
    "    - low  (or low_close)\n",
    "    - close (or close_close)\n",
    "\n",
    "Multiple files per symbol are supported (they are concatenated).\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "OUTPUT STRUCTURE\n",
    "-------------------------------------------------------------------------------\n",
    "OUT_15M_ROOT/\n",
    "    symbol=AAOI/\n",
    "        rs_15T.parquet\n",
    "    symbol=MSFT/\n",
    "        rs_15T.parquet\n",
    "    ...\n",
    "\n",
    "Each output file contains:\n",
    "    - _dt\n",
    "    - open\n",
    "    - high\n",
    "    - low\n",
    "    - close\n",
    "\n",
    "These are 15-minute resampled OHLC bars.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "KEY FEATURES\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "1) Market Hours Filtering (Optional)\n",
    "   If FILTER_MARKET_HOURS = True, raw 5-minute data is filtered to:\n",
    "       MARKET_START → MARKET_END\n",
    "   before resampling.\n",
    "\n",
    "2) 5-Min → 15-Min Resampling\n",
    "   Uses standard OHLC aggregation:\n",
    "       open  = first\n",
    "       high  = max\n",
    "       low   = min\n",
    "       close = last\n",
    "\n",
    "3) Incremental Mode (Recommended)\n",
    "   If INCREMENTAL = True:\n",
    "       - Script reads the existing rs_15T.parquet\n",
    "       - Only processes 5-minute data after the last saved timestamp\n",
    "       - Appends new 15-minute bars\n",
    "   This makes daily updates very fast.\n",
    "\n",
    "4) Robust File Handling\n",
    "   - Supports both CSV and parquet input\n",
    "   - Auto-detects common column name variations\n",
    "   - Sorts and removes duplicate timestamps\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "WHY THIS EXISTS\n",
    "-------------------------------------------------------------------------------\n",
    "Your trailing stop-loss backtest runs on 15-minute candles.\n",
    "\n",
    "If resampling is done inside the stop-loss script:\n",
    "    - It repeats work for every run\n",
    "    - It increases memory and runtime\n",
    "    - It slows multi-strategy iterations\n",
    "\n",
    "By building a dedicated 15-minute dataset once:\n",
    "    - Stop-loss code becomes much faster\n",
    "    - Data consistency improves\n",
    "    - Only incremental updates are required\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "HOW TO RUN\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "1) Set paths in CONFIG:\n",
    "       RAW_5M_ROOT\n",
    "       OUT_15M_ROOT\n",
    "\n",
    "2) (Optional) Adjust:\n",
    "       FILTER_MARKET_HOURS\n",
    "       MARKET_START / MARKET_END\n",
    "       INCREMENTAL\n",
    "\n",
    "3) Run:\n",
    "       python build_15T_from_5m.py\n",
    "\n",
    "4) Output will be saved under OUT_15M_ROOT.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "IMPORTANT ASSUMPTIONS\n",
    "-------------------------------------------------------------------------------\n",
    "- Raw timestamps are already in the correct timezone.\n",
    "- No timezone conversion is applied in this script.\n",
    "- Market hours filter is purely time-based (no exchange calendar logic).\n",
    "- Parquet writing requires pyarrow or fastparquet installed.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "INTENDED WORKFLOW\n",
    "-------------------------------------------------------------------------------\n",
    "Step 1: Run this builder script (one-time or daily incremental update)\n",
    "Step 2: Stop-loss simulation loads rs_15T.parquet directly\n",
    "Step 3: ATR and trailing logic run on prebuilt 15-minute bars\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e6651a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1920 symbol folders under: D:\\work\\Trade Analysis\\Polygon_5min_2016_to_today\n",
      "Processed 25/1920 symbols...\n",
      "Processed 50/1920 symbols...\n",
      "Processed 75/1920 symbols...\n",
      "Processed 100/1920 symbols...\n",
      "Processed 125/1920 symbols...\n",
      "Processed 150/1920 symbols...\n",
      "Processed 175/1920 symbols...\n",
      "Processed 200/1920 symbols...\n",
      "Processed 225/1920 symbols...\n",
      "Processed 250/1920 symbols...\n",
      "Processed 275/1920 symbols...\n",
      "Processed 300/1920 symbols...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================================================\n",
    "# CONFIG\n",
    "# =========================================================\n",
    "RAW_5M_ROOT = Path(r\"D:\\work\\Trade Analysis\\Polygon_5min_2016_to_today\")   # contains folders like AAOI, MSFT, etc.\n",
    "\n",
    "OUT_15M_ROOT = Path(r\"D:\\work\\Trade Analysis\\Polygon_15min_from_5min\")    # new cache location\n",
    "OUT_15M_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RESAMPLE_RULE = \"15T\"\n",
    "\n",
    "# If you want to match your stop-loss market window:\n",
    "FILTER_MARKET_HOURS = True\n",
    "MARKET_START = \"14:30\"\n",
    "MARKET_END   = \"21:00\"\n",
    "\n",
    "# Incremental build: only append bars after the last saved _dt\n",
    "INCREMENTAL = True\n",
    "\n",
    "# If your raw files are very large, this keeps memory reasonable\n",
    "SORT_AND_DEDUP = True\n",
    "\n",
    "# =========================================================\n",
    "# Helpers\n",
    "# =========================================================\n",
    "def normalize_ticker(x: str) -> str:\n",
    "    x = str(x).strip().upper()\n",
    "    return \"\".join([ch for ch in x if ch.isalnum()])\n",
    "\n",
    "def _pick_first_existing(df: pd.DataFrame, candidates):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in cols:\n",
    "            return cols[c.lower()]\n",
    "    return \"\"\n",
    "\n",
    "def _load_any_file(fp: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads either .csv or .parquet\"\"\"\n",
    "    if fp.suffix.lower() in [\".parquet\"]:\n",
    "        return pd.read_parquet(fp)\n",
    "    if fp.suffix.lower() in [\".csv\"]:\n",
    "        return pd.read_csv(fp, low_memory=False)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_symbol_5m(symbol_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "    - If directory contains multiple parquet/csv files, loads and concatenates them.\n",
    "    - Expected columns (any variants): date/trade_date, open/high/low/close (or *_close variants).\n",
    "    \"\"\"\n",
    "    if not symbol_dir.exists() or not symbol_dir.is_dir():\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    files = []\n",
    "    for ext in (\"*.parquet\", \"*.csv\"):\n",
    "        files.extend(sorted(symbol_dir.rglob(ext)))\n",
    "\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    parts = []\n",
    "    for fp in files:\n",
    "        df = _load_any_file(fp)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "\n",
    "        # detect columns\n",
    "        dt_col = _pick_first_existing(df, [\"date\", \"trade_date\", \"datetime\", \"timestamp\"])\n",
    "        o_col  = _pick_first_existing(df, [\"open\", \"open_close\", \"o\"])\n",
    "        h_col  = _pick_first_existing(df, [\"high\", \"high_close\", \"h\"])\n",
    "        l_col  = _pick_first_existing(df, [\"low\", \"low_close\", \"l\"])\n",
    "        c_col  = _pick_first_existing(df, [\"close\", \"close_close\", \"c\"])\n",
    "\n",
    "        if not all([dt_col, o_col, h_col, l_col, c_col]):\n",
    "            continue\n",
    "\n",
    "        x = df[[dt_col, o_col, h_col, l_col, c_col]].copy()\n",
    "        x.columns = [\"_dt\", \"open\", \"high\", \"low\", \"close\"]\n",
    "\n",
    "        x[\"_dt\"] = pd.to_datetime(x[\"_dt\"], errors=\"coerce\")\n",
    "        x = x.dropna(subset=[\"_dt\"])\n",
    "        for col in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "            x[col] = pd.to_numeric(x[col], errors=\"coerce\")\n",
    "        x = x.dropna(subset=[\"open\", \"high\", \"low\", \"close\"])\n",
    "\n",
    "        if not x.empty:\n",
    "            parts.append(x)\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    if SORT_AND_DEDUP:\n",
    "        out = out.sort_values(\"_dt\")\n",
    "        out = out.drop_duplicates(subset=[\"_dt\"], keep=\"last\")\n",
    "\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "def filter_market_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    dfi = df.set_index(\"_dt\").sort_index()\n",
    "    dfi = dfi.between_time(MARKET_START, MARKET_END, inclusive=\"both\")\n",
    "    return dfi.reset_index()\n",
    "\n",
    "def resample_ohlc(df: pd.DataFrame, rule: str) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df\n",
    "    dfi = df.set_index(\"_dt\").sort_index()\n",
    "    ohlc = dfi[[\"open\", \"high\", \"low\", \"close\"]].resample(rule).agg({\n",
    "        \"open\": \"first\",\n",
    "        \"high\": \"max\",\n",
    "        \"low\": \"min\",\n",
    "        \"close\": \"last\",\n",
    "    }).dropna()\n",
    "    return ohlc.reset_index()\n",
    "\n",
    "def get_out_path(symbol: str) -> Path:\n",
    "    sym = normalize_ticker(symbol)\n",
    "    sym_dir = OUT_15M_ROOT / f\"symbol={sym}\"\n",
    "    sym_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return sym_dir / \"rs_15T.parquet\"\n",
    "\n",
    "def load_existing_last_dt(out_fp: Path):\n",
    "    if not out_fp.exists():\n",
    "        return None\n",
    "    try:\n",
    "        existing = pd.read_parquet(out_fp, columns=[\"_dt\"])\n",
    "        if existing.empty:\n",
    "            return None\n",
    "        return pd.to_datetime(existing[\"_dt\"]).max()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def build_symbol(symbol: str):\n",
    "    sym = normalize_ticker(symbol)\n",
    "    in_dir = RAW_5M_ROOT / sym\n",
    "    out_fp = get_out_path(sym)\n",
    "\n",
    "    df5 = load_symbol_5m(in_dir)\n",
    "    if df5.empty:\n",
    "        return sym, \"NO_INPUT_DATA\", 0\n",
    "\n",
    "    # incremental cut\n",
    "    if INCREMENTAL:\n",
    "        last_dt = load_existing_last_dt(out_fp)\n",
    "        if last_dt is not None:\n",
    "            df5 = df5[df5[\"_dt\"] > last_dt].copy()\n",
    "            if df5.empty:\n",
    "                return sym, \"UP_TO_DATE\", 0\n",
    "\n",
    "    if FILTER_MARKET_HOURS:\n",
    "        df5 = filter_market_hours(df5)\n",
    "        if df5.empty:\n",
    "            return sym, \"EMPTY_AFTER_MARKET_FILTER\", 0\n",
    "\n",
    "    df15 = resample_ohlc(df5, RESAMPLE_RULE)\n",
    "    if df15.empty:\n",
    "        return sym, \"EMPTY_AFTER_RESAMPLE\", 0\n",
    "\n",
    "    # append/merge with existing\n",
    "    if out_fp.exists():\n",
    "        try:\n",
    "            old = pd.read_parquet(out_fp)\n",
    "            merged = pd.concat([old, df15], ignore_index=True)\n",
    "            merged = merged.sort_values(\"_dt\").drop_duplicates(subset=[\"_dt\"], keep=\"last\").reset_index(drop=True)\n",
    "        except Exception:\n",
    "            merged = df15\n",
    "    else:\n",
    "        merged = df15\n",
    "\n",
    "    merged.to_parquet(out_fp, index=False)\n",
    "    return sym, \"BUILT\", len(df15)\n",
    "\n",
    "def list_symbols(root: Path):\n",
    "    # folders like AAOI, MSFT ...\n",
    "    syms = []\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir():\n",
    "            syms.append(normalize_ticker(p.name))\n",
    "    return sorted(set(syms))\n",
    "\n",
    "def main():\n",
    "    symbols = list_symbols(RAW_5M_ROOT)\n",
    "    print(f\"Found {len(symbols)} symbol folders under: {RAW_5M_ROOT}\")\n",
    "\n",
    "    results = []\n",
    "    for i, sym in enumerate(symbols, 1):\n",
    "        s, status, n = build_symbol(sym)\n",
    "        results.append((s, status, n))\n",
    "        if i % 25 == 0:\n",
    "            print(f\"Processed {i}/{len(symbols)} symbols...\")\n",
    "\n",
    "    res = pd.DataFrame(results, columns=[\"Symbol\", \"Status\", \"New15MBarsWritten\"])\n",
    "    print(\"\\nStatus counts:\")\n",
    "    print(res[\"Status\"].value_counts(dropna=False).to_string())\n",
    "    out_log = OUT_15M_ROOT / \"build_15T_log.csv\"\n",
    "    res.to_csv(out_log, index=False)\n",
    "    print(f\"\\nSaved log: {out_log}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46340e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
