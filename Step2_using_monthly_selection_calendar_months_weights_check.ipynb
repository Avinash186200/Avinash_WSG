{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4d9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe72f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a55a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615aff70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS_60 sum: 1.0\n",
      "Weight buckets (effective, after normalization):\n",
      "         last_3m: 0.198020\n",
      "  months_4_to_12: 0.198020\n",
      "  months_13_to_24: 0.247525\n",
      "  months_25_to_36: 0.118812\n",
      "  months_37_to_48: 0.118812\n",
      "  months_49_to_60: 0.118812\n",
      "WARNING: WEIGHT_CONFIG sums to 1.010000 (not 1.0). Effective bucket weights are scaled by 1/1.010000.\n",
      "\n",
      "Processing: D:\\work\\Trade Analysis\\Monthly Selection\\old\n",
      "  Found monthly stats: D:\\work\\Trade Analysis\\Monthly Selection\\old\\monthly_stats_all.csv\n",
      "  Saved ALL scores → D:\\work\\Trade Analysis\\Monthly Selection\\old\\walk_forward_scores_all.csv\n",
      "  Saved SELECTION (Sharpe ≥ 1) → D:\\work\\Trade Analysis\\Monthly Selection\\old\\walk_forward_selection_sharpe_ge_1.csv\n",
      "\n",
      "DONE — Walk-forward monthly selection completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Root folder containing all strategy folders\n",
    "ROOT_DIR = r\"D:\\work\\Trade Analysis\\Monthly Selection\"\n",
    "\n",
    "# Pattern to find your monthly stats file (CSV only)\n",
    "MONTHLY_CSV_PATTERN = \"monthly_stats_all.csv\"\n",
    "\n",
    "# Rolling window = 5 years = 60 months\n",
    "WINDOW_MONTHS = 60\n",
    "\n",
    "# Sharpe filter\n",
    "MIN_SHARPE = 1.0\n",
    "\n",
    "# Recency weight configuration\n",
    "WEIGHT_CONFIG = {\n",
    "    \"recent_3m\": 0.20,   # last 3 months\n",
    "    \"rest_year1\": 0.20,  # months 4–12\n",
    "    \"year2\": 0.25,       # months 13–24\n",
    "    \"year3\": 0.12,       # months 25–36\n",
    "    \"year4\": 0.12,       # months 37–48\n",
    "    \"year5\": 0.12        # months 49–60\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# BUILD WEIGHTS\n",
    "# ============================================================\n",
    "\n",
    "def build_weights():\n",
    "    n = WINDOW_MONTHS\n",
    "    w = np.zeros(n)\n",
    "\n",
    "    w_r3 = WEIGHT_CONFIG[\"recent_3m\"]\n",
    "    w_y1 = WEIGHT_CONFIG[\"rest_year1\"]\n",
    "    w_y2 = WEIGHT_CONFIG[\"year2\"]\n",
    "    w_y3 = WEIGHT_CONFIG[\"year3\"]\n",
    "    w_y4 = WEIGHT_CONFIG[\"year4\"]\n",
    "    w_y5 = WEIGHT_CONFIG[\"year5\"]\n",
    "\n",
    "    for i in range(n):\n",
    "        m = (n - 1) - i  # 0=newest\n",
    "        if m <= 2:\n",
    "            w[i] = w_r3 / 3\n",
    "        elif m <= 11:\n",
    "            w[i] = w_y1 / 9\n",
    "        elif m <= 23:\n",
    "            w[i] = w_y2 / 12\n",
    "        elif m <= 35:\n",
    "            w[i] = w_y3 / 12\n",
    "        elif m <= 47:\n",
    "            w[i] = w_y4 / 12\n",
    "        else:\n",
    "            w[i] = w_y5 / 12\n",
    "\n",
    "    w /= w.sum()\n",
    "    return w\n",
    "\n",
    "\n",
    "WEIGHTS_60 = build_weights()\n",
    "\n",
    "# ----------------------------\n",
    "# Weight sanity-check (calendar-month buckets)\n",
    "# ----------------------------\n",
    "def summarize_weights(w):\n",
    "    w = np.asarray(w, dtype=float)\n",
    "    assert len(w) == WINDOW_MONTHS\n",
    "    # w is ordered oldest -> newest\n",
    "    buckets = {\n",
    "        \"last_3m\":            w[-3:].sum(),\n",
    "        \"months_4_to_12\":     w[-12:-3].sum(),   # 9 months\n",
    "        \"months_13_to_24\":    w[-24:-12].sum(),  # 12 months\n",
    "        \"months_25_to_36\":    w[-36:-24].sum(),  # 12 months\n",
    "        \"months_37_to_48\":    w[-48:-36].sum(),  # 12 months\n",
    "        \"months_49_to_60\":    w[:12].sum(),      # 12 months (oldest)\n",
    "    }\n",
    "    return buckets\n",
    "\n",
    "WEIGHT_BUCKETS = summarize_weights(WEIGHTS_60)\n",
    "\n",
    "print(\"WEIGHTS_60 sum:\", WEIGHTS_60.sum())\n",
    "print(\"Weight buckets (effective, after normalization):\")\n",
    "for k,v in WEIGHT_BUCKETS.items():\n",
    "    print(f\"  {k:>14s}: {v:.6f}\")\n",
    "\n",
    "# Optional: enforce that your intended config sums to 1.0 (recommended)\n",
    "cfg_sum = sum(WEIGHT_CONFIG.values())\n",
    "if abs(cfg_sum - 1.0) > 1e-9:\n",
    "    print(f\"WARNING: WEIGHT_CONFIG sums to {cfg_sum:.6f} (not 1.0). \"\n",
    "          f\"Effective bucket weights are scaled by 1/{cfg_sum:.6f}.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# WALK-FORWARD CALC\n",
    "# ============================================================\n",
    "\n",
    "def expand_to_calendar_months(group: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure *calendar* month continuity (month-end frequency) for one param combo.\n",
    "    Missing months (no trading activity) are inserted with:\n",
    "      - monthly_return = 0\n",
    "      - monthly_trades = 0\n",
    "\n",
    "    All other identifier columns are forward/back filled.\n",
    "    \"\"\"\n",
    "    g = group.sort_values(\"month_end\").copy()\n",
    "    g[\"month_end\"] = pd.to_datetime(g[\"month_end\"])\n",
    "\n",
    "    # Full calendar month-end index between first and last observed month_end\n",
    "    full_idx = pd.date_range(g[\"month_end\"].min(), g[\"month_end\"].max(), freq=\"ME\")\n",
    "\n",
    "    g = g.set_index(\"month_end\").reindex(full_idx)\n",
    "    g.index.name = \"month_end\"\n",
    "\n",
    "    # Fill missing performance months with zeros\n",
    "    for c in [\"monthly_return\", \"monthly_trades\"]:\n",
    "        if c in g.columns:\n",
    "            g[c] = g[c].fillna(0)\n",
    "\n",
    "    # Forward/back fill identifiers\n",
    "    id_cols = [\n",
    "        \"year\", \"month\", \"iteration_id\", \"strategy_name\",\n",
    "        \"symbol\", \"side\", \"strategy\", \"base_combo_key\", \"param_combo_key\"\n",
    "    ]\n",
    "    for c in id_cols:\n",
    "        if c in g.columns:\n",
    "            g[c] = g[c].ffill().bfill()\n",
    "\n",
    "    # Recompute year/month from calendar month_end (authoritative)\n",
    "    g[\"year\"] = g.index.year\n",
    "    g[\"month\"] = g.index.month\n",
    "\n",
    "    return g.reset_index()\n",
    "\n",
    "\n",
    "def compute_wf_metrics(group):\n",
    "    # Expand to calendar months first (fills non-trading months with 0s)\n",
    "    group = expand_to_calendar_months(group)\n",
    "    group = group.sort_values(\"month_end\").reset_index(drop=True)\n",
    "\n",
    "    if len(group) < WINDOW_MONTHS:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for i in range(WINDOW_MONTHS - 1, len(group)):\n",
    "        window = group.iloc[i - WINDOW_MONTHS + 1 : i + 1]\n",
    "\n",
    "        pnl = window[\"monthly_return\"].values\n",
    "        wmean = np.sum(WEIGHTS_60 * pnl)\n",
    "        wstd  = np.sqrt(np.sum(WEIGHTS_60 * (pnl - wmean) ** 2))\n",
    "\n",
    "        sharpe = (wmean / wstd) * np.sqrt(12) if wstd > 0 else np.nan\n",
    "        trades = window[\"monthly_trades\"].sum()\n",
    "\n",
    "        eval_month = group.loc[i, \"month_end\"]\n",
    "        trade_month = (pd.to_datetime(eval_month) + MonthBegin(1)).normalize()\n",
    "\n",
    "        row = group.loc[i].to_dict()\n",
    "        row.update({\n",
    "            \"eval_month\": eval_month,\n",
    "            \"trade_month\": trade_month,\n",
    "            \"wf_pnl\": wmean,\n",
    "            \"wf_sharpe\": sharpe,\n",
    "            \"wf_trades\": trades\n",
    "        })\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOAD MONTHLY STATS ALL\n",
    "# ============================================================\n",
    "\n",
    "def load_monthly_stats(strategy_folder):\n",
    "    pattern = os.path.join(strategy_folder, MONTHLY_CSV_PATTERN)\n",
    "    files = glob.glob(pattern)\n",
    "\n",
    "    if not files:\n",
    "        print(f\"  No monthly stats file found in: {strategy_folder}\")\n",
    "        return None\n",
    "\n",
    "    f = files[0]\n",
    "    print(f\"  Found monthly stats: {f}\")\n",
    "    return pd.read_csv(f)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PROCESS INDIVIDUAL STRATEGY FOLDER\n",
    "# ============================================================\n",
    "\n",
    "def process_strategy(strategy_folder):\n",
    "    print(f\"\\nProcessing: {strategy_folder}\")\n",
    "\n",
    "    df = load_monthly_stats(strategy_folder)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Mandatory columns\n",
    "    req = [\"year\", \"month\", \"month_end\", \"monthly_return\",\n",
    "           \"monthly_trades\", \"iteration_id\", \"strategy_name\"]\n",
    "\n",
    "    if any(col not in df.columns for col in req):\n",
    "        print(\"  Missing required cols → skipping.\")\n",
    "        return\n",
    "\n",
    "    # Force calendar month-end based on (year, month) so we align to true calendar months\n",
    "    df[\"month_end\"] = pd.to_datetime(dict(year=df[\"year\"], month=df[\"month\"], day=1)) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "    # Extract symbol + side from iteration_id\n",
    "    parts = df[\"iteration_id\"].str.split(\"_\")\n",
    "    df[\"symbol\"] = parts.str[0]\n",
    "    df[\"side\"]   = parts.str[-1]\n",
    "    df[\"strategy\"] = df[\"strategy_name\"]\n",
    "\n",
    "    df[\"base_combo_key\"]  = df[\"symbol\"] + \"|\" + df[\"strategy\"] + \"|\" + df[\"side\"]\n",
    "    df[\"param_combo_key\"] = df[\"iteration_id\"]\n",
    "\n",
    "    # WALK FORWARD per param combo\n",
    "    all_res = []\n",
    "    for key, g in df.groupby(\"param_combo_key\"):\n",
    "        res = compute_wf_metrics(g)\n",
    "        if not res.empty:\n",
    "            res[\"strategy_folder\"] = os.path.basename(strategy_folder)\n",
    "            all_res.append(res)\n",
    "\n",
    "    if not all_res:\n",
    "        print(\"  No combos with ≥60 months → skipping.\")\n",
    "        return\n",
    "\n",
    "    scores_df = pd.concat(all_res)\n",
    "\n",
    "    # Save ALL\n",
    "    path_all = os.path.join(strategy_folder, \"walk_forward_scores_all.csv\")\n",
    "    scores_df.to_csv(path_all, index=False)\n",
    "    print(f\"  Saved ALL scores → {path_all}\")\n",
    "\n",
    "    # Select best param per base combo + eval_month\n",
    "    scores_sorted = scores_df.sort_values(\n",
    "        [\"base_combo_key\", \"eval_month\", \"wf_sharpe\", \"wf_pnl\"],\n",
    "        ascending=[True, True, False, False]\n",
    "    )\n",
    "\n",
    "    best = (\n",
    "        scores_sorted\n",
    "        .groupby([\"base_combo_key\", \"eval_month\"], as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    # Filter Sharpe >= 1\n",
    "    final_sel = best[best[\"wf_sharpe\"] >= MIN_SHARPE].copy()\n",
    "\n",
    "    # Expand base columns\n",
    "    # print(final_sel)\n",
    "    try:\n",
    "        parts = final_sel[\"base_combo_key\"].str.split(\"|\", expand=True)\n",
    "        final_sel[\"symbol\"] = parts[0]\n",
    "        final_sel[\"strategy\"] = parts[1]\n",
    "        final_sel[\"side\"] = parts[2]\n",
    "    \n",
    "        path_sel = os.path.join(strategy_folder, \"walk_forward_selection_sharpe_ge_1.csv\")\n",
    "        final_sel.to_csv(path_sel, index=False)\n",
    "        print(f\"  Saved SELECTION (Sharpe ≥ 1) → {path_sel}\")\n",
    "    except:\n",
    "        print(\"Not selected\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN LOOP\n",
    "# ============================================================\n",
    "\n",
    "for folder in os.listdir(ROOT_DIR):\n",
    "    strat_path = os.path.join(ROOT_DIR, folder)\n",
    "    if os.path.isdir(strat_path):\n",
    "        process_strategy(strat_path)\n",
    "\n",
    "print(\"\\nDONE — Walk-forward monthly selection completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef415fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done: deduplicated selection files for all strategy folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Root folder where all strategy folders live\n",
    "ROOT_DIR = r\"D:\\work\\Trade Analysis\\Monthly Selection\"\n",
    "\n",
    "# Name of the selection file created by Stage 2\n",
    "SELECTION_FILE_NAME = \"walk_forward_selection_sharpe_ge_1.csv\"\n",
    "\n",
    "# Name of the deduplicated output file\n",
    "OUTPUT_FILE_NAME = \"walk_forward_selection_sharpe_ge_1_unique.csv\"\n",
    "\n",
    "\n",
    "def find_column(df, candidates, what_for, required=True):\n",
    "    \"\"\"\n",
    "    Helper to find a column by trying multiple candidate names (case-insensitive).\n",
    "    \"\"\"\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in lower_map:\n",
    "            return lower_map[cand.lower()]\n",
    "    if required:\n",
    "        raise ValueError(\n",
    "            f\"Could not find column for {what_for}. \"\n",
    "            f\"Tried: {candidates}. Available: {list(df.columns)}\"\n",
    "        )\n",
    "    return None\n",
    "\n",
    "\n",
    "def process_strategy_folder(folder_path: str):\n",
    "    sel_path = os.path.join(folder_path, SELECTION_FILE_NAME)\n",
    "    if not os.path.exists(sel_path):\n",
    "        # No selection file in this folder\n",
    "        return\n",
    "\n",
    "    print(f\"Processing selection file: {sel_path}\")\n",
    "    df = pd.read_csv(sel_path)\n",
    "\n",
    "    # --- Detect key columns (be a bit flexible on names) ---\n",
    "    # Strategy column\n",
    "    strategy_col = find_column(\n",
    "        df,\n",
    "        [\"strategy_name\", \"strategy\", \"strat_name\"],\n",
    "        what_for=\"strategy name\"\n",
    "    )\n",
    "    # Side column\n",
    "    side_col = find_column(\n",
    "        df,\n",
    "        [\"side\", \"long_short\", \"direction\"],\n",
    "        what_for=\"side (Long/Short)\"\n",
    "    )\n",
    "    # Month column (prefer trade_month, else eval_month)\n",
    "    if \"trade_month\" in df.columns:\n",
    "        month_col = \"trade_month\"\n",
    "    else:\n",
    "        month_col = find_column(\n",
    "            df,\n",
    "            [\"eval_month\", \"month\", \"month_end\"],\n",
    "            what_for=\"month\"\n",
    "        )\n",
    "\n",
    "    # Ensure month as datetime\n",
    "    df[month_col] = pd.to_datetime(df[month_col])\n",
    "\n",
    "    # Sharpe / PnL columns\n",
    "    sharpe_col = find_column(\n",
    "        df,\n",
    "        [\"wf_sharpe\", \"sharpe\", \"sr\", \"sharp\"],\n",
    "        what_for=\"walk-forward Sharpe\"\n",
    "    )\n",
    "    pnl_col = find_column(\n",
    "        df,\n",
    "        [\"wf_pnl\", \"pnl\", \"monthly_return\", \"return\"],\n",
    "        what_for=\"walk-forward PnL\"\n",
    "    )\n",
    "\n",
    "    # --- Sort so best row comes first in each group ---\n",
    "    df_sorted = df.sort_values(\n",
    "        [strategy_col, side_col, month_col, sharpe_col, pnl_col],\n",
    "        ascending=[True, True, True, False, False]\n",
    "    )\n",
    "\n",
    "    # --- Group by (strategy, side, month) and keep best (first) ---\n",
    "    group_cols = [strategy_col, side_col, month_col]\n",
    "    unique_df = (\n",
    "        df_sorted\n",
    "        .groupby(group_cols, as_index=False)\n",
    "        .first()\n",
    "    )\n",
    "\n",
    "    # --- Save deduplicated file ---\n",
    "    out_path = os.path.join(folder_path, OUTPUT_FILE_NAME)\n",
    "    unique_df.to_csv(out_path, index=False)\n",
    "    print(f\"  Saved unique selection to: {out_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    for name in os.listdir(ROOT_DIR):\n",
    "        strategy_folder = os.path.join(ROOT_DIR, name)\n",
    "        if not os.path.isdir(strategy_folder):\n",
    "            continue\n",
    "        try:\n",
    "            process_strategy_folder(strategy_folder)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {strategy_folder}: {e}\")\n",
    "\n",
    "    print(\"\\nDone: deduplicated selection files for all strategy folders.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b28c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pandas.tseries.offsets import MonthBegin\n",
    "\n",
    "# # ============================================================\n",
    "# # CONFIG – ADJUST THESE PATHS\n",
    "# # ============================================================\n",
    "\n",
    "# # Root folder where all strategy folders (with unique selection files) live\n",
    "# SELECTION_ROOT = r\"E:\\WSG Markets\\Bactesting 1 (Nov 2025) Year\\New Selection tech\\Results\\Monthly Selection\"\n",
    "\n",
    "# # Root folder where BT results (trade level data) live\n",
    "# # Inside: BT_RESULTS_ROOT / <bt_strategy_folder_name> / <trade-level-subfolder> / *<parent_combo_key>*.parquet\n",
    "# BT_RESULTS_ROOT = r\"E:\\WSG Markets\\Bactesting 1 (Nov 2025) Year\\BT results\"\n",
    "\n",
    "# # Master price file (your big CSV with all symbols)\n",
    "# MASTER_PRICE_FILE = r\"E:\\WSG Markets\\Bactesting 1 (Nov 2025) Year\\Raw Data\\Master_Data_5yrs_till_Nov2025_Clean.csv\"\n",
    "\n",
    "# # Name of the *deduped* selection file inside each strategy folder\n",
    "# SELECTION_FILE_NAME = \"walk_forward_selection_sharpe_ge_1_unique.csv\"\n",
    "\n",
    "# # Ex-ante volatility lookback (days)\n",
    "# VOL_LOOKBACK_DAYS = 90\n",
    "\n",
    "# # Target ex-ante portfolio fall (e.g., 20%)\n",
    "# TARGET_FALL = 0.20\n",
    "\n",
    "# # Candidate names for the trade-level-data subfolder under each BT strategy folder\n",
    "# TRADE_SUBFOLDER_CANDIDATES = [\n",
    "#     \"trade level data\",\n",
    "#     \"Trade level data\",\n",
    "#     \"Trade Level Data\",\n",
    "#     \"Trade_Level_Data\",\n",
    "#     \"TradeLevelData\"\n",
    "# ]\n",
    "\n",
    "# # ============================================================\n",
    "# # UTILITIES\n",
    "# # ============================================================\n",
    "\n",
    "# def find_column(df: pd.DataFrame, candidates, what_for: str, required: bool = True):\n",
    "#     \"\"\"Case-insensitive column finder.\"\"\"\n",
    "#     lower_map = {c.lower(): c for c in df.columns}\n",
    "#     for cand in candidates:\n",
    "#         if cand.lower() in lower_map:\n",
    "#             return lower_map[cand.lower()]\n",
    "#     if required:\n",
    "#         raise ValueError(\n",
    "#             f\"Could not find column for {what_for}. \"\n",
    "#             f\"Tried: {candidates}. Available: {list(df.columns)}\"\n",
    "#         )\n",
    "#     return None\n",
    "\n",
    "# # ============================================================\n",
    "# # MASTER PRICE DATA + VOL\n",
    "# # ============================================================\n",
    "\n",
    "# _master_price_df = None\n",
    "# _price_cache = {}  # per-symbol cache\n",
    "\n",
    "\n",
    "# def load_master_price():\n",
    "#     \"\"\"Load and prepare master price file once.\"\"\"\n",
    "#     global _master_price_df\n",
    "#     if _master_price_df is not None:\n",
    "#         return _master_price_df\n",
    "\n",
    "#     df = pd.read_csv(MASTER_PRICE_FILE)\n",
    "\n",
    "#     # Parse date as day-first (per your file)\n",
    "#     df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True)\n",
    "\n",
    "#     # Use mid-close as price\n",
    "#     df[\"mid_close\"] = (df[\"askclose\"] + df[\"bidclose\"]) / 2.0\n",
    "\n",
    "#     # Sort and compute daily returns per org_symbol\n",
    "#     df = df.sort_values([\"org_symbol\", \"date\"]).reset_index(drop=True)\n",
    "#     df[\"daily_return\"] = (\n",
    "#         df.groupby(\"org_symbol\")[\"mid_close\"]\n",
    "#           .pct_change()\n",
    "#           .values\n",
    "#     )\n",
    "\n",
    "#     _master_price_df = df\n",
    "#     return _master_price_df\n",
    "\n",
    "\n",
    "# def load_price_data_for_symbol(symbol: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Return price series for one symbol (org_symbol) with:\n",
    "#       - date\n",
    "#       - mid_close\n",
    "#       - daily_return\n",
    "#     \"\"\"\n",
    "#     if symbol in _price_cache:\n",
    "#         return _price_cache[symbol]\n",
    "\n",
    "#     master = load_master_price()\n",
    "#     sub = master[master[\"org_symbol\"].astype(str) == str(symbol)].copy()\n",
    "\n",
    "#     if sub.empty:\n",
    "#         print(f\"WARNING: No price data found in master for symbol '{symbol}'\")\n",
    "#         _price_cache[symbol] = pd.DataFrame(columns=[\"date\", \"mid_close\", \"daily_return\"])\n",
    "#         return _price_cache[symbol]\n",
    "\n",
    "#     sub = sub[[\"date\", \"mid_close\", \"daily_return\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "#     _price_cache[symbol] = sub\n",
    "#     return sub\n",
    "\n",
    "\n",
    "# def compute_ex_ante_vol(symbol: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Compute 90-day rolling ex-ante volatility for a symbol.\n",
    "#     Returns:\n",
    "#       - date\n",
    "#       - vol_90\n",
    "#     \"\"\"\n",
    "#     px = load_price_data_for_symbol(symbol)\n",
    "#     if px.empty:\n",
    "#         return pd.DataFrame(columns=[\"date\", \"vol_90\"])\n",
    "\n",
    "#     vol_df = px[[\"date\", \"daily_return\"]].copy()\n",
    "#     vol_df[\"vol_90\"] = vol_df[\"daily_return\"].rolling(VOL_LOOKBACK_DAYS).std()\n",
    "#     return vol_df[[\"date\", \"vol_90\"]]\n",
    "\n",
    "# # ============================================================\n",
    "# # TRADE-LEVEL DATA HELPERS\n",
    "# # ============================================================\n",
    "\n",
    "# def find_trade_subfolder(bt_strategy_folder: str):\n",
    "#     \"\"\"\n",
    "#     Find 'trade level data' subfolder under a BT strategy folder:\n",
    "#         BT_RESULTS_ROOT / <bt_strategy_folder_name> / <trade-level-subfolder>\n",
    "#     \"\"\"\n",
    "#     for cand in TRADE_SUBFOLDER_CANDIDATES:\n",
    "#         path = os.path.join(bt_strategy_folder, cand)\n",
    "#         if os.path.isdir(path):\n",
    "#             return path\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def load_trade_level_data_for_selection(bt_strategy_folder_name: str,\n",
    "#                                         parent_combo_key: str,\n",
    "#                                         symbol: str,\n",
    "#                                         trade_month: pd.Timestamp) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Load trade-level parquet for a selection row using:\n",
    "#         BT_RESULTS_ROOT / <bt_strategy_folder_name> / <trade-level-subfolder> / *<parent_combo_key>*.parquet\n",
    "\n",
    "#     Then filter it to trades active in the given trade_month.\n",
    "\n",
    "#     Uses:\n",
    "#       - 'date'          -> entry_date\n",
    "#       - 'Closing Date'  -> exit_date\n",
    "#     \"\"\"\n",
    "#     # 1) Find BT strategy folder\n",
    "#     bt_strategy_folder = os.path.join(BT_RESULTS_ROOT, bt_strategy_folder_name)\n",
    "#     if not os.path.isdir(bt_strategy_folder):\n",
    "#         print(f\"  WARNING: BT strategy folder not found: {bt_strategy_folder}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     # 2) Find 'trade level data' subfolder\n",
    "#     trade_subfolder = find_trade_subfolder(bt_strategy_folder)\n",
    "#     if trade_subfolder is None:\n",
    "#         print(f\"  WARNING: No 'trade level data' subfolder under {bt_strategy_folder}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     # 3) Find parquet file via parent_combo_key\n",
    "#     pattern = os.path.join(trade_subfolder, f\"*{parent_combo_key}*.parquet\")\n",
    "#     matches = [p for p in glob.glob(pattern)]\n",
    "#     if not matches:\n",
    "#         print(f\"  WARNING: No parquet file for parent_combo_key '{parent_combo_key}' in {trade_subfolder}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     parquet_path = matches[0]\n",
    "#     trades = pd.read_parquet(parquet_path)\n",
    "\n",
    "#     # ---- Map entry/exit explicitly for your schema ----\n",
    "#     # entry: 'date', exit: 'Closing Date'\n",
    "#     if \"date\" not in trades.columns or \"Closing Date\" not in trades.columns:\n",
    "#         raise ValueError(\n",
    "#             f\"Expected 'date' and 'Closing Date' in trade-level file {parquet_path}, \"\n",
    "#             f\"found: {list(trades.columns)}\"\n",
    "#         )\n",
    "\n",
    "#     trades[\"entry_date\"] = pd.to_datetime(trades[\"date\"], dayfirst=True, errors=\"coerce\")\n",
    "#     trades[\"exit_date\"] = pd.to_datetime(trades[\"Closing Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "#     # Symbol\n",
    "#     if \"org_symbol\" in trades.columns:\n",
    "#         trades[\"symbol\"] = trades[\"org_symbol\"].astype(str)\n",
    "#     elif \"Symbol\" in trades.columns:\n",
    "#         trades[\"symbol\"] = trades[\"Symbol\"].astype(str)\n",
    "#     elif \"symbol\" in trades.columns:\n",
    "#         trades[\"symbol\"] = trades[\"symbol\"].astype(str)\n",
    "#     else:\n",
    "#         trades[\"symbol\"] = symbol\n",
    "\n",
    "#     # Use parent_combo_key as iteration_id for reference\n",
    "#     trades[\"iteration_id\"] = parent_combo_key\n",
    "\n",
    "#     # Side (we will overwrite from selection anyway)\n",
    "#     if \"side\" in trades.columns:\n",
    "#         trades[\"side\"] = trades[\"side\"].astype(str)\n",
    "#     else:\n",
    "#         trades[\"side\"] = \"Long\"\n",
    "\n",
    "#     # Trade ID\n",
    "#     if \"trade_id\" in trades.columns:\n",
    "#         trades[\"trade_id\"] = trades[\"trade_id\"].astype(str)\n",
    "#     else:\n",
    "#         trades[\"trade_id\"] = (\n",
    "#             trades[\"iteration_id\"].astype(str) + \"_\" +\n",
    "#             trades[\"entry_date\"].dt.strftime(\"%Y%m%d\") + \"_\" +\n",
    "#             trades[\"exit_date\"].dt.strftime(\"%Y%m%d\")\n",
    "#         )\n",
    "\n",
    "#     # Filter to trades active in that trade_month\n",
    "#     month_start = trade_month.replace(day=1)\n",
    "#     month_end = (month_start + MonthBegin(1)) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "#     mask = (trades[\"entry_date\"] <= month_end) & (trades[\"exit_date\"] >= month_start)\n",
    "#     trades = trades.loc[mask].copy()\n",
    "\n",
    "#     return trades\n",
    "\n",
    "# # ============================================================\n",
    "# # LOAD DEDUPED SELECTIONS\n",
    "# # ============================================================\n",
    "\n",
    "# def load_all_selections(selection_root: str) -> pd.DataFrame:\n",
    "#     frames = []\n",
    "#     for name in os.listdir(selection_root):\n",
    "#         strat_path = os.path.join(selection_root, name)\n",
    "#         if not os.path.isdir(strat_path):\n",
    "#             continue\n",
    "\n",
    "#         sel_path = os.path.join(strat_path, SELECTION_FILE_NAME)\n",
    "#         if not os.path.exists(sel_path):\n",
    "#             continue\n",
    "\n",
    "#         df = pd.read_csv(sel_path)\n",
    "#         df[\"selection_folder\"] = strat_path\n",
    "#         frames.append(df)\n",
    "\n",
    "#     if not frames:\n",
    "#         print(\"No deduped selection files found.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     sel = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "#     # Strategy folder name for BT results (column S)\n",
    "#     bt_strategy_folder_col = find_column(\n",
    "#         sel,\n",
    "#         [\"strategy_folder\", \"strategy_folder_name\", \"bt_strategy_folder\", \"bt_folder\"],\n",
    "#         what_for=\"BT strategy folder name\"\n",
    "#     )\n",
    "\n",
    "#     # Instrument symbol\n",
    "#     symbol_col = find_column(\n",
    "#         sel,\n",
    "#         [\"symbol\", \"org_symbol\", \"ticker\"],\n",
    "#         what_for=\"symbol (instrument)\"\n",
    "#     )\n",
    "\n",
    "#     # Side\n",
    "#     side_col = find_column(\n",
    "#         sel,\n",
    "#         [\"side\", \"long_short\", \"direction\"],\n",
    "#         what_for=\"side (Long/Short)\"\n",
    "#     )\n",
    "\n",
    "#     # Trade month\n",
    "#     trade_month_col = find_column(\n",
    "#         sel,\n",
    "#         [\"trade_month\"],\n",
    "#         what_for=\"trade month\"\n",
    "#     )\n",
    "#     sel[\"trade_month\"] = pd.to_datetime(sel[trade_month_col])\n",
    "\n",
    "#     # Parent combo key\n",
    "#     parent_combo_col = find_column(\n",
    "#         sel,\n",
    "#         [\"parent_combo_key\", \"param_combo_key\", \"combination_key\", \"combination\"],\n",
    "#         what_for=\"parent combo key\"\n",
    "#     )\n",
    "\n",
    "#     # Store column names once in the DataFrame\n",
    "#     sel[\"_bt_strategy_folder_col\"] = bt_strategy_folder_col\n",
    "#     sel[\"_symbol_col\"] = symbol_col\n",
    "#     sel[\"_side_col\"] = side_col\n",
    "#     sel[\"_parent_combo_col\"] = parent_combo_col\n",
    "#     sel[\"_trade_month_col\"] = trade_month_col\n",
    "\n",
    "#     return sel\n",
    "\n",
    "# # ============================================================\n",
    "# # EXPAND TRADES TO DAILY USING MASTER PRICE\n",
    "# # ============================================================\n",
    "\n",
    "# def expand_trades_to_daily(trades_df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     For each trade, expand into daily rows using price data.\n",
    "\n",
    "#     trades_df must have:\n",
    "#       - symbol (org_symbol; e.g. ACN)\n",
    "#       - iteration_id (we use parent_combo_key here)\n",
    "#       - entry_date\n",
    "#       - exit_date\n",
    "#       - side (Long / Short)\n",
    "#     \"\"\"\n",
    "#     if trades_df.empty:\n",
    "#         return pd.DataFrame(columns=[\"date\", \"symbol\", \"iteration_id\", \"trade_id\", \"trade_daily_return\"])\n",
    "\n",
    "#     all_days = []\n",
    "\n",
    "#     for (symbol, iteration_id), group in trades_df.groupby([\"symbol\", \"iteration_id\"]):\n",
    "#         price_df = load_price_data_for_symbol(symbol)\n",
    "#         if price_df.empty:\n",
    "#             continue\n",
    "\n",
    "#         for _, tr in group.iterrows():\n",
    "#             entry = tr[\"entry_date\"]\n",
    "#             exit_ = tr[\"exit_date\"]\n",
    "\n",
    "#             mask = (price_df[\"date\"] >= entry) & (price_df[\"date\"] <= exit_)\n",
    "#             px = price_df.loc[mask, [\"date\", \"daily_return\"]].copy()\n",
    "#             if px.empty:\n",
    "#                 continue\n",
    "\n",
    "#             side = str(tr[\"side\"]).lower()\n",
    "#             sign = 1.0 if \"long\" in side else -1.0\n",
    "#             px[\"trade_daily_return\"] = sign * px[\"daily_return\"]\n",
    "\n",
    "#             px[\"symbol\"] = symbol\n",
    "#             px[\"iteration_id\"] = iteration_id\n",
    "#             px[\"trade_id\"] = tr[\"trade_id\"]\n",
    "#             all_days.append(px)\n",
    "\n",
    "#     if not all_days:\n",
    "#         return pd.DataFrame(columns=[\"date\", \"symbol\", \"iteration_id\", \"trade_id\", \"trade_daily_return\"])\n",
    "\n",
    "#     daily_trades = pd.concat(all_days, ignore_index=True)\n",
    "#     return daily_trades\n",
    "\n",
    "# # ============================================================\n",
    "# # BUILD PORTFOLIO (1/vol SIZING + 20% TARGET FALL)\n",
    "# # ============================================================\n",
    "\n",
    "# def build_portfolio_pnl(daily_trades: pd.DataFrame) -> pd.DataFrame:\n",
    "#     if daily_trades.empty:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     daily_trades[\"date\"] = pd.to_datetime(daily_trades[\"date\"])\n",
    "\n",
    "#     # Compute vol_90 for each symbol\n",
    "#     vol_frames = []\n",
    "#     for symbol in daily_trades[\"symbol\"].unique():\n",
    "#         vol_df = compute_ex_ante_vol(symbol)\n",
    "#         vol_df[\"symbol\"] = symbol\n",
    "#         vol_frames.append(vol_df)\n",
    "\n",
    "#     if not vol_frames:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     vol_all = pd.concat(vol_frames, ignore_index=True)\n",
    "\n",
    "#     df = daily_trades.merge(vol_all, on=[\"symbol\", \"date\"], how=\"left\")\n",
    "#     df = df.dropna(subset=[\"vol_90\"]).copy()\n",
    "\n",
    "#     portfolio_rows = []\n",
    "#     for date, grp in df.groupby(\"date\"):\n",
    "#         g = grp.copy()\n",
    "#         g[\"base_size\"] = 1.0 / g[\"vol_90\"]\n",
    "#         g.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#         g = g.dropna(subset=[\"base_size\"])\n",
    "\n",
    "#         if g.empty:\n",
    "#             continue\n",
    "\n",
    "#         n_active = len(g)\n",
    "#         k = TARGET_FALL / n_active  # scaling so total ex-ante fall ≈ 20%\n",
    "\n",
    "#         g[\"position_size\"] = k * g[\"base_size\"]\n",
    "#         g[\"pnl_contribution\"] = g[\"position_size\"] * g[\"trade_daily_return\"]\n",
    "\n",
    "#         portfolio_rows.append({\n",
    "#             \"date\": date,\n",
    "#             \"portfolio_return\": g[\"pnl_contribution\"].sum(),\n",
    "#             \"num_trades_active\": g[\"trade_id\"].nunique(),\n",
    "#             \"num_combos_active\": g[\"iteration_id\"].nunique(),\n",
    "#             \"avg_vol_90\": g[\"vol_90\"].mean(),\n",
    "#             \"avg_position_size\": g[\"position_size\"].mean()\n",
    "#         })\n",
    "\n",
    "#     if not portfolio_rows:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     port_df = pd.DataFrame(portfolio_rows).sort_values(\"date\").reset_index(drop=True)\n",
    "#     return port_df\n",
    "\n",
    "# # ============================================================\n",
    "# # MAIN\n",
    "# # ============================================================\n",
    "\n",
    "# def main():\n",
    "#     # 1) Load deduped selections\n",
    "#     selections = load_all_selections(SELECTION_ROOT)\n",
    "#     if selections.empty:\n",
    "#         print(\"No selections found. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Total selected rows (after dedupe): {len(selections)}\")\n",
    "\n",
    "#     # Resolve actual column names from stored markers\n",
    "#     bt_strategy_folder_col = selections[\"_bt_strategy_folder_col\"].iloc[0]\n",
    "#     symbol_col = selections[\"_symbol_col\"].iloc[0]\n",
    "#     side_col = selections[\"_side_col\"].iloc[0]\n",
    "#     parent_combo_col = selections[\"_parent_combo_col\"].iloc[0]\n",
    "#     trade_month_col = selections[\"_trade_month_col\"].iloc[0]\n",
    "\n",
    "#     trade_frames = []\n",
    "\n",
    "#     # 2) Load trade-level data for all selections\n",
    "#     for _, row in selections.iterrows():\n",
    "#         bt_strategy_folder_name = row[bt_strategy_folder_col]\n",
    "#         symbol = row[symbol_col]\n",
    "#         side = row[side_col]\n",
    "#         parent_combo_key = row[parent_combo_col]\n",
    "#         trade_month = row[trade_month_col]\n",
    "\n",
    "#         trades = load_trade_level_data_for_selection(\n",
    "#             bt_strategy_folder_name=bt_strategy_folder_name,\n",
    "#             parent_combo_key=parent_combo_key,\n",
    "#             symbol=symbol,\n",
    "#             trade_month=trade_month\n",
    "#         )\n",
    "#         if trades.empty:\n",
    "#             continue\n",
    "\n",
    "#         # Ensure side matches selection side\n",
    "#         trades[\"side\"] = side\n",
    "#         trade_frames.append(trades)\n",
    "\n",
    "#     if not trade_frames:\n",
    "#         print(\"No trade-level data loaded. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     all_trades = pd.concat(trade_frames, ignore_index=True)\n",
    "#     print(f\"Total trades loaded (rows): {len(all_trades)}\")\n",
    "\n",
    "#     # 3) Expand to daily\n",
    "#     daily_trades = expand_trades_to_daily(all_trades)\n",
    "#     if daily_trades.empty:\n",
    "#         print(\"No daily trades generated. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Total daily trade rows: {len(daily_trades)}\")\n",
    "\n",
    "#     # 4) Build portfolio\n",
    "#     portfolio_df = build_portfolio_pnl(daily_trades)\n",
    "#     if portfolio_df.empty:\n",
    "#         print(\"No portfolio P&L created. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Portfolio rows: {len(portfolio_df)}\")\n",
    "\n",
    "#     # 5) Save outputs\n",
    "#     out_portfolio = os.path.join(SELECTION_ROOT, \"portfolio_daily_pnl.csv\")\n",
    "#     portfolio_df.to_csv(out_portfolio, index=False)\n",
    "#     print(f\"Saved portfolio daily P&L → {out_portfolio}\")\n",
    "\n",
    "#     # Some basic counts\n",
    "#     print(\"\\nSUMMARY:\")\n",
    "#     print(f\"  Unique symbols: {all_trades['symbol'].nunique()}\")\n",
    "#     print(f\"  Unique iteration_ids (parent_combo_keys): {all_trades['iteration_id'].nunique()}\")\n",
    "#     print(f\"  Unique trades: {all_trades['trade_id'].nunique()}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "796c3d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # =========================================\n",
    "# # CONFIG – EDIT THESE 3 PATHS\n",
    "# # =========================================\n",
    "\n",
    "# # 1) Trade-level file for ONE strategy (the file you shared)\n",
    "# TRADE_FILE = r\"E:\\WSG Markets\\Backtesting V2 Nov 2025\\Monthly Selection\\Stairs strategy\\selected_trade_level_all_months.csv\"\n",
    "\n",
    "# # 2) Raw daily price file\n",
    "# RAW_PRICE_FILE = r\"E:\\WSG Markets\\Backtesting V2 Nov 2025\\Raw Data\\Master_Data till 31 0ct 2025.csv\"\n",
    "\n",
    "# # 3) Output file for the daily expansion of this strategy\n",
    "# OUTPUT_FILE = r\"E:\\WSG Markets\\Backtesting V2 Nov 2025\\Portfolios-Summary\\TradeDaily_OneStrategy_Test.csv\"\n",
    "\n",
    "# # Capital used per trade to compute quantity\n",
    "# PER_TRADE_CAPITAL = 100000.0\n",
    "\n",
    "# # =========================================\n",
    "# # HELPERS\n",
    "# # =========================================\n",
    "\n",
    "# _price_master = None\n",
    "# _price_cache = {}\n",
    "\n",
    "# def log(msg):\n",
    "#     print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n",
    "\n",
    "# def load_price_master():\n",
    "#     \"\"\"\n",
    "#     Load raw price file once and prepare for mapping:\n",
    "#     trade['Currency']  <->  price['Symbol'] (capital S).\n",
    "#     \"\"\"\n",
    "#     global _price_master\n",
    "#     if _price_master is not None:\n",
    "#         return _price_master\n",
    "\n",
    "#     log(f\"Loading raw price file: {RAW_PRICE_FILE}\")\n",
    "#     df = pd.read_csv(RAW_PRICE_FILE)\n",
    "\n",
    "#     if \"date\" not in df.columns:\n",
    "#         raise ValueError(\"Raw price file must have a 'date' column\")\n",
    "\n",
    "#     df[\"date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "#     if \"Symbol\" not in df.columns:\n",
    "#         raise ValueError(\"Raw price file must have a 'Symbol' column (capital S)\")\n",
    "\n",
    "#     # Mid close as daily price\n",
    "#     if {\"askclose\", \"bidclose\"}.issubset(df.columns):\n",
    "#         df[\"price\"] = (df[\"askclose\"] + df[\"bidclose\"]) / 2.0\n",
    "#     else:\n",
    "#         raise ValueError(\"Raw price file must contain 'askclose' and 'bidclose' columns\")\n",
    "\n",
    "#     df = df.sort_values([\"Symbol\", \"date\"]).reset_index(drop=True)\n",
    "#     _price_master = df\n",
    "#     log(\"Raw price file loaded.\")\n",
    "#     return _price_master\n",
    "\n",
    "# def get_price_series(currency_value: str) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Return price series (date, price) for given trade Currency.\n",
    "#     Mapping: Currency (trade) <-> Symbol (price).\n",
    "#     \"\"\"\n",
    "#     global _price_cache\n",
    "#     if currency_value in _price_cache:\n",
    "#         return _price_cache[currency_value]\n",
    "\n",
    "#     master = load_price_master()\n",
    "#     sub = master[master[\"Symbol\"].astype(str) == str(currency_value)].copy()\n",
    "#     if sub.empty:\n",
    "#         log(f\"  WARNING: No price series found for Currency='{currency_value}'\")\n",
    "#         _price_cache[currency_value] = pd.DataFrame()\n",
    "#         return _price_cache[currency_value]\n",
    "\n",
    "#     sub = sub[[\"date\", \"price\"]].sort_values(\"date\").reset_index(drop=True)\n",
    "#     _price_cache[currency_value] = sub\n",
    "#     return sub\n",
    "\n",
    "# # =========================================\n",
    "# # LOAD TRADE LEVEL (ONE STRATEGY)\n",
    "# # =========================================\n",
    "\n",
    "# def load_trades_one_strategy() -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Load selected_trade_level_all_months for one strategy and prepare:\n",
    "#       - entry_date, exit_date\n",
    "#       - entry_price\n",
    "#       - Currency, Strategy, Timeframe, Condition, side\n",
    "#     \"\"\"\n",
    "#     log(f\"Loading trade-level file: {TRADE_FILE}\")\n",
    "#     df = pd.read_csv(TRADE_FILE)\n",
    "\n",
    "#     required = [\"date\", \"Closing Date\", \"Currency\", \"Strategy\",\n",
    "#                 \"Timeframe\", \"Condition\", \"Trading Price Point\"]\n",
    "#     for c in required:\n",
    "#         if c not in df.columns:\n",
    "#             raise ValueError(f\"Missing required column '{c}' in trade file.\")\n",
    "\n",
    "#     # Parse dates from trade file\n",
    "#     df[\"entry_date\"] = pd.to_datetime(df[\"date\"], dayfirst=True, errors=\"coerce\")\n",
    "#     df[\"exit_date\"] = pd.to_datetime(df[\"Closing Date\"], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "#     # Core fields\n",
    "#     df[\"Currency\"] = df[\"Currency\"].astype(str)\n",
    "#     df[\"Strategy\"] = df[\"Strategy\"].astype(str)\n",
    "#     df[\"Timeframe\"] = df[\"Timeframe\"].astype(str)\n",
    "#     df[\"Condition\"] = df[\"Condition\"].astype(str)\n",
    "#     df[\"entry_price\"] = df[\"Trading Price Point\"].astype(float)\n",
    "\n",
    "#     # side (default Long if not present)\n",
    "#     if \"side\" in df.columns:\n",
    "#         df[\"side\"] = df[\"side\"].astype(str)\n",
    "#     else:\n",
    "#         df[\"side\"] = \"Long\"\n",
    "\n",
    "#     # Drop incomplete rows\n",
    "#     df = df.dropna(subset=[\"entry_date\", \"exit_date\", \"entry_price\"])\n",
    "#     df = df.sort_values([\"Currency\", \"entry_date\", \"exit_date\"]).reset_index(drop=True)\n",
    "\n",
    "#     # Create TradeID (0,1,2,...) for this strategy\n",
    "#     df[\"TradeID\"] = np.arange(len(df), dtype=int)\n",
    "\n",
    "#     log(f\"Loaded {len(df)} trades. \"\n",
    "#         f\"Earliest entry: {df['entry_date'].min().date()}, \"\n",
    "#         f\"latest entry: {df['entry_date'].max().date()}\")\n",
    "#     return df\n",
    "\n",
    "# # =========================================\n",
    "# # EXPAND TRADES TO DAILY\n",
    "# # =========================================\n",
    "\n",
    "# def expand_trades_to_daily(trades: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     For each trade in this strategy, create daily rows:\n",
    "\n",
    "#       - Dates: each trading day between entry_date and exit_date (inclusive)\n",
    "#       - One row per TradeID per date.\n",
    "#       - All dates are strictly >= entry_date and <= exit_date.\n",
    "#     \"\"\"\n",
    "#     if trades.empty:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     rows = []\n",
    "#     t0 = time.time()\n",
    "#     n_trades = len(trades)\n",
    "#     log(f\"Expanding {n_trades} trades to daily rows...\")\n",
    "\n",
    "#     for i, tr in trades.iterrows():\n",
    "#         if (i + 1) % 50 == 0 or i == n_trades - 1:\n",
    "#             log(f\"  Trade {i+1}/{n_trades} ({tr['Currency']} {tr['Strategy']})\")\n",
    "\n",
    "#         ccy = tr[\"Currency\"]\n",
    "#         px = get_price_series(ccy)\n",
    "#         if px.empty:\n",
    "#             continue\n",
    "\n",
    "#         entry_date = tr[\"entry_date\"]\n",
    "#         exit_date = tr[\"exit_date\"]\n",
    "\n",
    "#         # STRICT filter between entry and exit dates (inclusive)\n",
    "#         mask = (px[\"date\"] >= entry_date) & (px[\"date\"] <= exit_date)\n",
    "#         sub = px.loc[mask].copy()\n",
    "#         if sub.empty:\n",
    "#             # No prices inside this window → skip\n",
    "#             continue\n",
    "\n",
    "#         sub = sub.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "#         entry_price = tr[\"entry_price\"]\n",
    "#         trade_capital = PER_TRADE_CAPITAL\n",
    "#         qty = trade_capital / entry_price\n",
    "\n",
    "#         side_str = tr[\"side\"].lower()\n",
    "#         direction = 1 if \"long\" in side_str else -1\n",
    "\n",
    "#         # Price & price_prev\n",
    "#         sub[\"price\"] = sub[\"price\"].astype(float)\n",
    "#         sub[\"price_prev\"] = sub[\"price\"].shift(1)\n",
    "\n",
    "#         # daily_pnl: Qty * (price - price_prev) * Direction; first day pnl = 0\n",
    "#         sub[\"daily_pnl\"] = qty * (sub[\"price\"] - sub[\"price_prev\"].fillna(sub[\"price\"])) * direction\n",
    "#         sub.loc[sub.index[0], \"daily_pnl\"] = 0.0\n",
    "\n",
    "#         sub[\"trade_capital\"] = trade_capital\n",
    "#         sub[\"daily_return_vs_trade_capital\"] = sub[\"daily_pnl\"] / trade_capital\n",
    "\n",
    "#         # For reference: position value & exposure vs trade capital (like your sample)\n",
    "#         sub[\"position_value\"] = qty * sub[\"price\"] * direction\n",
    "#         sub[\"exposure_vs_trade_capital\"] = sub[\"position_value\"].abs() / trade_capital\n",
    "#         sub[\"exposure_vs_portfolio\"] = sub[\"exposure_vs_trade_capital\"]  # can be changed later\n",
    "\n",
    "#         # Final_PnL = sum over this TradeID\n",
    "#         final_pnl = sub[\"daily_pnl\"].sum()\n",
    "#         sub[\"Final_PnL\"] = final_pnl\n",
    "\n",
    "#         # Repeat opening/closing dates and metadata\n",
    "#         sub[\"Opening Date\"] = entry_date\n",
    "#         sub[\"Closing Date\"] = exit_date\n",
    "#         sub[\"Qty\"] = qty\n",
    "#         sub[\"Direction\"] = direction\n",
    "#         sub[\"TradeID\"] = tr[\"TradeID\"]\n",
    "#         sub[\"Strategy\"] = tr[\"Strategy\"]\n",
    "#         sub[\"Currency\"] = tr[\"Currency\"]\n",
    "#         sub[\"Timeframe\"] = tr[\"Timeframe\"]\n",
    "#         sub[\"Condition\"] = tr[\"Condition\"]\n",
    "\n",
    "#         # Combination as in your sample:\n",
    "#         # e.g. \"Stairs strategy_ACN_USD_Long_D1\"\n",
    "#         ccy_clean = tr[\"Currency\"].replace(\"/\", \"_\")\n",
    "#         combo = f\"{tr['Strategy']}_{ccy_clean}_{tr['Condition']}_{tr['Timeframe']}\"\n",
    "#         sub[\"Combination\"] = combo\n",
    "\n",
    "#         # Data_Type constant for now (you can change)\n",
    "#         sub[\"Data_Type\"] = \"SL_Updated\"\n",
    "\n",
    "#         rows.append(sub)\n",
    "\n",
    "#     if not rows:\n",
    "#         log(\"No daily rows were generated.\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "#     daily = pd.concat(rows, ignore_index=True)\n",
    "#     daily = daily.sort_values([\"TradeID\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "#     # Order columns similar to your TradeDaily_All sample\n",
    "#     cols_order = [\n",
    "#         \"date\", \"price\", \"TradeID\", \"Strategy\", \"Currency\", \"Timeframe\",\n",
    "#         \"Condition\", \"Combination\", \"Data_Type\", \"Qty\", \"Direction\",\n",
    "#         \"trade_capital\", \"Opening Date\", \"Closing Date\", \"Final_PnL\",\n",
    "#         \"price_prev\", \"position_value\", \"exposure_vs_trade_capital\",\n",
    "#         \"exposure_vs_portfolio\", \"daily_pnl\", \"daily_return_vs_trade_capital\",\n",
    "#     ]\n",
    "#     # Keep extra columns at the end if any\n",
    "#     cols_order = [c for c in cols_order if c in daily.columns] + \\\n",
    "#                  [c for c in daily.columns if c not in cols_order]\n",
    "\n",
    "#     daily = daily[cols_order]\n",
    "#     return daily\n",
    "\n",
    "# # =========================================\n",
    "# # MAIN\n",
    "# # =========================================\n",
    "\n",
    "# def main():\n",
    "#     os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "#     trades = load_trades_one_strategy()\n",
    "#     if trades.empty:\n",
    "#         log(\"No trades loaded – check TRADE_FILE path / content.\")\n",
    "#         return\n",
    "\n",
    "#     daily = expand_trades_to_daily(trades)\n",
    "#     log(f\"Daily rows generated: {len(daily)}\")\n",
    "\n",
    "#     daily.to_csv(OUTPUT_FILE, index=False)\n",
    "#     log(f\"Saved daily expansion for this strategy → {OUTPUT_FILE}\")\n",
    "\n",
    "#     # Quick sanity check on dates:\n",
    "#     if not daily.empty:\n",
    "#         log(f\"Earliest daily date in output: {daily['date'].min()}\")\n",
    "#         log(f\"Latest daily date in output:   {daily['date'].max()}\")\n",
    "#         log(f\"Earliest trade entry date:     {trades['entry_date'].min()}\")\n",
    "#         log(f\"Latest trade entry date:       {trades['entry_date'].max()}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ef02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf323b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
