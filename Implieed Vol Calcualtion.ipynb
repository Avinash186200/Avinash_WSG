{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffcf16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ba6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2606fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DNFileVault Windows Downloader ---\n",
      "Saving files to: D:\\work\\Trade Analysis\\Options Data\n",
      "Step 1: Logging in as ernest@predictnow.ai...\n",
      "Login successful!\n",
      "\n",
      "--- Checking Purchases ---\n",
      "No purchases found.\n",
      "\n",
      "--- Checking Groups ---\n",
      "Created folder: D:\\work\\Trade Analysis\\Options Data\\Groups\\2 - eodLevel3\n",
      "Downloading: L3_20260123.zip\n",
      "Downloading: L3_20260122.zip\n",
      "Downloading: L3_20260121.zip\n",
      "Downloading: L3_20260120.zip\n",
      "Downloading: L3_20260116.zip\n",
      "Downloading: L3_20260115.zip\n",
      "Downloading: L3_20260114.zip\n",
      "Downloading: L3_20260113.zip\n",
      "Downloading: L3_20260112.zip\n",
      "Downloading: L3_20260109.zip\n",
      "Downloading: L3_20260108.zip\n",
      "Downloading: L3_20260107.zip\n",
      "Downloading: L3_20260106.zip\n",
      "Downloading: L3_20260105.zip\n",
      "Downloading: L3_20260102.zip\n",
      "Downloading: L3_20251231.zip\n",
      "Downloading: L3_20251230.zip\n",
      "Downloading: L3_20251229.zip\n",
      "Downloading: L3_20251226.zip\n",
      "Downloading: L3_20251224.zip\n",
      "Downloading: L3_20251223.zip\n",
      "Downloading: L3_20251222.zip\n",
      "Downloading: L3_20251219.zip\n",
      "Downloading: L3_20251218.zip\n",
      "Downloading: L3_20251217.zip\n",
      "Downloading: L3_20251216.zip\n",
      "Downloading: L3_20251215.zip\n",
      "Downloading: L3_20251212.zip\n",
      "Downloading: L3_20251211.zip\n",
      "Downloading: L3_20251210.zip\n",
      "Downloading: L3_20251209.zip\n",
      "Downloading: L3_20251208.zip\n",
      "Downloading: L3_20251205.zip\n",
      "Downloading: L3_20251204.zip\n",
      "Downloading: L3_20251203.zip\n",
      "Downloading: L3_20251202.zip\n",
      "Downloading: L3_20251201.zip\n",
      "\n",
      "------------------------------------------------------------------\n",
      "All done!\n",
      "Check your files in: D:\\work\\Trade Analysis\\Options Data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DNFileVault Downloader for Windows (Python)\n",
    "------------------------------------------------------------------\n",
    "This script downloads ALL files from your DNFileVault account:\n",
    "1. All Purchases\n",
    "2. All Groups\n",
    "\n",
    "\n",
    "It is designed for Windows users and includes explanations for each part.\n",
    "\n",
    "\n",
    "BEFORE YOU RUN THIS:\n",
    "1. You need Python installed.\n",
    "2. You need the 'requests' library.\n",
    "   Open Command Prompt (cmd.exe) or PowerShell and run:\n",
    "       pip install requests\n",
    "\n",
    "\n",
    "HOW TO CONFIGURE:\n",
    "Scroll down to the \"CONFIGURATION\" section below and enter your\n",
    "email, password, and where you want files to go.\n",
    "\n",
    "\n",
    "TROUBLESHOOTING TIMEOUTS:\n",
    "DNFileVault has anti-scanner protection. If requests look \"bot-like\" \n",
    "(e.g., default python-requests/curl User-Agent), the API will \n",
    "intentionally slow down responses (throttling). \n",
    "We fix this by setting a custom User-Agent and using longer timeouts.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# We use the 'requests' library to talk to the internet (API).\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    print(\"ERROR: The 'requests' library is not installed.\")\n",
    "    print(\"Please open PowerShell or Command Prompt and run:\")\n",
    "    print(\"    pip install requests\")\n",
    "    print(\"\")\n",
    "    input(\"Press Enter to exit...\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "# Enter your DNFileVault email and password here.\n",
    "# NOTE: Keep the quotes \"\" around your text.\n",
    "\n",
    "\n",
    "EMAIL = \"ernest@predictnow.ai\"\n",
    "PASSWORD = \"saby@123\"\n",
    "\n",
    "\n",
    "# Where should the files be saved?\n",
    "# On Windows, you can use paths like r\"C:\\Downloads\\DNFileVault\"\n",
    "# The 'r' before the quote tells Python to treat backslashes `\\` normally.\n",
    "OUTPUT_FOLDER = r\"D:\\work\\Trade Analysis\\Options Data\"\n",
    "\n",
    "\n",
    "# Filter settings (Optional)\n",
    "# Set DAYS_TO_CHECK to a number (e.g., 1) to only download the newest files (top N).\n",
    "# For example, set to 1 to only download the very latest file.\n",
    "# Set to None to download EVERYTHING.\n",
    "DAYS_TO_CHECK = None\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Constants for the API\n",
    "BASE_URL = \"https://api.dnfilevault.com\"\n",
    "# A custom User-Agent identifies your script and prevents the API from throttling you.\n",
    "USER_AGENT = \"DNFileVaultClient/1.0 (+support@deltaneutral.com)\"\n",
    "\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"\n",
    "    Cleans up a filename so Windows is happy.\n",
    "    Windows doesn't like characters like: < > : \" / \\ | ? *\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return \"unnamed_file\"\n",
    "    # Replace bad characters with an underscore\n",
    "    clean = re.sub(r'[<>:\"/\\\\\\\\|?*]', \"_\", str(name))\n",
    "    return clean.strip() or \"unnamed_file\"\n",
    "\n",
    "\n",
    "def ensure_folder_exists(folder_path):\n",
    "    \"\"\"\n",
    "    Checks if a folder exists. If not, it creates it.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        try:\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Created folder: {folder_path}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating folder {folder_path}: {e}\")\n",
    "\n",
    "\n",
    "def login_to_api(session):\n",
    "    \"\"\"\n",
    "    Logs in using the EMAIL and PASSWORD variables.\n",
    "    Returns the 'token' needed for downloading files.\n",
    "    \"\"\"\n",
    "    print(f\"Step 1: Logging in as {EMAIL}...\")\n",
    "    \n",
    "    login_url = f\"{BASE_URL}/auth/login\"\n",
    "    payload = {\n",
    "        \"email\": EMAIL,\n",
    "        \"password\": PASSWORD\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send a POST request to the login page (using 60s timeout for stability)\n",
    "        response = session.post(login_url, json=payload, timeout=60)\n",
    "        \n",
    "        # Check if login worked (Status code 200 means OK)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            token = data.get(\"token\")\n",
    "            print(\"Login successful!\")\n",
    "            return token\n",
    "        elif response.status_code == 401:\n",
    "            print(\"Login failed: Incorrect email or password. Please check your EMAIL and PASSWORD settings.\")\n",
    "        else:\n",
    "            print(f\"Login failed: Server returned error {response.status_code}\")\n",
    "            print(response.text)\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Login failed: ERROR - No internet connection or DNS failure. (Could not reach api.dnfilevault.com)\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Login failed: ERROR - The request timed out. Your connection might be slow or the server is busy.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Login failed: ERROR - A network problem occurred: {e}\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def download_file(session, file_info, save_directory):\n",
    "    \"\"\"\n",
    "    Downloads a single file to the save_directory.\n",
    "    Checks if file already exists to avoid re-downloading.\n",
    "    \"\"\"\n",
    "    filename = file_info.get(\"uuid_filename\")\n",
    "    display_name = file_info.get(\"display_name\") or filename\n",
    "    \n",
    "    # Clean the name for Windows\n",
    "    safe_name = sanitize_filename(display_name)\n",
    "    full_save_path = os.path.join(save_directory, safe_name)\n",
    "    \n",
    "    # Check if we already have it\n",
    "    if os.path.exists(full_save_path):\n",
    "        # File exists - skip instructions\n",
    "        # print(f\"Skipping (already exists): {safe_name}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    download_url = f\"{BASE_URL}/download/{filename}\"\n",
    "    \n",
    "    print(f\"Downloading: {safe_name}\")\n",
    "    \n",
    "    try:\n",
    "        # stream=True allows us to download large files without using all RAM.\n",
    "        # We use a 300s (5 minute) timeout for downloads to handle large ZIP files.\n",
    "        with session.get(download_url, stream=True, timeout=300) as r:\n",
    "            if r.status_code == 200:\n",
    "                # Write to a temporary file first\n",
    "                temp_path = full_save_path + \".tmp\"\n",
    "                with open(temp_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=1024*1024): # 1 MB chunks\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                \n",
    "                # Rename temp file to final name\n",
    "                if os.path.exists(full_save_path):\n",
    "                    os.remove(full_save_path)\n",
    "                os.rename(temp_path, full_save_path)\n",
    "            else:\n",
    "                print(f\"Failed to download {safe_name}. Status: {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {safe_name}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"--- DNFileVault Windows Downloader ---\")\n",
    "    print(f\"Saving files to: {OUTPUT_FOLDER}\")\n",
    "    \n",
    "    # 1. Start a web session\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\"User-Agent\": USER_AGENT})\n",
    "    \n",
    "    # 2. Login\n",
    "    token = login_to_api(session)\n",
    "    if not token:\n",
    "        # If no token, we can't continue\n",
    "        print(\"Stopping script due to login failure.\")\n",
    "        input(\"Press Enter to exit...\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Add the token to all future requests\n",
    "    session.headers.update({\"Authorization\": f\"Bearer {token}\"})\n",
    "\n",
    "\n",
    "    # Prepare the output folder\n",
    "    ensure_folder_exists(OUTPUT_FOLDER)\n",
    "\n",
    "\n",
    "    # 3. Download Purchases\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n--- Checking Purchases ---\")\n",
    "    try:\n",
    "        resp = session.get(f\"{BASE_URL}/purchases\", timeout=60)\n",
    "        data = resp.json()\n",
    "        purchases = data.get(\"purchases\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking purchases: {e}\")\n",
    "        purchases = []\n",
    "        \n",
    "    if not purchases:\n",
    "        print(\"No purchases found.\")\n",
    "\n",
    "\n",
    "    for p in purchases:\n",
    "        # Create a folder for each product\n",
    "        folder_name = f\"{p['id']} - {p.get('product_name', 'Unknown')}\"\n",
    "        safe_folder_name = sanitize_filename(folder_name)\n",
    "        product_path = os.path.join(OUTPUT_FOLDER, \"Purchases\", safe_folder_name)\n",
    "        ensure_folder_exists(product_path)\n",
    "        \n",
    "        # Get files for this purchase\n",
    "        try:\n",
    "            files_resp = session.get(f\"{BASE_URL}/purchases/{p['id']}/files\", timeout=60)\n",
    "            files = files_resp.json().get(\"files\", [])\n",
    "            \n",
    "            # Determine which files to download\n",
    "            files_to_download = files\n",
    "            if DAYS_TO_CHECK is not None:\n",
    "                # If we have a limit, take only the first N files\n",
    "                # (Assumes the API returns them sorted newly created -> older)\n",
    "                files_to_download = files[:DAYS_TO_CHECK]\n",
    "\n",
    "\n",
    "            for f in files_to_download:\n",
    "                download_file(session, f, product_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting files for purchase {p['id']}: {e}\")\n",
    "\n",
    "\n",
    "    # 4. Download Groups\n",
    "    # --------------------------------------------------------------------------\n",
    "    print(\"\\n--- Checking Groups ---\")\n",
    "    try:\n",
    "        resp = session.get(f\"{BASE_URL}/groups\", timeout=60)\n",
    "        data = resp.json()\n",
    "        groups = data.get(\"groups\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking groups: {e}\")\n",
    "        groups = []\n",
    "\n",
    "\n",
    "    if not groups:\n",
    "        print(\"No groups found.\")\n",
    "\n",
    "\n",
    "    for g in groups:\n",
    "        # Create a folder for each group\n",
    "        folder_name = f\"{g['id']} - {g.get('name', 'Unknown')}\"\n",
    "        safe_folder_name = sanitize_filename(folder_name)\n",
    "        group_path = os.path.join(OUTPUT_FOLDER, \"Groups\", safe_folder_name)\n",
    "        ensure_folder_exists(group_path)\n",
    "        \n",
    "        # Get files for this group\n",
    "        try:\n",
    "            files_resp = session.get(f\"{BASE_URL}/groups/{g['id']}/files\", timeout=60)\n",
    "            files = files_resp.json().get(\"files\", [])\n",
    "            \n",
    "            # Determine which files to download\n",
    "            files_to_download = files\n",
    "            if DAYS_TO_CHECK is not None:\n",
    "                # If we have a limit, take only the first N files\n",
    "                files_to_download = files[:DAYS_TO_CHECK]\n",
    "\n",
    "\n",
    "            for f in files_to_download:\n",
    "                download_file(session, f, group_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting files for group {g['id']}: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n------------------------------------------------------------------\")\n",
    "    print(\"All done!\")\n",
    "    print(f\"Check your files in: {OUTPUT_FOLDER}\")\n",
    "    # Pause so the user can see the message if they double-clicked the script\n",
    "    input(\"Press Enter to close this window...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f533b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\avina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ranking to: D:\\work\\Trade Analysis\\Options Data\\Groups\\2 - eodLevel3\\_stoploss_wall_output\\underlying_match_to_XAUUSD.csv\n",
      "\n",
      "TOP 15 candidates:\n",
      "UnderlyingSymbol  OverlapDays  RetCorr_with_XAUUSD  MedianScale_XAUUSD_div_Underlying  ScaleStd  LastUnderlyingPrice  LastXAUUSD\n",
      "             UGL           34             0.952886                          76.368341  2.674075                71.64     4959.03\n",
      "             GLD           34             0.951281                          10.877538  0.030493               451.89     4959.03\n",
      "             IAU           34             0.950802                          53.112509  0.149589                92.56     4959.03\n",
      "            SGOL           34             0.950616                         104.959969  0.292273                46.86     4959.03\n",
      "            OUNZ           34             0.948353                         103.942970  0.299100                47.31     4959.03\n",
      "            GBUG           23             0.936928                          97.874319  1.988562                52.91     4959.03\n",
      "            GOEX           34             0.911387                          52.597637  1.860327               101.16     4959.03\n",
      "            SGDM           34             0.899353                          61.179061  1.724321                85.04     4959.03\n",
      "            JNUG           34             0.899325                          20.328065  2.221849               318.54     4959.03\n",
      "            RING           34             0.879980                          58.079707  1.925250                89.72     4959.03\n",
      "            GOAU           34             0.879077                          99.884314  2.710763                52.36     4959.03\n",
      "            GDXJ           34             0.878067                          37.575324  1.225144               142.09     4959.03\n",
      "            NUGT           34             0.877411                          22.786982  2.223057               272.99     4959.03\n",
      "             GDX           34             0.876345                          49.913126  1.590896               105.17     4959.03\n",
      "             ASA           34             0.868787                          73.088421  2.830161                71.50     4959.03\n"
     ]
    }
   ],
   "source": [
    "import os, re, zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_DIR = r\"D:\\work\\Trade Analysis\\Options Data\\Groups\\2 - eodLevel3\"\n",
    "XAUUSD_CSV = r\"D:\\work\\Trade Analysis\\XAUUSD.csv\"   # <-- change this path on your machine\n",
    "LOOKBACK_DAYS = 90\n",
    "CHUNKSIZE = 600_000\n",
    "\n",
    "# if your xau file uses different column names, set here:\n",
    "XAU_DATE_COL = \"date\"\n",
    "XAU_CLOSE_COL = \"close\"\n",
    "\n",
    "OUT_CSV = os.path.join(BASE_DIR, \"_stoploss_wall_output\", \"underlying_match_to_XAUUSD.csv\")\n",
    "os.makedirs(os.path.dirname(OUT_CSV), exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def list_l3_zips(base_dir: str):\n",
    "    items = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        m = re.match(r\"^L3[_\\-](\\d{8})\\.zip$\", name, re.IGNORECASE)\n",
    "        if m:\n",
    "            items.append((m.group(1), os.path.join(base_dir, name)))\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    return items\n",
    "\n",
    "def pick_main_member(zpath: str) -> str:\n",
    "    with zipfile.ZipFile(zpath, \"r\") as z:\n",
    "        members = [m for m in z.infolist() if not m.is_dir()]\n",
    "        members = [m for m in members if re.search(r\"\\.(csv|tsv|txt)$\", m.filename, re.I)]\n",
    "        if not members:\n",
    "            raise FileNotFoundError(f\"No csv/tsv/txt inside zip: {zpath}\")\n",
    "        option_like = [m for m in members if re.search(r\"option\", os.path.basename(m.filename), re.I)]\n",
    "        pool = option_like if option_like else members\n",
    "        pool.sort(key=lambda m: m.file_size, reverse=True)\n",
    "        return pool[0].filename\n",
    "\n",
    "def sniff_delim(first_line: str) -> str:\n",
    "    return \"\\t\" if first_line.count(\"\\t\") > first_line.count(\",\") else \",\"\n",
    "\n",
    "# =========================\n",
    "# LOAD XAUUSD DAILY SERIES\n",
    "# =========================\n",
    "xau = pd.read_csv(XAUUSD_CSV)\n",
    "xau[XAU_DATE_COL] = pd.to_datetime(xau[XAU_DATE_COL], errors=\"coerce\")\n",
    "xau[\"DataDate\"] = xau[XAU_DATE_COL].dt.date\n",
    "xau[\"DataDate\"] = pd.to_datetime(xau[\"DataDate\"])\n",
    "xau_daily = xau.groupby(\"DataDate\")[XAU_CLOSE_COL].last().reset_index()\n",
    "xau_daily = xau_daily.rename(columns={XAU_CLOSE_COL: \"XAUUSD_Close\"}).sort_values(\"DataDate\")\n",
    "\n",
    "# =========================\n",
    "# BUILD DAILY UNDERLYING PRICE FOR EACH SYMBOL FROM L3\n",
    "# =========================\n",
    "zips = list_l3_zips(BASE_DIR)[-LOOKBACK_DAYS:]\n",
    "all_daily = []\n",
    "\n",
    "for date_str, zpath in zips:\n",
    "    d = pd.to_datetime(date_str, format=\"%Y%m%d\")\n",
    "    member = pick_main_member(zpath)\n",
    "\n",
    "    with zipfile.ZipFile(zpath, \"r\") as z:\n",
    "        with z.open(member) as f:\n",
    "            first_line = f.readline().decode(\"utf-8\", errors=\"ignore\")\n",
    "        delim = sniff_delim(first_line)\n",
    "\n",
    "        with z.open(member) as f:\n",
    "            reader = pd.read_csv(\n",
    "                f, sep=delim,\n",
    "                usecols=lambda c: c.strip() in [\"UnderlyingSymbol\", \"UnderlyingPrice\"],\n",
    "                chunksize=CHUNKSIZE, low_memory=True\n",
    "            )\n",
    "\n",
    "            # accumulate per-symbol prices for that day\n",
    "            sym_prices = []\n",
    "            for chunk in reader:\n",
    "                # normalize column names\n",
    "                chunk.columns = [c.strip() for c in chunk.columns]\n",
    "                if \"UnderlyingSymbol\" not in chunk.columns or \"UnderlyingPrice\" not in chunk.columns:\n",
    "                    continue\n",
    "\n",
    "                chunk[\"UnderlyingSymbol\"] = chunk[\"UnderlyingSymbol\"].astype(str).str.strip()\n",
    "                chunk[\"UnderlyingPrice\"] = pd.to_numeric(chunk[\"UnderlyingPrice\"], errors=\"coerce\")\n",
    "                chunk = chunk.dropna(subset=[\"UnderlyingPrice\"])\n",
    "\n",
    "                # median per symbol (robust if there are any odd rows)\n",
    "                day_med = chunk.groupby(\"UnderlyingSymbol\")[\"UnderlyingPrice\"].median().reset_index()\n",
    "                sym_prices.append(day_med)\n",
    "\n",
    "            if sym_prices:\n",
    "                day_df = pd.concat(sym_prices, ignore_index=True)\n",
    "                # if duplicates across chunks, median again\n",
    "                day_df = day_df.groupby(\"UnderlyingSymbol\")[\"UnderlyingPrice\"].median().reset_index()\n",
    "                day_df[\"DataDate\"] = d.date()\n",
    "                all_daily.append(day_df)\n",
    "\n",
    "if not all_daily:\n",
    "    raise SystemExit(\"No underlying price data extracted from zips. Check file columns/delimiter.\")\n",
    "\n",
    "u = pd.concat(all_daily, ignore_index=True)\n",
    "u[\"DataDate\"] = pd.to_datetime(u[\"DataDate\"])\n",
    "u = u.sort_values([\"UnderlyingSymbol\", \"DataDate\"])\n",
    "\n",
    "# =========================\n",
    "# SCORE EACH SYMBOL VS XAUUSD\n",
    "# =========================\n",
    "scores = []\n",
    "for sym, g in u.groupby(\"UnderlyingSymbol\"):\n",
    "    m = g.merge(xau_daily, on=\"DataDate\", how=\"inner\")\n",
    "    if len(m) < 15:\n",
    "        continue\n",
    "\n",
    "    # return correlation (most important)\n",
    "    m[\"u_ret\"] = np.log(m[\"UnderlyingPrice\"] / m[\"UnderlyingPrice\"].shift(1))\n",
    "    m[\"x_ret\"] = np.log(m[\"XAUUSD_Close\"] / m[\"XAUUSD_Close\"].shift(1))\n",
    "    corr_ret = m[[\"u_ret\", \"x_ret\"]].corr().iloc[0, 1]\n",
    "\n",
    "    # level ratio stability (secondary)\n",
    "    ratio = (m[\"XAUUSD_Close\"] / m[\"UnderlyingPrice\"]).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    ratio_std = ratio.std() if len(ratio) else np.nan\n",
    "    ratio_med = ratio.median() if len(ratio) else np.nan\n",
    "\n",
    "    scores.append({\n",
    "        \"UnderlyingSymbol\": sym,\n",
    "        \"OverlapDays\": len(m),\n",
    "        \"RetCorr_with_XAUUSD\": corr_ret,\n",
    "        \"MedianScale_XAUUSD_div_Underlying\": ratio_med,\n",
    "        \"ScaleStd\": ratio_std,\n",
    "        \"LastUnderlyingPrice\": m[\"UnderlyingPrice\"].iloc[-1],\n",
    "        \"LastXAUUSD\": m[\"XAUUSD_Close\"].iloc[-1],\n",
    "    })\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scores_df = scores_df.sort_values([\"RetCorr_with_XAUUSD\", \"OverlapDays\"], ascending=[False, False])\n",
    "\n",
    "scores_df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Saved ranking to:\", OUT_CSV)\n",
    "print(\"\\nTOP 15 candidates:\")\n",
    "print(scores_df.head(15).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c381d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
