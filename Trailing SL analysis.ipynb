{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb56af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db842bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your standard convention\n",
    "data_path = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "\n",
    "# Point this to your M5 raw data folder (one CSV per symbol)\n",
    "RAW_M5_DIR = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"  # <-- CHANGE IF NEEDED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea3f146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_M5_DIR = D:/work/Client/Maatra/Trade Level Data/M5 Raw data\n",
      "CSV count: 131\n",
      "Sample files: ['D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\\\ACN.csv', 'D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\\\ADBE.csv', 'D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\\\ADI.csv', 'D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\\\ADP.csv', 'D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\\\ADSK.csv']\n",
      "[QC] Processed 50/131 files...\n",
      "[QC] Processed 100/131 files...\n",
      "[QC] Processed 131/131 files...\n",
      "Saved QC report: D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\QC Reports\\m5_qc_report.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>status</th>\n",
       "      <th>rows</th>\n",
       "      <th>cols</th>\n",
       "      <th>error</th>\n",
       "      <th>missing_cols</th>\n",
       "      <th>extra_cols</th>\n",
       "      <th>bad_datetime_rows</th>\n",
       "      <th>na_symbol</th>\n",
       "      <th>na_date</th>\n",
       "      <th>...</th>\n",
       "      <th>non_positive_close</th>\n",
       "      <th>negative_volume</th>\n",
       "      <th>mode_diff_min</th>\n",
       "      <th>pct_diff_eq_5m</th>\n",
       "      <th>max_gap_min</th>\n",
       "      <th>gaps_ge_flag</th>\n",
       "      <th>outlier_close_moves</th>\n",
       "      <th>dt_start</th>\n",
       "      <th>dt_end</th>\n",
       "      <th>unique_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACN.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>29666</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.829294</td>\n",
       "      <td>4990.0</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADBE.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>38143</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.893372</td>\n",
       "      <td>4985.0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01 00:00:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADI.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>30437</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.840189</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>460</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01 00:20:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADP.csv</td>\n",
       "      <td>CHECK</td>\n",
       "      <td>26815</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.821362</td>\n",
       "      <td>5595.0</td>\n",
       "      <td>567</td>\n",
       "      <td>15</td>\n",
       "      <td>2025-01-01 00:35:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADSK.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>27555</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.810880</td>\n",
       "      <td>5125.0</td>\n",
       "      <td>525</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-02 09:00:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AEO.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>29541</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.854028</td>\n",
       "      <td>5035.0</td>\n",
       "      <td>495</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-07 00:30:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AG.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>38643</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>4995.0</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01 00:05:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AJG.csv</td>\n",
       "      <td>CHECK</td>\n",
       "      <td>22970</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.825765</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>668</td>\n",
       "      <td>46</td>\n",
       "      <td>2025-01-01 00:25:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AKAM.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>25873</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.820153</td>\n",
       "      <td>5160.0</td>\n",
       "      <td>621</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01 00:50:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AMAT.csv</td>\n",
       "      <td>OK</td>\n",
       "      <td>39376</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.910984</td>\n",
       "      <td>5005.0</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-01-01 00:15:00</td>\n",
       "      <td>2025-12-27 00:55:00</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file status   rows  cols error missing_cols extra_cols  \\\n",
       "0   ACN.csv     OK  29666     7                                 \n",
       "1  ADBE.csv     OK  38143     7                                 \n",
       "2   ADI.csv     OK  30437     7                                 \n",
       "3   ADP.csv  CHECK  26815     7                                 \n",
       "4  ADSK.csv     OK  27555     7                                 \n",
       "5   AEO.csv     OK  29541     7                                 \n",
       "6    AG.csv     OK  38643     7                                 \n",
       "7   AJG.csv  CHECK  22970     7                                 \n",
       "8  AKAM.csv     OK  25873     7                                 \n",
       "9  AMAT.csv     OK  39376     7                                 \n",
       "\n",
       "   bad_datetime_rows  na_symbol  na_date  ...  non_positive_close  \\\n",
       "0                  0          0        0  ...                   0   \n",
       "1                  0          0        0  ...                   0   \n",
       "2                  0          0        0  ...                   0   \n",
       "3                  0          0        0  ...                   0   \n",
       "4                  0          0        0  ...                   0   \n",
       "5                  0          0        0  ...                   0   \n",
       "6                  0          0        0  ...                   0   \n",
       "7                  0          0        0  ...                   0   \n",
       "8                  0          0        0  ...                   0   \n",
       "9                  0          0        0  ...                   0   \n",
       "\n",
       "   negative_volume  mode_diff_min  pct_diff_eq_5m  max_gap_min  gaps_ge_flag  \\\n",
       "0                0            5.0        0.829294       4990.0           456   \n",
       "1                0            5.0        0.893372       4985.0           277   \n",
       "2                0            5.0        0.840189       5040.0           460   \n",
       "3                0            5.0        0.821362       5595.0           567   \n",
       "4                0            5.0        0.810880       5125.0           525   \n",
       "5                0            5.0        0.854028       5035.0           495   \n",
       "6                0            5.0        0.905983       4995.0           269   \n",
       "7                0            5.0        0.825765       5040.0           668   \n",
       "8                0            5.0        0.820153       5160.0           621   \n",
       "9                0            5.0        0.910984       5005.0           263   \n",
       "\n",
       "   outlier_close_moves            dt_start              dt_end  unique_days  \n",
       "0                    0 2025-01-01 00:00:00 2025-12-27 00:55:00          257  \n",
       "1                    0 2025-01-01 00:00:00 2025-12-27 00:55:00          259  \n",
       "2                    0 2025-01-01 00:20:00 2025-12-27 00:55:00          258  \n",
       "3                   15 2025-01-01 00:35:00 2025-12-27 00:55:00          255  \n",
       "4                    0 2025-01-02 09:00:00 2025-12-27 00:55:00          255  \n",
       "5                    0 2025-01-07 00:30:00 2025-12-27 00:55:00          251  \n",
       "6                    0 2025-01-01 00:05:00 2025-12-27 00:55:00          259  \n",
       "7                   46 2025-01-01 00:25:00 2025-12-27 00:55:00          248  \n",
       "8                    0 2025-01-01 00:50:00 2025-12-27 00:55:00          256  \n",
       "9                    0 2025-01-01 00:15:00 2025-12-27 00:55:00          259  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>issue</th>\n",
       "      <th>detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADP.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 15 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AJG.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 46 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMP.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 65 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATI.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 40 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BKNG.csv</td>\n",
       "      <td>IRREGULAR_FREQUENCY</td>\n",
       "      <td>pct_eq_5m=77.99%, mode=5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BKNG.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 15 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BR.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 124 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CDNS.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 3 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CHIQ.csv</td>\n",
       "      <td>IRREGULAR_FREQUENCY</td>\n",
       "      <td>pct_eq_5m=74.38%, mode=5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHIQ.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 83 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CLH.csv</td>\n",
       "      <td>IRREGULAR_FREQUENCY</td>\n",
       "      <td>pct_eq_5m=78.18%, mode=5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CLH.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 136 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CMA.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 18 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CNX.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 11 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CRUS.csv</td>\n",
       "      <td>IRREGULAR_FREQUENCY</td>\n",
       "      <td>pct_eq_5m=73.63%, mode=5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CSIQ.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 1 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CTSH.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 3 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CYBR.csv</td>\n",
       "      <td>IRREGULAR_FREQUENCY</td>\n",
       "      <td>pct_eq_5m=78.37%, mode=5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CYBR.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 46 bars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>EMN.csv</td>\n",
       "      <td>OUTLIER_CLOSE_MOVE</td>\n",
       "      <td>&gt;20%: 44 bars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                issue                      detail\n",
       "0    ADP.csv   OUTLIER_CLOSE_MOVE               >20%: 15 bars\n",
       "1    AJG.csv   OUTLIER_CLOSE_MOVE               >20%: 46 bars\n",
       "2    AMP.csv   OUTLIER_CLOSE_MOVE               >20%: 65 bars\n",
       "3    ATI.csv   OUTLIER_CLOSE_MOVE               >20%: 40 bars\n",
       "4   BKNG.csv  IRREGULAR_FREQUENCY  pct_eq_5m=77.99%, mode=5.0\n",
       "5   BKNG.csv   OUTLIER_CLOSE_MOVE               >20%: 15 bars\n",
       "6     BR.csv   OUTLIER_CLOSE_MOVE              >20%: 124 bars\n",
       "7   CDNS.csv   OUTLIER_CLOSE_MOVE                >20%: 3 bars\n",
       "8   CHIQ.csv  IRREGULAR_FREQUENCY  pct_eq_5m=74.38%, mode=5.0\n",
       "9   CHIQ.csv   OUTLIER_CLOSE_MOVE               >20%: 83 bars\n",
       "10   CLH.csv  IRREGULAR_FREQUENCY  pct_eq_5m=78.18%, mode=5.0\n",
       "11   CLH.csv   OUTLIER_CLOSE_MOVE              >20%: 136 bars\n",
       "12   CMA.csv   OUTLIER_CLOSE_MOVE               >20%: 18 bars\n",
       "13   CNX.csv   OUTLIER_CLOSE_MOVE               >20%: 11 bars\n",
       "14  CRUS.csv  IRREGULAR_FREQUENCY  pct_eq_5m=73.63%, mode=5.0\n",
       "15  CSIQ.csv   OUTLIER_CLOSE_MOVE                >20%: 1 bars\n",
       "16  CTSH.csv   OUTLIER_CLOSE_MOVE                >20%: 3 bars\n",
       "17  CYBR.csv  IRREGULAR_FREQUENCY  pct_eq_5m=78.37%, mode=5.0\n",
       "18  CYBR.csv   OUTLIER_CLOSE_MOVE               >20%: 46 bars\n",
       "19   EMN.csv   OUTLIER_CLOSE_MOVE               >20%: 44 bars"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT ONLY THIS SECTION)\n",
    "# ============================================================\n",
    "\n",
    "# Your standard convention\n",
    "data_path = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "\n",
    "# Point this to your M5 raw data folder (one CSV per symbol)\n",
    "RAW_M5_DIR = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"  # <-- CHANGE IF NEEDED\n",
    "\n",
    "# Where QC report should be saved\n",
    "OUT_DIR = os.path.join(data_path, \"QC Reports\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expected columns (case-insensitive mapping handled below)\n",
    "REQUIRED_COLS = [\"symbol\", \"date\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "# Column names used after standardization\n",
    "DT_COL = \"date\"\n",
    "SYMBOL_COL = \"symbol\"\n",
    "\n",
    "# Frequency diagnostics (we don't enforce, we report + optionally flag)\n",
    "EXPECTED_FREQ_MIN = 5\n",
    "\n",
    "# If pct of diffs == 5m is below this, we mark status as CHECK (set to 0 to disable)\n",
    "FLAG_IRREGULAR_IF_PCT_LT = 0.80\n",
    "\n",
    "# Count gaps >= this many minutes\n",
    "GAP_FLAG_MIN = 60\n",
    "\n",
    "# Close-to-close absolute pct move regarded as outlier (report only, also flags CHECK if >0)\n",
    "OUTLIER_PCT_MOVE = 0.20\n",
    "\n",
    "# ============================================================\n",
    "# UTILS\n",
    "# ============================================================\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make column names lower-case and strip spaces.\n",
    "    Also handles common variations like 'Date', 'datetime', 'Open', etc.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    # Common aliases mapping -> standard names\n",
    "    alias_map = {\n",
    "        \"datetime\": \"date\",\n",
    "        \"timestamp\": \"date\",\n",
    "        \"time\": \"date\",\n",
    "        \"dt\": \"date\",\n",
    "        \"ticker\": \"symbol\",\n",
    "        \"sym\": \"symbol\",\n",
    "        \"vol\": \"volume\",\n",
    "    }\n",
    "\n",
    "    df = df.rename(columns={k: v for k, v in alias_map.items() if k in df.columns})\n",
    "    return df\n",
    "\n",
    "\n",
    "def safe_to_numeric(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# QC LOGIC\n",
    "# ============================================================\n",
    "\n",
    "def qc_single_file(fp: str) -> tuple[dict, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns (summary_dict, issues_df) for a single CSV file.\n",
    "    Never raises; always returns something useful.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    file_name = os.path.basename(fp)\n",
    "\n",
    "    # -------------------------\n",
    "    # Read file\n",
    "    # -------------------------\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "    except Exception as e:\n",
    "        summary = {\n",
    "            \"file\": file_name,\n",
    "            \"status\": \"READ_FAIL\",\n",
    "            \"rows\": 0,\n",
    "            \"cols\": 0,\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "        issues_df = pd.DataFrame([{\"file\": file_name, \"issue\": \"READ_FAIL\", \"detail\": str(e)}])\n",
    "        return summary, issues_df\n",
    "\n",
    "    df = standardize_columns(df)\n",
    "\n",
    "    summary = {\n",
    "        \"file\": file_name,\n",
    "        \"status\": \"OK\",\n",
    "        \"rows\": int(len(df)),\n",
    "        \"cols\": int(len(df.columns)),\n",
    "        \"error\": \"\",\n",
    "    }\n",
    "\n",
    "    # -------------------------\n",
    "    # Column checks\n",
    "    # -------------------------\n",
    "    missing_cols = [c for c in REQUIRED_COLS if c not in df.columns]\n",
    "    extra_cols = [c for c in df.columns if c not in REQUIRED_COLS]\n",
    "\n",
    "    summary[\"missing_cols\"] = \",\".join(missing_cols) if missing_cols else \"\"\n",
    "    summary[\"extra_cols\"] = \",\".join(extra_cols) if extra_cols else \"\"\n",
    "\n",
    "    if missing_cols:\n",
    "        issues.append({\"file\": file_name, \"issue\": \"MISSING_COLUMNS\", \"detail\": \",\".join(missing_cols)})\n",
    "        summary[\"status\"] = \"CHECK\"\n",
    "\n",
    "    if extra_cols:\n",
    "        issues.append({\"file\": file_name, \"issue\": \"EXTRA_COLUMNS\", \"detail\": \",\".join(extra_cols)})\n",
    "\n",
    "    # If no datetime column, stop here (still return)\n",
    "    if DT_COL not in df.columns:\n",
    "        issues_df = pd.DataFrame(issues)\n",
    "        return summary, issues_df\n",
    "\n",
    "    # -------------------------\n",
    "    # Datetime parsing\n",
    "    # -------------------------\n",
    "    df[\"_dt\"] = pd.to_datetime(df[DT_COL], errors=\"coerce\")\n",
    "    bad_dt = int(df[\"_dt\"].isna().sum())\n",
    "    summary[\"bad_datetime_rows\"] = bad_dt\n",
    "    if bad_dt > 0:\n",
    "        issues.append({\"file\": file_name, \"issue\": \"BAD_DATETIME\", \"detail\": f\"{bad_dt} rows\"})\n",
    "        summary[\"status\"] = \"CHECK\"\n",
    "\n",
    "    # Drop rows where datetime is invalid for subsequent checks\n",
    "    df_valid = df.dropna(subset=[\"_dt\"]).copy()\n",
    "\n",
    "    if len(df_valid) == 0:\n",
    "        issues.append({\"file\": file_name, \"issue\": \"NO_VALID_DATETIME_ROWS\", \"detail\": \"all datetimes invalid\"})\n",
    "        summary[\"status\"] = \"CHECK\"\n",
    "        issues_df = pd.DataFrame(issues)\n",
    "        return summary, issues_df\n",
    "\n",
    "    # -------------------------\n",
    "    # Missingness summary\n",
    "    # -------------------------\n",
    "    for c in REQUIRED_COLS:\n",
    "        if c in df.columns:\n",
    "            nmiss = int(df[c].isna().sum())\n",
    "            summary[f\"na_{c}\"] = nmiss\n",
    "            if nmiss > 0:\n",
    "                issues.append({\"file\": file_name, \"issue\": \"MISSING_VALUES\", \"detail\": f\"{c}: {nmiss}\"})\n",
    "                summary[\"status\"] = \"CHECK\"\n",
    "        else:\n",
    "            summary[f\"na_{c}\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Sorting, duplicates, monotonic\n",
    "    # -------------------------\n",
    "    df_valid = df_valid.sort_values(\"_dt\")\n",
    "\n",
    "    if SYMBOL_COL in df_valid.columns:\n",
    "        dup = int(df_valid.duplicated(subset=[SYMBOL_COL, \"_dt\"]).sum())\n",
    "        summary[\"duplicate_symbol_dt\"] = dup\n",
    "        if dup > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"DUPLICATE_SYMBOL_DATETIME\", \"detail\": f\"{dup} rows\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        dup = int(df_valid.duplicated(subset=[\"_dt\"]).sum())\n",
    "        summary[\"duplicate_dt\"] = dup\n",
    "        if dup > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"DUPLICATE_DATETIME\", \"detail\": f\"{dup} rows\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "\n",
    "    summary[\"monotonic_time\"] = bool(df_valid[\"_dt\"].is_monotonic_increasing)\n",
    "    if not summary[\"monotonic_time\"]:\n",
    "        issues.append({\"file\": file_name, \"issue\": \"NON_MONOTONIC_TIME\", \"detail\": \"time not increasing\"})\n",
    "        summary[\"status\"] = \"CHECK\"\n",
    "\n",
    "    # -------------------------\n",
    "    # Numeric sanity checks (OHLCV)\n",
    "    # -------------------------\n",
    "    df_valid = safe_to_numeric(df_valid, [\"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
    "\n",
    "    # High < Low\n",
    "    if \"high\" in df_valid.columns and \"low\" in df_valid.columns:\n",
    "        bad_hl = int((df_valid[\"high\"] < df_valid[\"low\"]).sum())\n",
    "        summary[\"bad_high_lt_low\"] = bad_hl\n",
    "        if bad_hl > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"HIGH_LT_LOW\", \"detail\": f\"{bad_hl} rows\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        summary[\"bad_high_lt_low\"] = np.nan\n",
    "\n",
    "    # Open outside [low, high]\n",
    "    if {\"open\", \"low\", \"high\"}.issubset(df_valid.columns):\n",
    "        bad_open = int(((df_valid[\"open\"] < df_valid[\"low\"]) | (df_valid[\"open\"] > df_valid[\"high\"])).sum())\n",
    "        summary[\"bad_open_outside_range\"] = bad_open\n",
    "        if bad_open > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"OPEN_OUTSIDE_RANGE\", \"detail\": f\"{bad_open} rows\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        summary[\"bad_open_outside_range\"] = np.nan\n",
    "\n",
    "    # Close outside [low, high]\n",
    "    if {\"close\", \"low\", \"high\"}.issubset(df_valid.columns):\n",
    "        bad_close = int(((df_valid[\"close\"] < df_valid[\"low\"]) | (df_valid[\"close\"] > df_valid[\"high\"])).sum())\n",
    "        summary[\"bad_close_outside_range\"] = bad_close\n",
    "        if bad_close > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"CLOSE_OUTSIDE_RANGE\", \"detail\": f\"{bad_close} rows\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        summary[\"bad_close_outside_range\"] = np.nan\n",
    "\n",
    "    # Non-positive close\n",
    "    if \"close\" in df_valid.columns:\n",
    "        nonpos = int((df_valid[\"close\"] <= 0).sum())\n",
    "        summary[\"non_positive_close\"] = nonpos\n",
    "        if nonpos > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"NON_POSITIVE_PRICE\", \"detail\": f\"{nonpos} rows (close<=0)\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        summary[\"non_positive_close\"] = np.nan\n",
    "\n",
    "    # Negative volume\n",
    "    if \"volume\" in df_valid.columns:\n",
    "        neg_vol = int((df_valid[\"volume\"] < 0).sum())\n",
    "        summary[\"negative_volume\"] = neg_vol\n",
    "        if neg_vol > 0:\n",
    "            issues.append({\"file\": file_name, \"issue\": \"NEGATIVE_VOLUME\", \"detail\": f\"{neg_vol} rows\"})\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        summary[\"negative_volume\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Frequency diagnostics (REPORT + optional flag)\n",
    "    # -------------------------\n",
    "    if len(df_valid) >= 2:\n",
    "        diffs_min = df_valid[\"_dt\"].diff().dt.total_seconds().div(60).dropna()\n",
    "\n",
    "        if len(diffs_min) > 0:\n",
    "            mode_series = diffs_min.mode()\n",
    "            mode_val = float(mode_series.iloc[0]) if len(mode_series) else np.nan\n",
    "\n",
    "            pct_eq_5m = float((diffs_min == EXPECTED_FREQ_MIN).mean())\n",
    "            max_gap = float(diffs_min.max())\n",
    "            gaps_ge_flag = int((diffs_min >= GAP_FLAG_MIN).sum())\n",
    "\n",
    "            summary[\"mode_diff_min\"] = mode_val\n",
    "            summary[\"pct_diff_eq_5m\"] = pct_eq_5m\n",
    "            summary[\"max_gap_min\"] = max_gap\n",
    "            summary[\"gaps_ge_flag\"] = gaps_ge_flag\n",
    "\n",
    "            # Flag irregular only if enabled\n",
    "            if FLAG_IRREGULAR_IF_PCT_LT is not None and FLAG_IRREGULAR_IF_PCT_LT > 0:\n",
    "                if pct_eq_5m < FLAG_IRREGULAR_IF_PCT_LT:\n",
    "                    issues.append({\n",
    "                        \"file\": file_name,\n",
    "                        \"issue\": \"IRREGULAR_FREQUENCY\",\n",
    "                        \"detail\": f\"pct_eq_5m={pct_eq_5m:.2%}, mode={mode_val}\"\n",
    "                    })\n",
    "                    summary[\"status\"] = \"CHECK\"\n",
    "        else:\n",
    "            summary[\"mode_diff_min\"] = np.nan\n",
    "            summary[\"pct_diff_eq_5m\"] = np.nan\n",
    "            summary[\"max_gap_min\"] = np.nan\n",
    "            summary[\"gaps_ge_flag\"] = np.nan\n",
    "    else:\n",
    "        summary[\"mode_diff_min\"] = np.nan\n",
    "        summary[\"pct_diff_eq_5m\"] = np.nan\n",
    "        summary[\"max_gap_min\"] = np.nan\n",
    "        summary[\"gaps_ge_flag\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Outlier close moves (REPORT + flags CHECK if any)\n",
    "    # -------------------------\n",
    "    if \"close\" in df_valid.columns and len(df_valid) >= 2:\n",
    "        ret_abs = df_valid[\"close\"].pct_change().abs()\n",
    "        outlier = int((ret_abs > OUTLIER_PCT_MOVE).sum())\n",
    "        summary[\"outlier_close_moves\"] = outlier\n",
    "        if outlier > 0:\n",
    "            issues.append({\n",
    "                \"file\": file_name,\n",
    "                \"issue\": \"OUTLIER_CLOSE_MOVE\",\n",
    "                \"detail\": f\">{OUTLIER_PCT_MOVE:.0%}: {outlier} bars\"\n",
    "            })\n",
    "            summary[\"status\"] = \"CHECK\"\n",
    "    else:\n",
    "        summary[\"outlier_close_moves\"] = np.nan\n",
    "\n",
    "    # -------------------------\n",
    "    # Coverage\n",
    "    # -------------------------\n",
    "    summary[\"dt_start\"] = df_valid[\"_dt\"].min()\n",
    "    summary[\"dt_end\"] = df_valid[\"_dt\"].max()\n",
    "    summary[\"unique_days\"] = int(df_valid[\"_dt\"].dt.date.nunique())\n",
    "\n",
    "    issues_df = pd.DataFrame(issues)\n",
    "    return summary, issues_df\n",
    "\n",
    "\n",
    "def qc_folder(folder: str, pattern: str = \"*.csv\") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern)))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(f\"[QC] WARNING: No CSV files found in: {folder}\")\n",
    "        # Return empty but structured DFs\n",
    "        summary_df = pd.DataFrame(columns=[\"file\", \"status\", \"rows\", \"cols\", \"error\"])\n",
    "        issues_df = pd.DataFrame(columns=[\"file\", \"issue\", \"detail\"])\n",
    "        return summary_df, issues_df\n",
    "\n",
    "    summaries = []\n",
    "    all_issues = []\n",
    "\n",
    "    for i, fp in enumerate(files, 1):\n",
    "        s, iss = qc_single_file(fp)\n",
    "        summaries.append(s)\n",
    "        if iss is not None and len(iss) > 0:\n",
    "            all_issues.append(iss)\n",
    "\n",
    "        # Progress print\n",
    "        if i % 50 == 0 or i == len(files):\n",
    "            print(f\"[QC] Processed {i}/{len(files)} files...\")\n",
    "\n",
    "    summary_df = pd.DataFrame(summaries)\n",
    "    issues_df = pd.concat(all_issues, ignore_index=True) if len(all_issues) > 0 else pd.DataFrame(columns=[\"file\", \"issue\", \"detail\"])\n",
    "    return summary_df, issues_df\n",
    "\n",
    "\n",
    "def save_qc_report(summary_df: pd.DataFrame, issues_df: pd.DataFrame, out_xlsx: str) -> None:\n",
    "    \"\"\"\n",
    "    Always writes at least one visible sheet to avoid openpyxl 'no visible sheet' errors.\n",
    "    \"\"\"\n",
    "    # Ensure expected columns exist\n",
    "    if summary_df is None or len(summary_df) == 0:\n",
    "        summary_df = pd.DataFrame(columns=[\n",
    "            \"file\", \"status\", \"rows\", \"cols\", \"error\",\n",
    "            \"missing_cols\", \"extra_cols\", \"bad_datetime_rows\",\n",
    "            \"duplicate_symbol_dt\", \"duplicate_dt\",\n",
    "            \"monotonic_time\",\n",
    "            \"mode_diff_min\", \"pct_diff_eq_5m\", \"max_gap_min\", \"gaps_ge_flag\",\n",
    "            \"outlier_close_moves\", \"dt_start\", \"dt_end\", \"unique_days\"\n",
    "        ])\n",
    "\n",
    "    if \"status\" not in summary_df.columns:\n",
    "        summary_df[\"status\"] = \"CHECK\"\n",
    "    if \"file\" not in summary_df.columns:\n",
    "        summary_df[\"file\"] = \"\"\n",
    "\n",
    "    sort_cols = [c for c in [\"status\", \"file\"] if c in summary_df.columns]\n",
    "    if sort_cols:\n",
    "        summary_out = summary_df.sort_values(sort_cols)\n",
    "    else:\n",
    "        summary_out = summary_df.copy()\n",
    "\n",
    "    # Guarantee issues_df exists\n",
    "    if issues_df is None:\n",
    "        issues_df = pd.DataFrame(columns=[\"file\", \"issue\", \"detail\"])\n",
    "    if len(issues_df) == 0:\n",
    "        issues_df = pd.DataFrame(columns=[\"file\", \"issue\", \"detail\"])\n",
    "\n",
    "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as writer:\n",
    "        summary_out.to_excel(writer, index=False, sheet_name=\"summary\")\n",
    "        issues_df.to_excel(writer, index=False, sheet_name=\"issues\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "\n",
    "print(\"RAW_M5_DIR =\", RAW_M5_DIR)\n",
    "files = glob.glob(os.path.join(RAW_M5_DIR, \"*.csv\"))\n",
    "print(\"CSV count:\", len(files))\n",
    "print(\"Sample files:\", files[:5])\n",
    "\n",
    "summary_df, issues_df = qc_folder(RAW_M5_DIR)\n",
    "\n",
    "out_xlsx = os.path.join(OUT_DIR, \"m5_qc_report.xlsx\")\n",
    "save_qc_report(summary_df, issues_df, out_xlsx)\n",
    "\n",
    "print(\"Saved QC report:\", out_xlsx)\n",
    "\n",
    "# Quick peek\n",
    "display(summary_df.head(10))\n",
    "display(issues_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5780f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a477c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total M5 files: 131\n",
      "Saved to: D:/work/Client/Maatra/Trade Level Data/M5 Raw data\\m5_file_list.csv\n",
      "\n",
      "Sample:\n",
      "    filename ticker\n",
      "0    ACN.csv    ACN\n",
      "1   ADBE.csv   ADBE\n",
      "2    ADI.csv    ADI\n",
      "3    ADP.csv    ADP\n",
      "4   ADSK.csv   ADSK\n",
      "5    AEO.csv    AEO\n",
      "6     AG.csv     AG\n",
      "7    AJG.csv    AJG\n",
      "8   AKAM.csv   AKAM\n",
      "9   AMAT.csv   AMAT\n",
      "10   AMP.csv    AMP\n",
      "11  ASML.csv   ASML\n",
      "12   ATI.csv    ATI\n",
      "13    BA.csv     BA\n",
      "14  BKNG.csv   BKNG\n",
      "15    BR.csv     BR\n",
      "16   BSX.csv    BSX\n",
      "17  CDNS.csv   CDNS\n",
      "18   CFG.csv    CFG\n",
      "19  CHIQ.csv   CHIQ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "M5_DIR = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "\n",
    "files = []\n",
    "for f in os.listdir(M5_DIR):\n",
    "    if f.lower().endswith(\".csv\"):\n",
    "        ticker = os.path.splitext(f)[0].strip().upper()\n",
    "        files.append({\n",
    "            \"filename\": f,\n",
    "            \"ticker\": ticker\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(files).sort_values(\"ticker\").reset_index(drop=True)\n",
    "\n",
    "out_path = os.path.join(M5_DIR, \"m5_file_list.csv\")\n",
    "df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Total M5 files:\", len(df))\n",
    "print(\"Saved to:\", out_path)\n",
    "print(\"\\nSample:\")\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fc2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRADE_XLSX_PATH = r\"D:/work/Client/Maatra/Trade Level Data/3 Month vol 14 data with sharpe prob Fnl.xlsx\"   # <-- your file\n",
    "\n",
    "M5_DIR = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"          # <-- your M5 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb05da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade file: D:/work/Client/Maatra/Trade Level Data/3 Month vol 14 data with sharpe prob Fnl.xlsx\n",
      "M5 dir: D:/work/Client/Maatra/Trade Level Data/M5 Raw data\n",
      "M5 tickers available: 131\n",
      "Output dir: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\n",
      "Unique trades with clean entry/exit: 12815\n",
      "Running ALL trades: 12815\n",
      "Processed 1250/12815 trades...\n",
      "Processed 1750/12815 trades...\n",
      "Processed 2250/12815 trades...\n",
      "Processed 4500/12815 trades...\n",
      "Processed 7500/12815 trades...\n",
      "Processed 7750/12815 trades...\n",
      "Processed 8000/12815 trades...\n",
      "Processed 8750/12815 trades...\n",
      "Processed 9000/12815 trades...\n",
      "Processed 9250/12815 trades...\n",
      "Processed 9500/12815 trades...\n",
      "Processed 11000/12815 trades...\n",
      "Processed 11750/12815 trades...\n",
      "Processed 12250/12815 trades...\n",
      "Processed 12750/12815 trades...\n",
      "\n",
      "Done.\n",
      "Saved results: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\all_trades_trailing_sl_results.csv\n",
      "Saved missing list: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\missing_m5_trades.csv\n",
      "Saved debug checks: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\debug_trade_checks.csv\n",
      "Saved debug traces: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\debug_trade_traces.csv\n",
      "\n",
      "=== SUMMARY (WHAT YOU ASKED) ===\n",
      "Total trades evaluated: 4765\n",
      "Trailing triggered: 4602 (96.58%)\n",
      "Trailing NOT triggered: 163 (3.42%)\n",
      "\n",
      "Within TRIGGERED trades:\n",
      "  Improved: 2696\n",
      "  Worse: 1906\n",
      "  Unchanged: 0\n",
      "  Win-rate (improved): 58.58%\n",
      "\n",
      "Overall impact:\n",
      "  Scheduled PnL sum: -4160.010580000002\n",
      "  Trailing PnL sum:  -829.1371871428598\n",
      "  Uplift (Trailing - Scheduled): 3330.873392857142\n",
      "  Uplift %: -80.07%\n",
      "\n",
      "Saved summaries:\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\summary_overall.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\summary_by_symbol.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\summary_by_direction.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_FullRun\\summary_by_symbol_direction.csv\n",
      "\n",
      "================ QC CHECKS ================\n",
      "Missing M5 trades: 8050/12815 (62.82%)\n",
      "QC WARNING: Missing M5 % is high. Now that we used file list, this usually means Currency->Ticker normalization mismatch.\n",
      "Trailing exit rate: 96.58%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT THESE)\n",
    "# ============================================================\n",
    "\n",
    "TRADE_XLSX_PATH   = r\"D:/work/Client/Maatra/Trade Level Data/3 Month vol 14 data with sharpe prob Fnl.xlsx\"\n",
    "M5_DIR            = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "M5_FILE_LIST_PATH = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data/m5_file_list.csv\"\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(TRADE_XLSX_PATH), \"TrailingSL_TeamPack\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Run size: set to None to run ALL trades\n",
    "N_TRADES = None\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Trailing stop parameters\n",
    "ATR_WINDOW = 14\n",
    "ATR_MULT = 3.0\n",
    "\n",
    "SLOPE_WINDOW = 12\n",
    "SLOPE_METHOD = \"regression\"   # \"regression\" or \"polyfit\"\n",
    "USE_TIME_AWARE_SLOPE = True\n",
    "\n",
    "# Regime thresholds on ATR-normalised slope\n",
    "SLOPE_UPPER = 0.30\n",
    "SLOPE_LOWER = 0.10\n",
    "\n",
    "# Tightening / acceleration controls\n",
    "ACCEL_FACTOR = 1.0\n",
    "MIN_MULT = 0.5\n",
    "\n",
    "# Entry is at EOD on EntryDate, so trailing starts from next day\n",
    "APPLY_FROM_NEXT_DAY = True\n",
    "\n",
    "# Uncapped mode safety: stop after X calendar days if still not hit\n",
    "UNCAPPED_MAX_DAYS = 120  # set 90/120/180 as you like\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Utilities: tickers + M5 map\n",
    "# ============================================================\n",
    "\n",
    "def normalize_ticker(x: str) -> str:\n",
    "    \"\"\"Uppercase alnum only. Matches your M5 filenames like ADBE.csv, ADP.csv etc.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    x = str(x).strip().upper()\n",
    "    return \"\".join([ch for ch in x if ch.isalnum()])\n",
    "\n",
    "def ticker_from_currency(currency: str) -> str:\n",
    "    \"\"\"Trade file stores Currency like 'ADBE/USD'. We take base and normalize.\"\"\"\n",
    "    if currency is None or (isinstance(currency, float) and np.isnan(currency)):\n",
    "        return \"\"\n",
    "    s = str(currency).strip().upper()\n",
    "    base = s.split(\"/\")[0].strip() if \"/\" in s else s\n",
    "    return normalize_ticker(base)\n",
    "\n",
    "def load_m5_file_map(m5_dir: str, file_list_path: str) -> dict:\n",
    "    \"\"\"Build ticker_norm -> absolute filepath from m5_file_list.csv.\"\"\"\n",
    "    fl = pd.read_csv(file_list_path)\n",
    "    fl[\"ticker_norm\"] = fl[\"ticker\"].apply(normalize_ticker)\n",
    "    fl[\"path\"] = fl[\"filename\"].apply(lambda f: os.path.join(m5_dir, f))\n",
    "    return dict(zip(fl[\"ticker_norm\"], fl[\"path\"]))\n",
    "\n",
    "M5_MAP = load_m5_file_map(M5_DIR, M5_FILE_LIST_PATH)\n",
    "\n",
    "# Cache to avoid re-reading same M5 file repeatedly\n",
    "_m5_cache = {}\n",
    "\n",
    "def load_m5(ticker_norm: str) -> pd.DataFrame | None:\n",
    "    \"\"\"Load M5 OHLCV data for ticker_norm. Returns df sorted by datetime with _dt column.\"\"\"\n",
    "    if ticker_norm in _m5_cache:\n",
    "        return _m5_cache[ticker_norm]\n",
    "\n",
    "    fp = M5_MAP.get(ticker_norm)\n",
    "    if not fp or (not os.path.exists(fp)):\n",
    "        _m5_cache[ticker_norm] = None\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "\n",
    "    needed = [\"date\", \"open\", \"high\", \"low\", \"close\"]\n",
    "    if not all(c in df.columns for c in needed):\n",
    "        _m5_cache[ticker_norm] = None\n",
    "        return None\n",
    "\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"_dt\"]).sort_values(\"_dt\").reset_index(drop=True)\n",
    "\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # basic cleanup\n",
    "    df = df.dropna(subset=[\"open\", \"high\", \"low\", \"close\"])\n",
    "    _m5_cache[ticker_norm] = df\n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Indicators\n",
    "# ============================================================\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, window: int) -> pd.Series:\n",
    "    \"\"\"ATR computed from True Range on M5 bars.\"\"\"\n",
    "    high = df[\"high\"]\n",
    "    low = df[\"low\"]\n",
    "    close = df[\"close\"]\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr.rolling(window, min_periods=window).mean()\n",
    "\n",
    "def rolling_slope(y: pd.Series, tmin: pd.Series | None, method: str) -> float:\n",
    "    \"\"\"Slope of y over time. If tmin is provided, slope is per-minute scaled; otherwise per-index.\"\"\"\n",
    "    yy = y.values.astype(float)\n",
    "    if len(yy) < 2 or np.all(np.isnan(yy)):\n",
    "        return np.nan\n",
    "\n",
    "    if tmin is None:\n",
    "        xx = np.arange(len(yy), dtype=float)\n",
    "    else:\n",
    "        xx = tmin.values.astype(float)\n",
    "        xx = xx - xx[0]\n",
    "\n",
    "    mask = ~np.isnan(xx) & ~np.isnan(yy)\n",
    "    xx = xx[mask]; yy = yy[mask]\n",
    "    if len(yy) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    if method == \"polyfit\":\n",
    "        return np.polyfit(xx, yy, 1)[0]\n",
    "\n",
    "    xm = xx.mean(); ym = yy.mean()\n",
    "    denom = ((xx - xm) ** 2).sum()\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return (((xx - xm) * (yy - ym)).sum()) / denom\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build trade list from Excel\n",
    "# ============================================================\n",
    "\n",
    "def build_trade_summary(daily: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts your daily/row-level trade file into one row per TradeID.\n",
    "\n",
    "    We use:\n",
    "      - Entry: DayStatus=OPEN and TradeRole=OPEN_POSITION (price = entry price)\n",
    "      - Scheduled exit price: TradeRole=EXIT_TRADE if present else last price in trade\n",
    "      - EntryDate = Opening Date\n",
    "      - ExitDate_Scheduled = Closing Date\n",
    "    \"\"\"\n",
    "    df = daily.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required = [\"TradeID\", \"Currency\", \"Direction\", \"date\", \"price\",\n",
    "                \"Opening Date\", \"Closing Date\", \"DayStatus\", \"TradeRole\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Trade file missing required columns: {missing}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"Opening Date\"] = pd.to_datetime(df[\"Opening Date\"], errors=\"coerce\")\n",
    "    df[\"Closing Date\"] = pd.to_datetime(df[\"Closing Date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    df[\"DayStatusU\"] = df[\"DayStatus\"].astype(str).str.upper().str.strip()\n",
    "    df[\"TradeRoleU\"] = df[\"TradeRole\"].astype(str).str.upper().str.strip()\n",
    "    df[\"TickerNorm\"] = df[\"Currency\"].apply(ticker_from_currency)\n",
    "\n",
    "    open_rows = df[(df[\"DayStatusU\"] == \"OPEN\") & (df[\"TradeRoleU\"] == \"OPEN_POSITION\")].copy()\n",
    "    exit_rows = df[df[\"TradeRoleU\"] == \"EXIT_TRADE\"].copy()\n",
    "\n",
    "    entry = open_rows.sort_values(\"date\").groupby(\"TradeID\", as_index=False).first()\n",
    "    entry = entry[[\"TradeID\", \"TickerNorm\", \"Currency\", \"Direction\", \"Opening Date\", \"Closing Date\", \"price\"]].rename(columns={\n",
    "        \"Opening Date\": \"EntryDate\",\n",
    "        \"Closing Date\": \"ExitDate_Scheduled\",\n",
    "        \"price\": \"EntryPrice\"\n",
    "    })\n",
    "\n",
    "    if len(exit_rows) > 0:\n",
    "        ex = exit_rows.sort_values(\"date\").groupby(\"TradeID\", as_index=False).last()\n",
    "        ex = ex[[\"TradeID\", \"price\"]].rename(columns={\"price\": \"ExitPrice_Scheduled\"})\n",
    "        out = entry.merge(ex, on=\"TradeID\", how=\"left\")\n",
    "    else:\n",
    "        out = entry.copy()\n",
    "        out[\"ExitPrice_Scheduled\"] = np.nan\n",
    "\n",
    "    # fallback scheduled exit price if EXIT_TRADE missing\n",
    "    last_px = df.sort_values(\"date\").groupby(\"TradeID\", as_index=False).last()[[\"TradeID\", \"price\"]].rename(columns={\"price\": \"ExitPrice_Fallback\"})\n",
    "    out = out.merge(last_px, on=\"TradeID\", how=\"left\")\n",
    "    out[\"ExitPrice_Scheduled\"] = out[\"ExitPrice_Scheduled\"].fillna(out[\"ExitPrice_Fallback\"])\n",
    "    out = out.drop(columns=[\"ExitPrice_Fallback\"])\n",
    "\n",
    "    out = out.dropna(subset=[\"TradeID\",\"TickerNorm\",\"Direction\",\"EntryDate\",\"ExitDate_Scheduled\",\"EntryPrice\",\"ExitPrice_Scheduled\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Trailing stop simulation (two modes)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class TrailResult:\n",
    "    exit_time: pd.Timestamp\n",
    "    exit_price: float\n",
    "    exit_reason: str  # TrailingSL / Scheduled / MaxDays / NoData\n",
    "    best_favorable: float\n",
    "    max_high: float\n",
    "    min_low: float\n",
    "    stop_last: float\n",
    "    mult_last: float\n",
    "\n",
    "def simulate_trailing(tr: pd.Series, m5: pd.DataFrame, mode: str) -> TrailResult:\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - 'capped': trailing active only until scheduled exit date (T+3 cap)\n",
    "      - 'uncapped': trailing runs beyond scheduled exit until stop hit (or max days / no data)\n",
    "    \"\"\"\n",
    "    entry_date = pd.to_datetime(tr[\"EntryDate\"])\n",
    "    sched_exit_date = pd.to_datetime(tr[\"ExitDate_Scheduled\"])\n",
    "    entry_price = float(tr[\"EntryPrice\"])\n",
    "    sched_exit_price = float(tr[\"ExitPrice_Scheduled\"])\n",
    "    direction = str(tr[\"Direction\"]).strip().upper()\n",
    "    is_long = (direction == \"LONG\")\n",
    "\n",
    "    start_dt = (entry_date + timedelta(days=1)) if APPLY_FROM_NEXT_DAY else entry_date\n",
    "\n",
    "    if mode == \"capped\":\n",
    "        end_dt = sched_exit_date + timedelta(days=1)  # include that day\n",
    "    else:\n",
    "        end_dt = start_dt + timedelta(days=UNCAPPED_MAX_DAYS)\n",
    "\n",
    "    df = m5[(m5[\"_dt\"] >= start_dt) & (m5[\"_dt\"] < end_dt)].copy()\n",
    "    if len(df) == 0:\n",
    "        # no intraday bars -> fallback to scheduled\n",
    "        return TrailResult(\n",
    "            exit_time=sched_exit_date,\n",
    "            exit_price=sched_exit_price,\n",
    "            exit_reason=\"NoData\",\n",
    "            best_favorable=entry_price,\n",
    "            max_high=np.nan,\n",
    "            min_low=np.nan,\n",
    "            stop_last=np.nan,\n",
    "            mult_last=np.nan\n",
    "        )\n",
    "\n",
    "    # Indicators\n",
    "    df[\"ATR\"] = compute_atr(df, ATR_WINDOW)\n",
    "\n",
    "    if USE_TIME_AWARE_SLOPE:\n",
    "        df[\"_tmin\"] = (df[\"_dt\"] - df[\"_dt\"].iloc[0]).dt.total_seconds() / 60.0\n",
    "        df[\"slope\"] = df[\"close\"].rolling(SLOPE_WINDOW, min_periods=SLOPE_WINDOW).apply(\n",
    "            lambda s: rolling_slope(s, tmin=df.loc[s.index, \"_tmin\"], method=SLOPE_METHOD),\n",
    "            raw=False\n",
    "        )\n",
    "    else:\n",
    "        df[\"slope\"] = df[\"close\"].rolling(SLOPE_WINDOW, min_periods=SLOPE_WINDOW).apply(\n",
    "            lambda s: rolling_slope(s, tmin=None, method=SLOPE_METHOD),\n",
    "            raw=False\n",
    "        )\n",
    "\n",
    "    df[\"slope_norm\"] = df[\"slope\"] / df[\"ATR\"]\n",
    "\n",
    "    # Track extremes over the whole evaluation window\n",
    "    max_high = float(df[\"high\"].max())\n",
    "    min_low = float(df[\"low\"].min())\n",
    "\n",
    "    # \"Best favorable price\" for the trade direction\n",
    "    best_favorable = entry_price\n",
    "    stop = -np.inf if is_long else np.inf\n",
    "    stop_last = np.nan\n",
    "    mult_last = np.nan\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        dt = row[\"_dt\"]\n",
    "        atr = row[\"ATR\"]\n",
    "        if pd.isna(atr) or atr <= 0:\n",
    "            continue\n",
    "\n",
    "        # update favorable extreme using bar extremes\n",
    "        if is_long:\n",
    "            best_favorable = max(best_favorable, row[\"high\"])\n",
    "        else:\n",
    "            best_favorable = min(best_favorable, row[\"low\"])\n",
    "\n",
    "        slope_norm = row[\"slope_norm\"]\n",
    "\n",
    "        mult = ATR_MULT\n",
    "        if not pd.isna(slope_norm) and slope_norm > SLOPE_UPPER:\n",
    "            mult = max(MIN_MULT, ATR_MULT - (slope_norm * ACCEL_FACTOR))\n",
    "        elif not pd.isna(slope_norm) and slope_norm < SLOPE_LOWER:\n",
    "            mult = ATR_MULT\n",
    "\n",
    "        # chandelier-style trailing stop (anchored to best favorable extreme)\n",
    "        if is_long:\n",
    "            stop = max(stop, best_favorable - mult * atr)\n",
    "            stop_last = float(stop)\n",
    "            mult_last = float(mult)\n",
    "            if row[\"low\"] <= stop:\n",
    "                return TrailResult(dt, float(stop), \"TrailingSL\", float(best_favorable), max_high, min_low, stop_last, mult_last)\n",
    "        else:\n",
    "            stop = min(stop, best_favorable + mult * atr)  # best_favorable is the lowest low for shorts\n",
    "            stop_last = float(stop)\n",
    "            mult_last = float(mult)\n",
    "            if row[\"high\"] >= stop:\n",
    "                return TrailResult(dt, float(stop), \"TrailingSL\", float(best_favorable), max_high, min_low, stop_last, mult_last)\n",
    "\n",
    "    # If not hit:\n",
    "    if mode == \"capped\":\n",
    "        return TrailResult(sched_exit_date, sched_exit_price, \"Scheduled\",\n",
    "                           float(best_favorable), max_high, min_low, stop_last, mult_last)\n",
    "    else:\n",
    "        # uncapped: if not hit by max days, mark as MaxDays exit at last close\n",
    "        last_close = float(df[\"close\"].iloc[-1])\n",
    "        return TrailResult(df[\"_dt\"].iloc[-1], last_close, \"MaxDays\",\n",
    "                           float(best_favorable), max_high, min_low, stop_last, mult_last)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Metrics + summary helpers\n",
    "# ============================================================\n",
    "\n",
    "def direction_aware_better(direction: str, new_exit: float, old_exit: float) -> float:\n",
    "    \"\"\"Returns improvement in *price units* where >0 is better for the trade.\"\"\"\n",
    "    d = str(direction).strip().upper()\n",
    "    if pd.isna(new_exit) or pd.isna(old_exit):\n",
    "        return np.nan\n",
    "    if d == \"LONG\":\n",
    "        return float(new_exit - old_exit)\n",
    "    else:\n",
    "        return float(old_exit - new_exit)\n",
    "\n",
    "def summarize(trades_out: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Builds a compact summary table overall and split by direction.\"\"\"\n",
    "    df = trades_out.copy()\n",
    "    df[\"ImprovedFlag\"] = df[\"PriceImprovement\"] > 0\n",
    "    df[\"WorseFlag\"] = df[\"PriceImprovement\"] < 0\n",
    "    df[\"UnchangedFlag\"] = df[\"PriceImprovement\"] == 0\n",
    "    df[\"TriggeredFlag\"] = df[\"ExitReason\"].astype(str).eq(\"TrailingSL\")\n",
    "\n",
    "    def agg_block(g: pd.DataFrame) -> dict:\n",
    "        n = len(g)\n",
    "        trig = int(g[\"TriggeredFlag\"].sum())\n",
    "        notrig = int((~g[\"TriggeredFlag\"]).sum())\n",
    "        improved = int(g[\"ImprovedFlag\"].sum())\n",
    "        worse = int(g[\"WorseFlag\"].sum())\n",
    "        unchanged = int(g[\"UnchangedFlag\"].sum())\n",
    "        improved_trig = int((g[\"TriggeredFlag\"] & g[\"ImprovedFlag\"]).sum())\n",
    "        worse_trig = int((g[\"TriggeredFlag\"] & g[\"WorseFlag\"]).sum())\n",
    "        unchanged_trig = int((g[\"TriggeredFlag\"] & g[\"UnchangedFlag\"]).sum())\n",
    "\n",
    "        return {\n",
    "            \"Trades\": n,\n",
    "            \"TrailingTriggered\": trig,\n",
    "            \"TrailingNotTriggered\": notrig,\n",
    "            \"PctTriggered\": trig / n if n else np.nan,\n",
    "\n",
    "            \"Improved\": improved,\n",
    "            \"Worse\": worse,\n",
    "            \"Unchanged\": unchanged,\n",
    "            \"WinRate_Improved\": improved / n if n else np.nan,\n",
    "\n",
    "            \"Improved_Triggered\": improved_trig,\n",
    "            \"Worse_Triggered\": worse_trig,\n",
    "            \"Unchanged_Triggered\": unchanged_trig,\n",
    "\n",
    "            \"SumPriceImprovement\": float(g[\"PriceImprovement\"].sum(skipna=True)),\n",
    "            \"AvgPriceImprovement\": float(g[\"PriceImprovement\"].mean(skipna=True)),\n",
    "            \"MedianPriceImprovement\": float(g[\"PriceImprovement\"].median(skipna=True)),\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    rows.append({\"Group\": \"ALL\", **agg_block(df)})\n",
    "    for d, g in df.groupby(df[\"Direction\"].astype(str).str.upper().str.strip()):\n",
    "        rows.append({\"Group\": f\"Direction={d}\", **agg_block(g)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Main runner: produce both capped + uncapped outputs\n",
    "# ============================================================\n",
    "\n",
    "def run(mode: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    daily = pd.read_excel(TRADE_XLSX_PATH)\n",
    "    trades = build_trade_summary(daily)\n",
    "\n",
    "    if N_TRADES is None:\n",
    "        sample = trades.reset_index(drop=True)\n",
    "    else:\n",
    "        sample = trades.sample(n=min(N_TRADES, len(trades)), random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    out_rows = []\n",
    "    missing_m5 = 0\n",
    "\n",
    "    for i in range(len(sample)):\n",
    "        tr = sample.iloc[i]\n",
    "        t = tr[\"TickerNorm\"]\n",
    "        m5 = load_m5(t)\n",
    "        if m5 is None:\n",
    "            missing_m5 += 1\n",
    "            continue\n",
    "\n",
    "        res = simulate_trailing(tr, m5, mode=mode)\n",
    "\n",
    "        old_exit_price = float(tr[\"ExitPrice_Scheduled\"])\n",
    "        new_exit_price = float(res.exit_price)\n",
    "\n",
    "        # direction-aware improvement in price units\n",
    "        price_impr = direction_aware_better(tr[\"Direction\"], new_exit_price, old_exit_price)\n",
    "\n",
    "        out_rows.append({\n",
    "            \"TradeID\": tr[\"TradeID\"],\n",
    "            \"Currency\": tr[\"Currency\"],\n",
    "            \"TickerNorm\": tr[\"TickerNorm\"],\n",
    "            \"Direction\": tr[\"Direction\"],\n",
    "            \"EntryDate\": tr[\"EntryDate\"],\n",
    "            \"EntryPrice\": tr[\"EntryPrice\"],\n",
    "\n",
    "            # Original exit (your existing logic)\n",
    "            \"ExitDate_Original\": tr[\"ExitDate_Scheduled\"],\n",
    "            \"ExitPrice_Original\": old_exit_price,\n",
    "\n",
    "            # Trailing exit\n",
    "            \"ExitTime_Trailing\": res.exit_time,\n",
    "            \"ExitDate_Trailing\": pd.to_datetime(res.exit_time).date() if pd.notna(res.exit_time) else np.nan,\n",
    "            \"ExitPrice_Trailing\": new_exit_price,\n",
    "            \"ExitReason\": res.exit_reason,\n",
    "\n",
    "            # Extremes / excursions\n",
    "            \"MaxHigh_InWindow\": res.max_high,\n",
    "            \"MinLow_InWindow\": res.min_low,\n",
    "            \"BestFavorable_InWindow\": res.best_favorable,  # long=max high seen; short=min low seen\n",
    "\n",
    "            # Improvement metric\n",
    "            \"PriceImprovement\": price_impr,\n",
    "            \"PctImprovement_vs_OriginalExit\": (price_impr / old_exit_price) if old_exit_price != 0 else np.nan,\n",
    "\n",
    "            # For quick review\n",
    "            \"ClosedEarlierOrSameDay\": (pd.to_datetime(res.exit_time).date() <= pd.to_datetime(tr[\"ExitDate_Scheduled\"]).date())\n",
    "                                      if (pd.notna(res.exit_time) and pd.notna(tr[\"ExitDate_Scheduled\"])) else np.nan,\n",
    "        })\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"[{mode}] Processed {i+1}/{len(sample)} trades...\")\n",
    "\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    summary = summarize(out)\n",
    "\n",
    "    print(f\"\\n[{mode}] Done. Trades in output: {len(out)} | Missing M5 skipped: {missing_m5}\")\n",
    "    return out, summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Execute both modes + save\n",
    "# ============================================================\n",
    "\n",
    "capped_trades, capped_summary = run(mode=\"capped\")\n",
    "uncapped_trades, uncapped_summary = run(mode=\"uncapped\")\n",
    "\n",
    "capped_trades_csv = os.path.join(OUT_DIR, \"trades_trailing_capped.csv\")\n",
    "capped_summary_csv = os.path.join(OUT_DIR, \"summary_trailing_capped.csv\")\n",
    "\n",
    "uncapped_trades_csv = os.path.join(OUT_DIR, \"trades_trailing_uncapped.csv\")\n",
    "uncapped_summary_csv = os.path.join(OUT_DIR, \"summary_trailing_uncapped.csv\")\n",
    "\n",
    "capped_trades.to_csv(capped_trades_csv, index=False)\n",
    "capped_summary.to_csv(capped_summary_csv, index=False)\n",
    "\n",
    "uncapped_trades.to_csv(uncapped_trades_csv, index=False)\n",
    "uncapped_summary.to_csv(uncapped_summary_csv, index=False)\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(capped_trades_csv)\n",
    "print(capped_summary_csv)\n",
    "print(uncapped_trades_csv)\n",
    "print(uncapped_summary_csv)\n",
    "\n",
    "print(\"\\n=== CAPPED SUMMARY (T+3 cap) ===\")\n",
    "print(capped_summary)\n",
    "\n",
    "print(\"\\n=== UNCAPED SUMMARY (trail-only, max-days safety) ===\")\n",
    "print(uncapped_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0ca8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers in results: 62\n",
      "\n",
      "================ RESULTS (Baseline = ClosingDate last M5 close) ================\n",
      "Trades processed: 4765\n",
      "Baseline M5 close missing: 0 (0.00%)\n",
      "\n",
      "=== Summary (what you asked) ===\n",
      "Total trades: 4765\n",
      "Trailing triggered: 4602 (96.58%)\n",
      "Trailing NOT triggered: 163 (3.42%)\n",
      "\n",
      "Within TRIGGERED trades:\n",
      "  Improved: 2656\n",
      "  Worse: 1946\n",
      "  Unchanged: 0\n",
      "  Win-rate (improved): 57.71%\n",
      "\n",
      "Overall impact (price units):\n",
      "  Total price improvement (sum): 4046.4170228571293\n",
      "  Avg price improvement: 0.8491955976615172\n",
      "  Median price improvement: 0.3274499999999989\n",
      "\n",
      "Overall impact (%):\n",
      "  Avg % improvement: 0.2459%\n",
      "  Median % improvement: 0.2733%\n",
      "\n",
      "Saved outputs:\n",
      "D:/work/Client/Maatra/Trade Level Data/TrailingSL_FullRun\\PostProcess_BaselineM5\\results_with_baseline_m5.csv\n",
      "D:/work/Client/Maatra/Trade Level Data/TrailingSL_FullRun\\PostProcess_BaselineM5\\summary_overall_m5exit.csv\n",
      "D:/work/Client/Maatra/Trade Level Data/TrailingSL_FullRun\\PostProcess_BaselineM5\\summary_by_symbol_m5exit.csv\n",
      "D:/work/Client/Maatra/Trade Level Data/TrailingSL_FullRun\\PostProcess_BaselineM5\\summary_by_direction_m5exit.csv\n",
      "D:/work/Client/Maatra/Trade Level Data/TrailingSL_FullRun\\PostProcess_BaselineM5\\summary_by_symbol_direction_m5exit.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT THESE 3 PATHS)\n",
    "# ============================================================\n",
    "\n",
    "RESULTS_CSV = r\"D:/work/Client/Maatra/Trade Level Data/TrailingSL_FullRun/all_trades_trailing_sl_results.csv\"\n",
    "M5_DIR = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "M5_FILE_LIST_PATH = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data/m5_file_list.csv\"\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(RESULTS_CSV), \"PostProcess_BaselineM5\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# If True: baseline is last candle close on ExitDate.\n",
    "# If no candle exists for that calendar date, fallback to last candle BEFORE end of that day.\n",
    "USE_FALLBACK_LAST_BEFORE_DAY_END = True\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def normalize_ticker(x: str) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    x = str(x).strip().upper()\n",
    "    return \"\".join([ch for ch in x if ch.isalnum()])\n",
    "\n",
    "def load_m5_file_map(m5_dir: str, file_list_path: str) -> dict:\n",
    "    fl = pd.read_csv(file_list_path)\n",
    "    fl[\"ticker_norm\"] = fl[\"ticker\"].apply(normalize_ticker)\n",
    "    fl[\"path\"] = fl[\"filename\"].apply(lambda f: os.path.join(m5_dir, f))\n",
    "    return dict(zip(fl[\"ticker_norm\"], fl[\"path\"]))\n",
    "\n",
    "M5_MAP = load_m5_file_map(M5_DIR, M5_FILE_LIST_PATH)\n",
    "\n",
    "_m5_daily_last_close_cache = {}\n",
    "\n",
    "def build_daily_last_close_for_ticker(ticker_norm: str) -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    Returns a Series indexed by 'date' (datetime.date) containing the last M5 close of that date.\n",
    "    Cached per ticker.\n",
    "    \"\"\"\n",
    "    if ticker_norm in _m5_daily_last_close_cache:\n",
    "        return _m5_daily_last_close_cache[ticker_norm]\n",
    "\n",
    "    fp = M5_MAP.get(ticker_norm)\n",
    "    if not fp or (not os.path.exists(fp)):\n",
    "        _m5_daily_last_close_cache[ticker_norm] = None\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    needed = [\"date\", \"close\"]\n",
    "    if not all(c in df.columns for c in needed):\n",
    "        _m5_daily_last_close_cache[ticker_norm] = None\n",
    "        return None\n",
    "\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"_dt\"]).sort_values(\"_dt\")\n",
    "    df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"close\"])\n",
    "\n",
    "    if len(df) == 0:\n",
    "        _m5_daily_last_close_cache[ticker_norm] = None\n",
    "        return None\n",
    "\n",
    "    df[\"_d\"] = df[\"_dt\"].dt.date\n",
    "    # last close per day\n",
    "    last_close = df.groupby(\"_d\")[\"close\"].last()\n",
    "    _m5_daily_last_close_cache[ticker_norm] = last_close\n",
    "    return last_close\n",
    "\n",
    "def baseline_exit_price_m5(ticker_norm: str, exit_date: pd.Timestamp) -> float:\n",
    "    \"\"\"\n",
    "    Baseline exit = last M5 close on exit_date.\n",
    "    If missing and fallback enabled, uses last close before end-of-day.\n",
    "    \"\"\"\n",
    "    s = build_daily_last_close_for_ticker(ticker_norm)\n",
    "    if s is None or pd.isna(exit_date):\n",
    "        return np.nan\n",
    "\n",
    "    d = exit_date.date()\n",
    "    if d in s.index:\n",
    "        return float(s.loc[d])\n",
    "\n",
    "    if not USE_FALLBACK_LAST_BEFORE_DAY_END:\n",
    "        return np.nan\n",
    "\n",
    "    # Fallback: find nearest previous day that exists (still consistent \"exit around that day end\")\n",
    "    # This is a pragmatic fallback in case of holiday / missing data.\n",
    "    prev_days = s.index[s.index < d]\n",
    "    if len(prev_days) == 0:\n",
    "        return np.nan\n",
    "    return float(s.loc[prev_days[-1]])\n",
    "\n",
    "def direction_aware_improvement(direction: str, trailing_exit: float, baseline_exit: float) -> float:\n",
    "    \"\"\"\n",
    "    PriceImprovement > 0 means trailing exit is better than baseline exit.\n",
    "    \"\"\"\n",
    "    if pd.isna(trailing_exit) or pd.isna(baseline_exit):\n",
    "        return np.nan\n",
    "    d = str(direction).strip().upper()\n",
    "    if d == \"LONG\":\n",
    "        return float(trailing_exit - baseline_exit)\n",
    "    else:\n",
    "        # SHORT\n",
    "        return float(baseline_exit - trailing_exit)\n",
    "\n",
    "def build_summary(df: pd.DataFrame) -> dict:\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return {\"n_trades\": 0}\n",
    "\n",
    "    triggered = df[\"ExitReason\"].astype(str).eq(\"TrailingSL\")\n",
    "    n_trig = int(triggered.sum())\n",
    "    n_not = int((~triggered).sum())\n",
    "\n",
    "    # use PriceImprovement (direction-aware)\n",
    "    imp = df[\"PriceImprovement\"].copy()\n",
    "\n",
    "    def bucket(sub: pd.Series):\n",
    "        sub = sub.dropna()\n",
    "        if len(sub) == 0:\n",
    "            return {\"count\": 0, \"improved\": 0, \"worse\": 0, \"unchanged\": 0,\n",
    "                    \"win_rate\": np.nan, \"avg\": np.nan, \"median\": np.nan, \"sum\": np.nan}\n",
    "        return {\n",
    "            \"count\": int(len(sub)),\n",
    "            \"improved\": int((sub > 0).sum()),\n",
    "            \"worse\": int((sub < 0).sum()),\n",
    "            \"unchanged\": int((sub == 0).sum()),\n",
    "            \"win_rate\": float((sub > 0).mean()),\n",
    "            \"avg\": float(sub.mean()),\n",
    "            \"median\": float(sub.median()),\n",
    "            \"sum\": float(sub.sum()),\n",
    "        }\n",
    "\n",
    "    overall_b = bucket(imp)\n",
    "    trig_b = bucket(df.loc[triggered, \"PriceImprovement\"])\n",
    "    not_b = bucket(df.loc[~triggered, \"PriceImprovement\"])\n",
    "\n",
    "    # Also % improvement stats\n",
    "    pct_imp = df[\"PctImprovement\"].dropna()\n",
    "    pct_avg = float(pct_imp.mean()) if len(pct_imp) else np.nan\n",
    "    pct_median = float(pct_imp.median()) if len(pct_imp) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_trades\": n,\n",
    "        \"n_trailing_triggered\": n_trig,\n",
    "        \"n_trailing_not_triggered\": n_not,\n",
    "        \"pct_trailing_triggered\": n_trig / n if n else np.nan,\n",
    "\n",
    "        \"overall_improved\": overall_b[\"improved\"],\n",
    "        \"overall_worse\": overall_b[\"worse\"],\n",
    "        \"overall_unchanged\": overall_b[\"unchanged\"],\n",
    "        \"overall_win_rate\": overall_b[\"win_rate\"],\n",
    "        \"overall_avg_price_improvement\": overall_b[\"avg\"],\n",
    "        \"overall_median_price_improvement\": overall_b[\"median\"],\n",
    "        \"overall_sum_price_improvement\": overall_b[\"sum\"],\n",
    "\n",
    "        \"triggered_improved\": trig_b[\"improved\"],\n",
    "        \"triggered_worse\": trig_b[\"worse\"],\n",
    "        \"triggered_unchanged\": trig_b[\"unchanged\"],\n",
    "        \"triggered_win_rate\": trig_b[\"win_rate\"],\n",
    "        \"triggered_avg_price_improvement\": trig_b[\"avg\"],\n",
    "        \"triggered_median_price_improvement\": trig_b[\"median\"],\n",
    "        \"triggered_sum_price_improvement\": trig_b[\"sum\"],\n",
    "\n",
    "        \"not_triggered_improved\": not_b[\"improved\"],\n",
    "        \"not_triggered_worse\": not_b[\"worse\"],\n",
    "        \"not_triggered_unchanged\": not_b[\"unchanged\"],\n",
    "\n",
    "        \"overall_avg_pct_improvement\": pct_avg,\n",
    "        \"overall_median_pct_improvement\": pct_median,\n",
    "    }\n",
    "\n",
    "def group_summary(df: pd.DataFrame, group_cols: list[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for keys, g in df.groupby(group_cols):\n",
    "        if not isinstance(keys, tuple):\n",
    "            keys = (keys,)\n",
    "        s = build_summary(g)\n",
    "        row = {col: val for col, val in zip(group_cols, keys)}\n",
    "        row.update(s)\n",
    "        rows.append(row)\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"overall_sum_price_improvement\" in out.columns:\n",
    "        out = out.sort_values(\"overall_sum_price_improvement\", ascending=False)\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "\n",
    "df = pd.read_csv(RESULTS_CSV)\n",
    "\n",
    "# Required columns from your trailing run output\n",
    "required_cols = [\"TradeID\", \"TickerNorm\", \"Direction\", \"ExitDate_Scheduled\", \"ExitPrice_TrailingSL\", \"ExitReason\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Results CSV missing required columns: {missing}\")\n",
    "\n",
    "df[\"ExitDate_Scheduled\"] = pd.to_datetime(df[\"ExitDate_Scheduled\"], errors=\"coerce\")\n",
    "\n",
    "# Compute baseline exit price from M5 (last candle close on exit date)\n",
    "baseline_prices = []\n",
    "missing_baseline = 0\n",
    "\n",
    "unique_tickers = df[\"TickerNorm\"].dropna().astype(str).unique()\n",
    "print(\"Tickers in results:\", len(unique_tickers))\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    t = str(row[\"TickerNorm\"]).strip()\n",
    "    ed = row[\"ExitDate_Scheduled\"]\n",
    "    px = baseline_exit_price_m5(t, ed)\n",
    "    if pd.isna(px):\n",
    "        missing_baseline += 1\n",
    "    baseline_prices.append(px)\n",
    "\n",
    "df[\"ExitPrice_Baseline_M5Close\"] = baseline_prices\n",
    "\n",
    "# Direction-aware improvement (this is your core metric)\n",
    "df[\"PriceImprovement\"] = df.apply(\n",
    "    lambda r: direction_aware_improvement(r[\"Direction\"], r[\"ExitPrice_TrailingSL\"], r[\"ExitPrice_Baseline_M5Close\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# % improvement (normalize by baseline exit price)\n",
    "df[\"PctImprovement\"] = df[\"PriceImprovement\"] / df[\"ExitPrice_Baseline_M5Close\"]\n",
    "\n",
    "# Save enriched results\n",
    "out_results = os.path.join(OUT_DIR, \"results_with_baseline_m5.csv\")\n",
    "df.to_csv(out_results, index=False)\n",
    "\n",
    "# Overall summary + group summaries\n",
    "overall = build_summary(df)\n",
    "overall_df = pd.DataFrame([overall])\n",
    "\n",
    "summary_overall_csv = os.path.join(OUT_DIR, \"summary_overall_m5exit.csv\")\n",
    "overall_df.to_csv(summary_overall_csv, index=False)\n",
    "\n",
    "by_symbol = group_summary(df, [\"TickerNorm\"])\n",
    "by_dir = group_summary(df, [\"Direction\"])\n",
    "by_symbol_dir = group_summary(df, [\"TickerNorm\", \"Direction\"])\n",
    "\n",
    "by_symbol_csv = os.path.join(OUT_DIR, \"summary_by_symbol_m5exit.csv\")\n",
    "by_dir_csv = os.path.join(OUT_DIR, \"summary_by_direction_m5exit.csv\")\n",
    "by_symbol_dir_csv = os.path.join(OUT_DIR, \"summary_by_symbol_direction_m5exit.csv\")\n",
    "\n",
    "by_symbol.to_csv(by_symbol_csv, index=False)\n",
    "by_dir.to_csv(by_dir_csv, index=False)\n",
    "by_symbol_dir.to_csv(by_symbol_dir_csv, index=False)\n",
    "\n",
    "# Print what you care about\n",
    "print(\"\\n================ RESULTS (Baseline = ClosingDate last M5 close) ================\")\n",
    "print(f\"Trades processed: {len(df)}\")\n",
    "print(f\"Baseline M5 close missing: {missing_baseline} ({missing_baseline / max(len(df),1):.2%})\")\n",
    "\n",
    "print(\"\\n=== Summary (what you asked) ===\")\n",
    "print(f\"Total trades: {overall['n_trades']}\")\n",
    "print(f\"Trailing triggered: {overall['n_trailing_triggered']} ({overall['pct_trailing_triggered']:.2%})\")\n",
    "print(f\"Trailing NOT triggered: {overall['n_trailing_not_triggered']} ({(overall['n_trailing_not_triggered']/overall['n_trades']):.2%})\")\n",
    "\n",
    "print(\"\\nWithin TRIGGERED trades:\")\n",
    "print(f\"  Improved: {overall['triggered_improved']}\")\n",
    "print(f\"  Worse: {overall['triggered_worse']}\")\n",
    "print(f\"  Unchanged: {overall['triggered_unchanged']}\")\n",
    "print(f\"  Win-rate (improved): {overall['triggered_win_rate']:.2%}\" if pd.notna(overall['triggered_win_rate']) else \"  Win-rate: NA\")\n",
    "\n",
    "print(\"\\nOverall impact (price units):\")\n",
    "print(f\"  Total price improvement (sum): {overall['overall_sum_price_improvement']}\")\n",
    "print(f\"  Avg price improvement: {overall['overall_avg_price_improvement']}\")\n",
    "print(f\"  Median price improvement: {overall['overall_median_price_improvement']}\")\n",
    "\n",
    "print(\"\\nOverall impact (%):\")\n",
    "print(f\"  Avg % improvement: {overall['overall_avg_pct_improvement']:.4%}\" if pd.notna(overall['overall_avg_pct_improvement']) else \"  Avg % improvement: NA\")\n",
    "print(f\"  Median % improvement: {overall['overall_median_pct_improvement']:.4%}\" if pd.notna(overall['overall_median_pct_improvement']) else \"  Median % improvement: NA\")\n",
    "\n",
    "print(\"\\nSaved outputs:\")\n",
    "print(out_results)\n",
    "print(summary_overall_csv)\n",
    "print(by_symbol_csv)\n",
    "print(by_dir_csv)\n",
    "print(by_symbol_dir_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7535ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67cae89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CAPPED ONLY + MARKET HOURS ONLY\n",
      "Market hours: 14:30 to 21:00\n",
      "Trade file: D:/work/Client/Maatra/Trade Level Data/3 Month vol 14 data with sharpe prob Fnl.xlsx\n",
      "Output: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100\n",
      "\n",
      "Coverage report:\n",
      " Raw_Unique_TradeIDs  Extracted_Unique_TradeIDs  Trades_With_All_Core_Fields  Trades_Missing_Something  Trades_Missing_M5  Unique_M5_Tickers_Available  Unique_M5_Tickers_Used\n",
      "               14002                      14002                        11578                      2424               2174                          131                     160\n",
      "\n",
      "Trades to evaluate (with core fields): 11578\n",
      "Processed ticker ACN | trades: 7 | total done so far: 7/11578\n",
      "Processed ticker ADBE | trades: 83 | total done so far: 90/11578\n",
      "Processed ticker ADI | trades: 273 | total done so far: 363/11578\n",
      "Processed ticker ADP | trades: 124 | total done so far: 487/11578\n",
      "Processed ticker ADSK | trades: 94 | total done so far: 581/11578\n",
      "Processed ticker AG | trades: 11 | total done so far: 592/11578\n",
      "Processed ticker AJG | trades: 230 | total done so far: 822/11578\n",
      "Processed ticker AKAM | trades: 107 | total done so far: 929/11578\n",
      "Processed ticker AMAT | trades: 100 | total done so far: 1029/11578\n",
      "Processed ticker AMP | trades: 19 | total done so far: 1048/11578\n",
      "Processed ticker ASML | trades: 135 | total done so far: 1183/11578\n",
      "Processed ticker ATI | trades: 64 | total done so far: 1247/11578\n",
      "Processed ticker BKNG | trades: 52 | total done so far: 1299/11578\n",
      "Processed ticker BR | trades: 122 | total done so far: 1421/11578\n",
      "Processed ticker BSX | trades: 270 | total done so far: 1691/11578\n",
      "Processed ticker CDNS | trades: 400 | total done so far: 2091/11578\n",
      "Processed ticker CFG | trades: 34 | total done so far: 2125/11578\n",
      "Processed ticker CHIQ | trades: 13 | total done so far: 2138/11578\n",
      "Processed ticker CLH | trades: 128 | total done so far: 2266/11578\n",
      "Processed ticker CMA | trades: 30 | total done so far: 2296/11578\n",
      "Processed ticker CNX | trades: 67 | total done so far: 2363/11578\n",
      "Processed ticker COST | trades: 127 | total done so far: 2490/11578\n",
      "Processed ticker CRUS | trades: 13 | total done so far: 2503/11578\n",
      "Processed ticker CSCO | trades: 79 | total done so far: 2582/11578\n",
      "Processed ticker CSIQ | trades: 5 | total done so far: 2587/11578\n",
      "Processed ticker CTSH | trades: 10 | total done so far: 2597/11578\n",
      "Processed ticker CVX | trades: 43 | total done so far: 2640/11578\n",
      "Processed ticker CYBR | trades: 22 | total done so far: 2662/11578\n",
      "Processed ticker DAL | trades: 38 | total done so far: 2700/11578\n",
      "Processed ticker EA | trades: 14 | total done so far: 2714/11578\n",
      "Processed ticker ERIC | trades: 70 | total done so far: 2784/11578\n",
      "Processed ticker EXP | trades: 89 | total done so far: 2873/11578\n",
      "Processed ticker FAS | trades: 45 | total done so far: 2918/11578\n",
      "Processed ticker FDN | trades: 44 | total done so far: 2962/11578\n",
      "Processed ticker FFIV | trades: 10 | total done so far: 2972/11578\n",
      "Processed ticker FSLR | trades: 45 | total done so far: 3017/11578\n",
      "Processed ticker FTEC | trades: 86 | total done so far: 3103/11578\n",
      "Processed ticker GDDY | trades: 194 | total done so far: 3297/11578\n",
      "Processed ticker GFF | trades: 85 | total done so far: 3382/11578\n",
      "Processed ticker HBI | trades: 15 | total done so far: 3397/11578\n",
      "Processed ticker HIG | trades: 228 | total done so far: 3625/11578\n",
      "Processed ticker HP | trades: 8 | total done so far: 3633/11578\n",
      "Processed ticker HUBS | trades: 73 | total done so far: 3706/11578\n",
      "Processed ticker IBN | trades: 135 | total done so far: 3841/11578\n",
      "Processed ticker IDXX | trades: 99 | total done so far: 3940/11578\n",
      "Processed ticker INTU | trades: 338 | total done so far: 4278/11578\n",
      "Processed ticker IT | trades: 17 | total done so far: 4295/11578\n",
      "Processed ticker ITRI | trades: 143 | total done so far: 4438/11578\n",
      "Processed ticker ITUB | trades: 12 | total done so far: 4450/11578\n",
      "Processed ticker IWO | trades: 44 | total done so far: 4494/11578\n",
      "Processed ticker JD | trades: 18 | total done so far: 4512/11578\n",
      "Processed ticker KIM | trades: 37 | total done so far: 4549/11578\n",
      "Processed ticker KLAC | trades: 380 | total done so far: 4929/11578\n",
      "Processed ticker KNX | trades: 117 | total done so far: 5046/11578\n",
      "Processed ticker LEN | trades: 85 | total done so far: 5131/11578\n",
      "Processed ticker LITE | trades: 68 | total done so far: 5199/11578\n",
      "Processed ticker LPG | trades: 8 | total done so far: 5207/11578\n",
      "Processed ticker LPLA | trades: 95 | total done so far: 5302/11578\n",
      "Processed ticker LUV | trades: 3 | total done so far: 5305/11578\n",
      "Processed ticker LVS | trades: 34 | total done so far: 5339/11578\n",
      "Processed ticker MAR | trades: 35 | total done so far: 5374/11578\n",
      "Processed ticker MCD | trades: 39 | total done so far: 5413/11578\n",
      "Processed ticker MOS | trades: 2 | total done so far: 5415/11578\n",
      "Processed ticker MPC | trades: 71 | total done so far: 5486/11578\n",
      "Processed ticker MPWR | trades: 173 | total done so far: 5659/11578\n",
      "Processed ticker MRVL | trades: 67 | total done so far: 5726/11578\n",
      "Processed ticker MSFT | trades: 440 | total done so far: 6166/11578\n",
      "Processed ticker MSI | trades: 82 | total done so far: 6248/11578\n",
      "Processed ticker MTB | trades: 14 | total done so far: 6262/11578\n",
      "Processed ticker MTD | trades: 15 | total done so far: 6277/11578\n",
      "Processed ticker NBIX | trades: 106 | total done so far: 6383/11578\n",
      "Processed ticker NFLX | trades: 117 | total done so far: 6500/11578\n",
      "Processed ticker NOW | trades: 311 | total done so far: 6811/11578\n",
      "Processed ticker NTRS | trades: 34 | total done so far: 6845/11578\n",
      "Processed ticker NXPI | trades: 419 | total done so far: 7264/11578\n",
      "Processed ticker OC | trades: 51 | total done so far: 7315/11578\n",
      "Processed ticker OLED | trades: 65 | total done so far: 7380/11578\n",
      "Processed ticker OLLI | trades: 313 | total done so far: 7693/11578\n",
      "Processed ticker ORCL | trades: 239 | total done so far: 7932/11578\n",
      "Processed ticker PAAS | trades: 60 | total done so far: 7992/11578\n",
      "Processed ticker PAYC | trades: 60 | total done so far: 8052/11578\n",
      "Processed ticker PBR | trades: 22 | total done so far: 8074/11578\n",
      "Processed ticker PCAR | trades: 94 | total done so far: 8168/11578\n",
      "Processed ticker PGR | trades: 242 | total done so far: 8410/11578\n",
      "Processed ticker PHM | trades: 67 | total done so far: 8477/11578\n",
      "Processed ticker PPG | trades: 87 | total done so far: 8564/11578\n",
      "Processed ticker PWR | trades: 333 | total done so far: 8897/11578\n",
      "Processed ticker PYPL | trades: 87 | total done so far: 8984/11578\n",
      "Processed ticker QCOM | trades: 44 | total done so far: 9028/11578\n",
      "Processed ticker QRVO | trades: 24 | total done so far: 9052/11578\n",
      "Processed ticker RACE | trades: 64 | total done so far: 9116/11578\n",
      "Processed ticker ROST | trades: 32 | total done so far: 9148/11578\n",
      "Processed ticker RS | trades: 100 | total done so far: 9248/11578\n",
      "Processed ticker SAP | trades: 99 | total done so far: 9347/11578\n",
      "Processed ticker SFM | trades: 16 | total done so far: 9363/11578\n",
      "Processed ticker SPG | trades: 35 | total done so far: 9398/11578\n",
      "Processed ticker SYNA | trades: 29 | total done so far: 9427/11578\n",
      "Processed ticker TBT | trades: 36 | total done so far: 9463/11578\n",
      "Processed ticker TDOC | trades: 15 | total done so far: 9478/11578\n",
      "Processed ticker TEAM | trades: 11 | total done so far: 9489/11578\n",
      "Processed ticker TM | trades: 3 | total done so far: 9492/11578\n",
      "Processed ticker TPR | trades: 64 | total done so far: 9556/11578\n",
      "Processed ticker TRV | trades: 95 | total done so far: 9651/11578\n",
      "Processed ticker TSM | trades: 214 | total done so far: 9865/11578\n",
      "Processed ticker TXN | trades: 328 | total done so far: 10193/11578\n",
      "Processed ticker UAL | trades: 13 | total done so far: 10206/11578\n",
      "Processed ticker UI | trades: 18 | total done so far: 10224/11578\n",
      "Processed ticker UNM | trades: 30 | total done so far: 10254/11578\n",
      "Processed ticker UNP | trades: 30 | total done so far: 10284/11578\n",
      "Processed ticker URBN | trades: 246 | total done so far: 10530/11578\n",
      "Processed ticker URI | trades: 56 | total done so far: 10586/11578\n",
      "Processed ticker USB | trades: 5 | total done so far: 10591/11578\n",
      "Processed ticker V | trades: 254 | total done so far: 10845/11578\n",
      "Processed ticker VFC | trades: 61 | total done so far: 10906/11578\n",
      "Processed ticker VGT | trades: 91 | total done so far: 10997/11578\n",
      "Processed ticker VRTX | trades: 1 | total done so far: 10998/11578\n",
      "Processed ticker VTR | trades: 66 | total done so far: 11064/11578\n",
      "Processed ticker WIX | trades: 40 | total done so far: 11104/11578\n",
      "Processed ticker WKC | trades: 25 | total done so far: 11129/11578\n",
      "Processed ticker XBI | trades: 39 | total done so far: 11168/11578\n",
      "Processed ticker XLB | trades: 15 | total done so far: 11183/11578\n",
      "Processed ticker XLK | trades: 155 | total done so far: 11338/11578\n",
      "Processed ticker XLY | trades: 53 | total done so far: 11391/11578\n",
      "Processed ticker XRAY | trades: 187 | total done so far: 11578/11578\n",
      "\n",
      "Saved:\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100\\trades_trailing_capped_market_only.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100\\summary_trailing_capped_market_only.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100\\m5_market_hours_filter_stats.csv\n",
      "\n",
      "=== SUMMARY (CAPPED, MARKET HOURS ONLY) ===\n",
      "       Group  Trades  TrailingTriggered  TrailingNotTriggered  PctTriggered  Improved  Worse  Unchanged  SumPriceImprovement  AvgPriceImprovement  MedianPriceImprovement  AvgPctImprovement  MedianPctImprovement\n",
      "         ALL   11578              10581                   997      0.913888      6849   3732        997         15816.400523             1.366074                0.365465           0.003927              0.002479\n",
      "Direction=-1     656                578                    78      0.881098       299    279         78           -65.530426            -0.099894                0.000000          -0.000690              0.000000\n",
      " Direction=1   10922              10003                   919      0.915858      6550   3453        919         15881.930949             1.454123                0.454301           0.004204              0.002774\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT THESE PATHS)\n",
    "# ============================================================\n",
    "\n",
    "TRADE_XLSX_PATH      = r\"D:/work/Client/Maatra/Trade Level Data/3 Month vol 14 data with sharpe prob Fnl.xlsx\"\n",
    "M5_DIR               = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "M5_FILE_LIST_PATH    = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data/m5_file_list.csv\"\n",
    "MAPPING_XLSX_PATH    = r\"D:/work/Client/Maatra/Trade Level Data/Instrument Mapping.xlsx\"\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(TRADE_XLSX_PATH), \"TrailingSL_CAPPED_ONLY_MKT1430_2100\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_ALL_TRADES = True\n",
    "N_SAMPLE_TRADES = 2000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ============================================================\n",
    "# MARKET HOURS FILTER (ONLY USE 14:30 to 21:00)\n",
    "# ============================================================\n",
    "MARKET_START = \"14:30\"\n",
    "MARKET_END   = \"21:00\"   # inclusive\n",
    "\n",
    "# ============================================================\n",
    "# Trailing Stop Parameters (your approach)\n",
    "# ============================================================\n",
    "\n",
    "ATR_WINDOW = 14\n",
    "ATR_MULT = 3.0\n",
    "\n",
    "SLOPE_WINDOW = 12\n",
    "SLOPE_METHOD = \"regression\"     # \"regression\" or \"polyfit\"\n",
    "USE_TIME_AWARE_SLOPE = True\n",
    "\n",
    "SLOPE_UPPER = 0.30\n",
    "SLOPE_LOWER = 0.10\n",
    "\n",
    "ACCEL_FACTOR = 1.0\n",
    "MIN_MULT = 0.5\n",
    "\n",
    "APPLY_FROM_NEXT_DAY = True\n",
    "\n",
    "# ============================================================\n",
    "# Utilities: normalization + mapping\n",
    "# ============================================================\n",
    "\n",
    "def normalize_ticker(x: str) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    x = str(x).strip().upper()\n",
    "    return \"\".join([ch for ch in x if ch.isalnum()])\n",
    "\n",
    "def trade_base_from_currency(currency: str) -> str:\n",
    "    if currency is None or (isinstance(currency, float) and np.isnan(currency)):\n",
    "        return \"\"\n",
    "    s = str(currency).strip().upper()\n",
    "    base = s.split(\"/\")[0].strip() if \"/\" in s else s\n",
    "    return normalize_ticker(base)\n",
    "\n",
    "def load_trade_to_m5_mapping(mapping_xlsx_path: str) -> dict:\n",
    "    mp = pd.read_excel(mapping_xlsx_path)\n",
    "    mp.columns = [c.strip() for c in mp.columns]\n",
    "    if \"org_symbol\" not in mp.columns or \"Symbol\" not in mp.columns:\n",
    "        raise ValueError(\"Mapping file must contain columns: org_symbol, Symbol\")\n",
    "\n",
    "    mp[\"m5_ticker\"] = mp[\"org_symbol\"].apply(normalize_ticker)\n",
    "    mp[\"trade_base\"] = mp[\"Symbol\"].astype(str).str.upper().str.strip().str.split(\"/\").str[0].apply(normalize_ticker)\n",
    "    mp = mp[(mp[\"trade_base\"] != \"\") & (mp[\"m5_ticker\"] != \"\")]\n",
    "    mp = mp.drop_duplicates(subset=[\"trade_base\"], keep=\"first\")\n",
    "    return dict(zip(mp[\"trade_base\"], mp[\"m5_ticker\"]))\n",
    "\n",
    "TRADE_TO_M5 = load_trade_to_m5_mapping(MAPPING_XLSX_PATH)\n",
    "\n",
    "def map_trade_base_to_m5(trade_base: str) -> str:\n",
    "    t = normalize_ticker(trade_base)\n",
    "    return TRADE_TO_M5.get(t, t)\n",
    "\n",
    "# ============================================================\n",
    "# M5 map\n",
    "# ============================================================\n",
    "\n",
    "def load_m5_file_map(m5_dir: str, file_list_path: str) -> dict:\n",
    "    fl = pd.read_csv(file_list_path)\n",
    "    fl[\"ticker_norm\"] = fl[\"ticker\"].apply(normalize_ticker)\n",
    "    fl[\"path\"] = fl[\"filename\"].apply(lambda f: os.path.join(m5_dir, f))\n",
    "    return dict(zip(fl[\"ticker_norm\"], fl[\"path\"]))\n",
    "\n",
    "M5_MAP = load_m5_file_map(M5_DIR, M5_FILE_LIST_PATH)\n",
    "M5_TICKERS = set(M5_MAP.keys())\n",
    "\n",
    "# ============================================================\n",
    "# Indicators\n",
    "# ============================================================\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, window: int) -> pd.Series:\n",
    "    high = df[\"high\"]\n",
    "    low = df[\"low\"]\n",
    "    close = df[\"close\"]\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr.rolling(window, min_periods=window).mean()\n",
    "\n",
    "def rolling_slope(y: pd.Series, tmin: pd.Series | None, method: str) -> float:\n",
    "    yy = y.values.astype(float)\n",
    "    if len(yy) < 2 or np.all(np.isnan(yy)):\n",
    "        return np.nan\n",
    "\n",
    "    if tmin is None:\n",
    "        xx = np.arange(len(yy), dtype=float)\n",
    "    else:\n",
    "        xx = tmin.values.astype(float)\n",
    "        xx = xx - xx[0]\n",
    "\n",
    "    mask = ~np.isnan(xx) & ~np.isnan(yy)\n",
    "    xx = xx[mask]; yy = yy[mask]\n",
    "    if len(yy) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    if method == \"polyfit\":\n",
    "        return np.polyfit(xx, yy, 1)[0]\n",
    "\n",
    "    xm = xx.mean(); ym = yy.mean()\n",
    "    denom = ((xx - xm) ** 2).sum()\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return (((xx - xm) * (yy - ym)).sum()) / denom\n",
    "\n",
    "# ============================================================\n",
    "# Market-hours filter\n",
    "# ============================================================\n",
    "\n",
    "def filter_market_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only rows with time between MARKET_START and MARKET_END (inclusive),\n",
    "    based on df['_dt'] (naive timestamps as present in your M5 files).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Use between_time on a DatetimeIndex (fast and correct)\n",
    "    dfi = df.set_index(\"_dt\", drop=False).sort_index()\n",
    "\n",
    "    try:\n",
    "        dfi = dfi.between_time(MARKET_START, MARKET_END, inclusive=\"both\")\n",
    "    except TypeError:\n",
    "        # older pandas fallback (should not happen in most setups)\n",
    "        idx = dfi.index.indexer_between_time(\n",
    "            pd.to_datetime(MARKET_START).time(),\n",
    "            pd.to_datetime(MARKET_END).time(),\n",
    "            include_start=True,\n",
    "            include_end=True\n",
    "        )\n",
    "        dfi = dfi.iloc[idx]\n",
    "\n",
    "    return dfi.reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# Trade extraction (1 row per TradeID)\n",
    "# ============================================================\n",
    "\n",
    "def build_trade_summary(raw: pd.DataFrame):\n",
    "    df = raw.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required = [\"TradeID\", \"Currency\", \"Direction\", \"date\", \"price\", \"Closing Date\", \"DayStatus\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Trade file missing required columns: {missing}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"Closing Date\"] = pd.to_datetime(df[\"Closing Date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    df[\"DayStatusU\"] = df[\"DayStatus\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "    df[\"TradeBase\"] = df[\"Currency\"].apply(trade_base_from_currency)\n",
    "    df[\"TickerNorm\"] = df[\"TradeBase\"].apply(map_trade_base_to_m5)\n",
    "\n",
    "    rows = []\n",
    "    reasons = []\n",
    "\n",
    "    for tid, g in df.groupby(\"TradeID\", dropna=True):\n",
    "        g = g.sort_values(\"date\")\n",
    "\n",
    "        currency = g[\"Currency\"].dropna().iloc[0] if g[\"Currency\"].notna().any() else np.nan\n",
    "        direction = g[\"Direction\"].dropna().iloc[0] if g[\"Direction\"].notna().any() else np.nan\n",
    "        trade_base = g[\"TradeBase\"].dropna().iloc[0] if g[\"TradeBase\"].notna().any() else \"\"\n",
    "        ticker = g[\"TickerNorm\"].dropna().iloc[0] if g[\"TickerNorm\"].notna().any() else \"\"\n",
    "\n",
    "        open_mask = g[\"DayStatusU\"].eq(\"OPEN\")\n",
    "        entry_date = g.loc[open_mask, \"date\"].dropna().min() if open_mask.any() else g[\"date\"].dropna().min()\n",
    "\n",
    "        exit_date = g[\"Closing Date\"].dropna().max()\n",
    "\n",
    "        entry_px = np.nan\n",
    "        if pd.notna(entry_date):\n",
    "            day = entry_date.normalize()\n",
    "            day_rows = g[g[\"date\"].dt.normalize() == day]\n",
    "            if day_rows[\"price\"].notna().any():\n",
    "                entry_px = float(day_rows[\"price\"].dropna().iloc[-1])\n",
    "\n",
    "        exit_px = np.nan\n",
    "        if pd.notna(exit_date):\n",
    "            day = exit_date.normalize()\n",
    "            day_rows = g[g[\"date\"].dt.normalize() == day]\n",
    "            if day_rows[\"price\"].notna().any():\n",
    "                exit_px = float(day_rows[\"price\"].dropna().iloc[-1])\n",
    "\n",
    "        miss = []\n",
    "        if pd.isna(entry_date): miss.append(\"MISSING_ENTRY_DATE\")\n",
    "        if pd.isna(exit_date): miss.append(\"MISSING_EXIT_DATE\")\n",
    "        if pd.isna(entry_px): miss.append(\"MISSING_ENTRY_PRICE\")\n",
    "        if pd.isna(exit_px): miss.append(\"MISSING_EXIT_PRICE\")\n",
    "        if not ticker: miss.append(\"MISSING_TICKER\")\n",
    "        if ticker and (ticker not in M5_TICKERS): miss.append(\"TICKER_NOT_IN_M5_LIST\")\n",
    "\n",
    "        if miss:\n",
    "            reasons.append({\n",
    "                \"TradeID\": tid, \"Currency\": currency, \"TradeBase\": trade_base, \"TickerNorm\": ticker,\n",
    "                \"MissingReasons\": \";\".join(miss)\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"TradeID\": tid,\n",
    "            \"Currency\": currency,\n",
    "            \"TradeBase\": trade_base,\n",
    "            \"TickerNorm\": ticker,\n",
    "            \"Direction\": direction,\n",
    "            \"EntryDate\": entry_date,\n",
    "            \"ExitDate_Original\": exit_date,\n",
    "            \"EntryPrice\": entry_px,\n",
    "            \"ExitPrice_Original\": exit_px,\n",
    "            \"HasAllCoreFields\": (len(miss) == 0),\n",
    "            \"MissingReasons\": \";\".join(miss) if miss else \"\"\n",
    "        })\n",
    "\n",
    "    trade_summary = pd.DataFrame(rows)\n",
    "    missing_df = pd.DataFrame(reasons)\n",
    "\n",
    "    coverage = pd.DataFrame([{\n",
    "        \"Raw_Unique_TradeIDs\": int(df[\"TradeID\"].nunique()),\n",
    "        \"Extracted_Unique_TradeIDs\": int(trade_summary[\"TradeID\"].nunique()),\n",
    "        \"Trades_With_All_Core_Fields\": int(trade_summary[\"HasAllCoreFields\"].sum()),\n",
    "        \"Trades_Missing_Something\": int((~trade_summary[\"HasAllCoreFields\"]).sum()),\n",
    "        \"Trades_Missing_M5\": int(trade_summary[\"MissingReasons\"].astype(str).str.contains(\"TICKER_NOT_IN_M5_LIST\", na=False).sum()),\n",
    "        \"Unique_M5_Tickers_Available\": int(len(M5_TICKERS)),\n",
    "        \"Unique_M5_Tickers_Used\": int(trade_summary[\"TickerNorm\"].nunique())\n",
    "    }])\n",
    "\n",
    "    return trade_summary, missing_df, coverage\n",
    "\n",
    "# ============================================================\n",
    "# Trailing logic (CAPPED ONLY)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class TrailResult:\n",
    "    exit_time: pd.Timestamp\n",
    "    exit_price: float\n",
    "    exit_reason: str\n",
    "    best_favorable: float\n",
    "    max_high: float\n",
    "    min_low: float\n",
    "\n",
    "def direction_aware_improvement(direction: str, new_exit: float, old_exit: float) -> float:\n",
    "    d = str(direction).strip().upper()\n",
    "    if pd.isna(new_exit) or pd.isna(old_exit):\n",
    "        return np.nan\n",
    "    return float(new_exit - old_exit) if d == \"LONG\" else float(old_exit - new_exit)\n",
    "\n",
    "def prepare_m5_with_indicators_market_only(ticker: str) -> tuple[pd.DataFrame | None, dict]:\n",
    "    fp = M5_MAP.get(ticker)\n",
    "    if not fp or not os.path.exists(fp):\n",
    "        return None, {\"ticker\": ticker, \"status\": \"missing_file\"}\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    needed = [\"date\", \"open\", \"high\", \"low\", \"close\"]\n",
    "    if not all(c in df.columns for c in needed):\n",
    "        return None, {\"ticker\": ticker, \"status\": \"bad_columns\"}\n",
    "\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"_dt\"]).sort_values(\"_dt\").reset_index(drop=True)\n",
    "\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"open\", \"high\", \"low\", \"close\"])\n",
    "\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # FILTER MARKET HOURS ONLY\n",
    "    df = filter_market_hours(df)\n",
    "    kept_rows = len(df)\n",
    "\n",
    "    if kept_rows == 0:\n",
    "        return None, {\"ticker\": ticker, \"status\": \"no_rows_in_market_hours\", \"total_rows\": total_rows, \"kept_rows\": 0}\n",
    "\n",
    "    # Indicators on market-only data\n",
    "    df[\"ATR\"] = compute_atr(df, ATR_WINDOW)\n",
    "\n",
    "    if USE_TIME_AWARE_SLOPE:\n",
    "        df[\"_tmin\"] = (df[\"_dt\"] - df[\"_dt\"].iloc[0]).dt.total_seconds() / 60.0\n",
    "        df[\"slope\"] = df[\"close\"].rolling(SLOPE_WINDOW, min_periods=SLOPE_WINDOW).apply(\n",
    "            lambda s: rolling_slope(s, tmin=df.loc[s.index, \"_tmin\"], method=SLOPE_METHOD),\n",
    "            raw=False\n",
    "        )\n",
    "    else:\n",
    "        df[\"slope\"] = df[\"close\"].rolling(SLOPE_WINDOW, min_periods=SLOPE_WINDOW).apply(\n",
    "            lambda s: rolling_slope(s, tmin=None, method=SLOPE_METHOD),\n",
    "            raw=False\n",
    "        )\n",
    "\n",
    "    df[\"slope_norm\"] = df[\"slope\"] / df[\"ATR\"]\n",
    "\n",
    "    info = {\n",
    "        \"ticker\": ticker,\n",
    "        \"status\": \"ok\",\n",
    "        \"total_rows\": total_rows,\n",
    "        \"kept_rows\": kept_rows,\n",
    "        \"kept_pct\": (kept_rows / total_rows) if total_rows else np.nan\n",
    "    }\n",
    "    return df, info\n",
    "\n",
    "def simulate_trailing_capped(tr: pd.Series, m5: pd.DataFrame) -> TrailResult:\n",
    "    entry_date = pd.to_datetime(tr[\"EntryDate\"])\n",
    "    exit_date = pd.to_datetime(tr[\"ExitDate_Original\"])\n",
    "    entry_price = float(tr[\"EntryPrice\"])\n",
    "    orig_exit_price = float(tr[\"ExitPrice_Original\"])\n",
    "\n",
    "    direction = str(tr[\"Direction\"]).strip().upper()\n",
    "    is_long = (direction == \"LONG\")\n",
    "\n",
    "    start_dt = (entry_date + timedelta(days=1)) if APPLY_FROM_NEXT_DAY else entry_date\n",
    "    end_dt = exit_date + timedelta(days=1)\n",
    "\n",
    "    window = m5[(m5[\"_dt\"] >= start_dt) & (m5[\"_dt\"] < end_dt)].copy()\n",
    "    if len(window) == 0:\n",
    "        return TrailResult(exit_date, orig_exit_price, \"NoM5DataInMarketHours\", entry_price, np.nan, np.nan)\n",
    "\n",
    "    max_high = float(window[\"high\"].max())\n",
    "    min_low = float(window[\"low\"].min())\n",
    "\n",
    "    best_fav = entry_price\n",
    "    stop = -np.inf if is_long else np.inf\n",
    "\n",
    "    for _, row in window.iterrows():\n",
    "        atr = row[\"ATR\"]\n",
    "        if pd.isna(atr) or atr <= 0:\n",
    "            continue\n",
    "\n",
    "        if is_long:\n",
    "            best_fav = max(best_fav, row[\"high\"])\n",
    "        else:\n",
    "            best_fav = min(best_fav, row[\"low\"])\n",
    "\n",
    "        slope_norm = row[\"slope_norm\"]\n",
    "        mult = ATR_MULT\n",
    "        if not pd.isna(slope_norm) and slope_norm > SLOPE_UPPER:\n",
    "            mult = max(MIN_MULT, ATR_MULT - slope_norm * ACCEL_FACTOR)\n",
    "        elif not pd.isna(slope_norm) and slope_norm < SLOPE_LOWER:\n",
    "            mult = ATR_MULT\n",
    "\n",
    "        if is_long:\n",
    "            stop = max(stop, best_fav - mult * atr)\n",
    "            if row[\"low\"] <= stop:\n",
    "                return TrailResult(row[\"_dt\"], float(stop), \"TrailingSL\", float(best_fav), max_high, min_low)\n",
    "        else:\n",
    "            stop = min(stop, best_fav + mult * atr)\n",
    "            if row[\"high\"] >= stop:\n",
    "                return TrailResult(row[\"_dt\"], float(stop), \"TrailingSL\", float(best_fav), max_high, min_low)\n",
    "\n",
    "    return TrailResult(exit_date, orig_exit_price, \"OriginalExit\", float(best_fav), max_high, min_low)\n",
    "\n",
    "def summarize(out: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = out.copy()\n",
    "    df[\"TriggeredFlag\"] = df[\"ExitReason\"].astype(str).eq(\"TrailingSL\")\n",
    "    df[\"ImprovedFlag\"] = df[\"PriceImprovement\"] > 0\n",
    "    df[\"WorseFlag\"] = df[\"PriceImprovement\"] < 0\n",
    "\n",
    "    def agg(g: pd.DataFrame) -> dict:\n",
    "        n = len(g)\n",
    "        trig = int(g[\"TriggeredFlag\"].sum())\n",
    "        return {\n",
    "            \"Trades\": n,\n",
    "            \"TrailingTriggered\": trig,\n",
    "            \"TrailingNotTriggered\": int((~g[\"TriggeredFlag\"]).sum()),\n",
    "            \"PctTriggered\": trig / n if n else np.nan,\n",
    "            \"Improved\": int(g[\"ImprovedFlag\"].sum()),\n",
    "            \"Worse\": int(g[\"WorseFlag\"].sum()),\n",
    "            \"Unchanged\": int(((g[\"PriceImprovement\"] == 0) | g[\"PriceImprovement\"].isna()).sum()),\n",
    "            \"SumPriceImprovement\": float(g[\"PriceImprovement\"].sum(skipna=True)),\n",
    "            \"AvgPriceImprovement\": float(g[\"PriceImprovement\"].mean(skipna=True)),\n",
    "            \"MedianPriceImprovement\": float(g[\"PriceImprovement\"].median(skipna=True)),\n",
    "            \"AvgPctImprovement\": float(g[\"PctImprovement_vs_OriginalExit\"].mean(skipna=True)),\n",
    "            \"MedianPctImprovement\": float(g[\"PctImprovement_vs_OriginalExit\"].median(skipna=True)),\n",
    "        }\n",
    "\n",
    "    rows = [{\"Group\": \"ALL\", **agg(df)}]\n",
    "    for d, g in df.groupby(df[\"Direction\"].astype(str).str.upper().str.strip()):\n",
    "        rows.append({\"Group\": f\"Direction={d}\", **agg(g)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN (CAPPED ONLY, MARKET HOURS ONLY)\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"Running CAPPED ONLY + MARKET HOURS ONLY\")\n",
    "    print(\"Market hours:\", MARKET_START, \"to\", MARKET_END)\n",
    "    print(\"Trade file:\", TRADE_XLSX_PATH)\n",
    "    print(\"Output:\", OUT_DIR)\n",
    "\n",
    "    raw = pd.read_excel(TRADE_XLSX_PATH)\n",
    "    trade_summary, missing_df, coverage_df = build_trade_summary(raw)\n",
    "\n",
    "    trade_summary.to_csv(os.path.join(OUT_DIR, \"trade_summary_extracted.csv\"), index=False)\n",
    "    missing_df.to_csv(os.path.join(OUT_DIR, \"trade_summary_missing_reasons.csv\"), index=False)\n",
    "    coverage_df.to_csv(os.path.join(OUT_DIR, \"coverage_report.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nCoverage report:\")\n",
    "    print(coverage_df.to_string(index=False))\n",
    "\n",
    "    if RUN_ALL_TRADES:\n",
    "        sample = trade_summary.copy()\n",
    "    else:\n",
    "        sample = trade_summary.sample(n=min(N_SAMPLE_TRADES, len(trade_summary)), random_state=RANDOM_SEED).copy()\n",
    "\n",
    "    sample = sample[sample[\"HasAllCoreFields\"]].reset_index(drop=True)\n",
    "    print(\"\\nTrades to evaluate (with core fields):\", len(sample))\n",
    "\n",
    "    out_rows = []\n",
    "    total = len(sample)\n",
    "\n",
    "    m5_stats = []\n",
    "\n",
    "    for tkr, grp in sample.groupby(\"TickerNorm\"):\n",
    "        if tkr not in M5_TICKERS:\n",
    "            continue\n",
    "\n",
    "        m5, info = prepare_m5_with_indicators_market_only(tkr)\n",
    "        m5_stats.append(info)\n",
    "\n",
    "        if m5 is None:\n",
    "            continue\n",
    "\n",
    "        grp = grp.sort_values(\"EntryDate\")\n",
    "        for _, tr in grp.iterrows():\n",
    "            res = simulate_trailing_capped(tr, m5)\n",
    "\n",
    "            old_exit = float(tr[\"ExitPrice_Original\"])\n",
    "            new_exit = float(res.exit_price)\n",
    "            impr = direction_aware_improvement(tr[\"Direction\"], new_exit, old_exit)\n",
    "\n",
    "            orig_exit_date = pd.to_datetime(tr[\"ExitDate_Original\"])\n",
    "            new_exit_time = pd.to_datetime(res.exit_time)\n",
    "            new_exit_date = new_exit_time.normalize() if pd.notna(new_exit_time) else pd.NaT\n",
    "\n",
    "            out_rows.append({\n",
    "                \"TradeID\": tr[\"TradeID\"],\n",
    "                \"Currency\": tr[\"Currency\"],\n",
    "                \"TradeBase\": tr[\"TradeBase\"],\n",
    "                \"TickerNorm\": tr[\"TickerNorm\"],\n",
    "                \"Direction\": tr[\"Direction\"],\n",
    "\n",
    "                \"EntryDate\": tr[\"EntryDate\"],\n",
    "                \"EntryPrice\": float(tr[\"EntryPrice\"]),\n",
    "\n",
    "                \"ExitDate_Original\": orig_exit_date,\n",
    "                \"ExitPrice_Original\": old_exit,\n",
    "\n",
    "                \"ExitTime_Trailing\": new_exit_time,\n",
    "                \"ExitDate_Trailing\": new_exit_date,\n",
    "                \"ExitPrice_Trailing\": new_exit,\n",
    "                \"ExitReason\": res.exit_reason,\n",
    "\n",
    "                \"MaxHigh_InWindow\": res.max_high,\n",
    "                \"MinLow_InWindow\": res.min_low,\n",
    "                \"BestFavorable_InWindow\": res.best_favorable,\n",
    "\n",
    "                \"PriceImprovement\": impr,\n",
    "                \"PctImprovement_vs_OriginalExit\": (impr / old_exit) if old_exit != 0 else np.nan,\n",
    "\n",
    "                \"ClosedEarlierOrSameDay\": (new_exit_date.date() <= orig_exit_date.date())\n",
    "                                          if (pd.notna(new_exit_date) and pd.notna(orig_exit_date)) else np.nan\n",
    "            })\n",
    "\n",
    "        print(f\"Processed ticker {tkr} | trades: {len(grp)} | total done so far: {len(out_rows)}/{total}\")\n",
    "\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    summ = summarize(out)\n",
    "\n",
    "    out_path = os.path.join(OUT_DIR, \"trades_trailing_capped_market_only.csv\")\n",
    "    summ_path = os.path.join(OUT_DIR, \"summary_trailing_capped_market_only.csv\")\n",
    "    m5_stats_path = os.path.join(OUT_DIR, \"m5_market_hours_filter_stats.csv\")\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "    summ.to_csv(summ_path, index=False)\n",
    "    pd.DataFrame(m5_stats).to_csv(m5_stats_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(out_path)\n",
    "    print(summ_path)\n",
    "    print(m5_stats_path)\n",
    "\n",
    "    print(\"\\n=== SUMMARY (CAPPED, MARKET HOURS ONLY) ===\")\n",
    "    print(summ.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c58617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: CAPPED ONLY + MARKET HOURS ONLY\n",
      "Market session: 14:30 to 21:00\n",
      "Output: D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100_FIXEDDIR\n",
      "\n",
      "Coverage:\n",
      " Raw_Unique_TradeIDs  Extracted_Unique_TradeIDs  Trades_With_All_Core_Fields  Trades_Missing_Something  Trades_Missing_M5  Trades_Unknown_Direction  Unique_M5_Tickers_Available  Unique_M5_Tickers_Used\n",
      "               14002                      14002                        11578                      2424               2174                         0                          131                     160\n",
      "\n",
      "Trades to evaluate: 11578\n",
      "Processed ticker ACN | trades: 7 | total done: 7/11578\n",
      "Processed ticker ADBE | trades: 83 | total done: 90/11578\n",
      "Processed ticker ADI | trades: 273 | total done: 363/11578\n",
      "Processed ticker ADP | trades: 124 | total done: 487/11578\n",
      "Processed ticker ADSK | trades: 94 | total done: 581/11578\n",
      "Processed ticker AG | trades: 11 | total done: 592/11578\n",
      "Processed ticker AJG | trades: 230 | total done: 822/11578\n",
      "Processed ticker AKAM | trades: 107 | total done: 929/11578\n",
      "Processed ticker AMAT | trades: 100 | total done: 1029/11578\n",
      "Processed ticker AMP | trades: 19 | total done: 1048/11578\n",
      "Processed ticker ASML | trades: 135 | total done: 1183/11578\n",
      "Processed ticker ATI | trades: 64 | total done: 1247/11578\n",
      "Processed ticker BKNG | trades: 52 | total done: 1299/11578\n",
      "Processed ticker BR | trades: 122 | total done: 1421/11578\n",
      "Processed ticker BSX | trades: 270 | total done: 1691/11578\n",
      "Processed ticker CDNS | trades: 400 | total done: 2091/11578\n",
      "Processed ticker CFG | trades: 34 | total done: 2125/11578\n",
      "Processed ticker CHIQ | trades: 13 | total done: 2138/11578\n",
      "Processed ticker CLH | trades: 128 | total done: 2266/11578\n",
      "Processed ticker CMA | trades: 30 | total done: 2296/11578\n",
      "Processed ticker CNX | trades: 67 | total done: 2363/11578\n",
      "Processed ticker COST | trades: 127 | total done: 2490/11578\n",
      "Processed ticker CRUS | trades: 13 | total done: 2503/11578\n",
      "Processed ticker CSCO | trades: 79 | total done: 2582/11578\n",
      "Processed ticker CSIQ | trades: 5 | total done: 2587/11578\n",
      "Processed ticker CTSH | trades: 10 | total done: 2597/11578\n",
      "Processed ticker CVX | trades: 43 | total done: 2640/11578\n",
      "Processed ticker CYBR | trades: 22 | total done: 2662/11578\n",
      "Processed ticker DAL | trades: 38 | total done: 2700/11578\n",
      "Processed ticker EA | trades: 14 | total done: 2714/11578\n",
      "Processed ticker ERIC | trades: 70 | total done: 2784/11578\n",
      "Processed ticker EXP | trades: 89 | total done: 2873/11578\n",
      "Processed ticker FAS | trades: 45 | total done: 2918/11578\n",
      "Processed ticker FDN | trades: 44 | total done: 2962/11578\n",
      "Processed ticker FFIV | trades: 10 | total done: 2972/11578\n",
      "Processed ticker FSLR | trades: 45 | total done: 3017/11578\n",
      "Processed ticker FTEC | trades: 86 | total done: 3103/11578\n",
      "Processed ticker GDDY | trades: 194 | total done: 3297/11578\n",
      "Processed ticker GFF | trades: 85 | total done: 3382/11578\n",
      "Processed ticker HBI | trades: 15 | total done: 3397/11578\n",
      "Processed ticker HIG | trades: 228 | total done: 3625/11578\n",
      "Processed ticker HP | trades: 8 | total done: 3633/11578\n",
      "Processed ticker HUBS | trades: 73 | total done: 3706/11578\n",
      "Processed ticker IBN | trades: 135 | total done: 3841/11578\n",
      "Processed ticker IDXX | trades: 99 | total done: 3940/11578\n",
      "Processed ticker INTU | trades: 338 | total done: 4278/11578\n",
      "Processed ticker IT | trades: 17 | total done: 4295/11578\n",
      "Processed ticker ITRI | trades: 143 | total done: 4438/11578\n",
      "Processed ticker ITUB | trades: 12 | total done: 4450/11578\n",
      "Processed ticker IWO | trades: 44 | total done: 4494/11578\n",
      "Processed ticker JD | trades: 18 | total done: 4512/11578\n",
      "Processed ticker KIM | trades: 37 | total done: 4549/11578\n",
      "Processed ticker KLAC | trades: 380 | total done: 4929/11578\n",
      "Processed ticker KNX | trades: 117 | total done: 5046/11578\n",
      "Processed ticker LEN | trades: 85 | total done: 5131/11578\n",
      "Processed ticker LITE | trades: 68 | total done: 5199/11578\n",
      "Processed ticker LPG | trades: 8 | total done: 5207/11578\n",
      "Processed ticker LPLA | trades: 95 | total done: 5302/11578\n",
      "Processed ticker LUV | trades: 3 | total done: 5305/11578\n",
      "Processed ticker LVS | trades: 34 | total done: 5339/11578\n",
      "Processed ticker MAR | trades: 35 | total done: 5374/11578\n",
      "Processed ticker MCD | trades: 39 | total done: 5413/11578\n",
      "Processed ticker MOS | trades: 2 | total done: 5415/11578\n",
      "Processed ticker MPC | trades: 71 | total done: 5486/11578\n",
      "Processed ticker MPWR | trades: 173 | total done: 5659/11578\n",
      "Processed ticker MRVL | trades: 67 | total done: 5726/11578\n",
      "Processed ticker MSFT | trades: 440 | total done: 6166/11578\n",
      "Processed ticker MSI | trades: 82 | total done: 6248/11578\n",
      "Processed ticker MTB | trades: 14 | total done: 6262/11578\n",
      "Processed ticker MTD | trades: 15 | total done: 6277/11578\n",
      "Processed ticker NBIX | trades: 106 | total done: 6383/11578\n",
      "Processed ticker NFLX | trades: 117 | total done: 6500/11578\n",
      "Processed ticker NOW | trades: 311 | total done: 6811/11578\n",
      "Processed ticker NTRS | trades: 34 | total done: 6845/11578\n",
      "Processed ticker NXPI | trades: 419 | total done: 7264/11578\n",
      "Processed ticker OC | trades: 51 | total done: 7315/11578\n",
      "Processed ticker OLED | trades: 65 | total done: 7380/11578\n",
      "Processed ticker OLLI | trades: 313 | total done: 7693/11578\n",
      "Processed ticker ORCL | trades: 239 | total done: 7932/11578\n",
      "Processed ticker PAAS | trades: 60 | total done: 7992/11578\n",
      "Processed ticker PAYC | trades: 60 | total done: 8052/11578\n",
      "Processed ticker PBR | trades: 22 | total done: 8074/11578\n",
      "Processed ticker PCAR | trades: 94 | total done: 8168/11578\n",
      "Processed ticker PGR | trades: 242 | total done: 8410/11578\n",
      "Processed ticker PHM | trades: 67 | total done: 8477/11578\n",
      "Processed ticker PPG | trades: 87 | total done: 8564/11578\n",
      "Processed ticker PWR | trades: 333 | total done: 8897/11578\n",
      "Processed ticker PYPL | trades: 87 | total done: 8984/11578\n",
      "Processed ticker QCOM | trades: 44 | total done: 9028/11578\n",
      "Processed ticker QRVO | trades: 24 | total done: 9052/11578\n",
      "Processed ticker RACE | trades: 64 | total done: 9116/11578\n",
      "Processed ticker ROST | trades: 32 | total done: 9148/11578\n",
      "Processed ticker RS | trades: 100 | total done: 9248/11578\n",
      "Processed ticker SAP | trades: 99 | total done: 9347/11578\n",
      "Processed ticker SFM | trades: 16 | total done: 9363/11578\n",
      "Processed ticker SPG | trades: 35 | total done: 9398/11578\n",
      "Processed ticker SYNA | trades: 29 | total done: 9427/11578\n",
      "Processed ticker TBT | trades: 36 | total done: 9463/11578\n",
      "Processed ticker TDOC | trades: 15 | total done: 9478/11578\n",
      "Processed ticker TEAM | trades: 11 | total done: 9489/11578\n",
      "Processed ticker TM | trades: 3 | total done: 9492/11578\n",
      "Processed ticker TPR | trades: 64 | total done: 9556/11578\n",
      "Processed ticker TRV | trades: 95 | total done: 9651/11578\n",
      "Processed ticker TSM | trades: 214 | total done: 9865/11578\n",
      "Processed ticker TXN | trades: 328 | total done: 10193/11578\n",
      "Processed ticker UAL | trades: 13 | total done: 10206/11578\n",
      "Processed ticker UI | trades: 18 | total done: 10224/11578\n",
      "Processed ticker UNM | trades: 30 | total done: 10254/11578\n",
      "Processed ticker UNP | trades: 30 | total done: 10284/11578\n",
      "Processed ticker URBN | trades: 246 | total done: 10530/11578\n",
      "Processed ticker URI | trades: 56 | total done: 10586/11578\n",
      "Processed ticker USB | trades: 5 | total done: 10591/11578\n",
      "Processed ticker V | trades: 254 | total done: 10845/11578\n",
      "Processed ticker VFC | trades: 61 | total done: 10906/11578\n",
      "Processed ticker VGT | trades: 91 | total done: 10997/11578\n",
      "Processed ticker VRTX | trades: 1 | total done: 10998/11578\n",
      "Processed ticker VTR | trades: 66 | total done: 11064/11578\n",
      "Processed ticker WIX | trades: 40 | total done: 11104/11578\n",
      "Processed ticker WKC | trades: 25 | total done: 11129/11578\n",
      "Processed ticker XBI | trades: 39 | total done: 11168/11578\n",
      "Processed ticker XLB | trades: 15 | total done: 11183/11578\n",
      "Processed ticker XLK | trades: 155 | total done: 11338/11578\n",
      "Processed ticker XLY | trades: 53 | total done: 11391/11578\n",
      "Processed ticker XRAY | trades: 187 | total done: 11578/11578\n",
      "\n",
      "Saved:\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100_FIXEDDIR\\trades_trailing_capped_market_only.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100_FIXEDDIR\\summary_trailing_capped_market_only.csv\n",
      "D:/work/Client/Maatra/Trade Level Data\\TrailingSL_CAPPED_ONLY_MKT1430_2100_FIXEDDIR\\m5_market_hours_filter_stats.csv\n",
      "\n",
      "=== SUMMARY ===\n",
      "          Group  Trades  TrailingTriggered  TrailingNotTriggered  PctTriggered  Improved  Worse  Unchanged  SumPriceImprovement  AvgPriceImprovement  MedianPriceImprovement  AvgPctImprovement  MedianPctImprovement\n",
      "            ALL   11578              10600                   978      0.915529      5212   5386        980          4404.183114             0.380392                     0.0           0.000850                   0.0\n",
      " Direction=LONG   10922              10022                   900      0.917598      4913   5107        902          4469.713539             0.409239                     0.0           0.000943                   0.0\n",
      "Direction=SHORT     656                578                    78      0.881098       299    279         78           -65.530426            -0.099894                     0.0          -0.000690                   0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "\n",
    "# ============================================================\n",
    "# PURPOSE\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "CAPPED trailing stop-loss backtest using M5 candles.\n",
    "\n",
    "Key assumptions:\n",
    "1) Trades are entered at end-of-day (EOD) on EntryDate, so trailing logic starts from next day (configurable).\n",
    "2) Trades are NOT allowed to extend beyond the original exit date (CAPPED).\n",
    "   - If the trailing stop triggers before the original exit date => we exit early at the stop.\n",
    "   - If it never triggers within the window => we exit at the original scheduled exit date price.\n",
    "3) Only market-session M5 bars are used (pre/post market excluded).\n",
    "   - Market session filter: 14:30 to 21:00 (inclusive), based on M5 timestamps.\n",
    "4) Direction handling:\n",
    "   - Direction may be \"LONG\"/\"SHORT\" or numeric (1, -1). We normalize it robustly.\n",
    "\n",
    "Outputs:\n",
    "- trade_summary_extracted.csv: 1 row per TradeID (entry/exit pulled from trade file)\n",
    "- trades_trailing_capped_market_only.csv: per-trade trailing results (exit time/price/reason + analytics)\n",
    "- summary_trailing_capped_market_only.csv: high-level summary\n",
    "- coverage_report.csv: coverage checks\n",
    "- m5_market_hours_filter_stats.csv: per-ticker M5 filtering stats\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG (EDIT THESE PATHS)\n",
    "# ============================================================\n",
    "\n",
    "TRADE_XLSX_PATH      = r\"D:/work/Client/Maatra/Trade Level Data/3 Month vol 14 data with sharpe prob Fnl.xlsx\"\n",
    "M5_DIR               = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data\"\n",
    "M5_FILE_LIST_PATH    = r\"D:/work/Client/Maatra/Trade Level Data/M5 Raw data/m5_file_list.csv\"\n",
    "MAPPING_XLSX_PATH    = r\"D:/work/Client/Maatra/Trade Level Data/Instrument Mapping.xlsx\"\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(TRADE_XLSX_PATH), \"TrailingSL_CAPPED_ONLY_MKT1430_2100_FIXEDDIR\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "RUN_ALL_TRADES = True\n",
    "N_SAMPLE_TRADES = 2000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ============================================================\n",
    "# MARKET HOURS FILTER (ONLY USE 14:30 to 21:00)\n",
    "# ============================================================\n",
    "MARKET_START = \"14:30\"\n",
    "MARKET_END   = \"21:00\"   # inclusive\n",
    "\n",
    "# ============================================================\n",
    "# TRAILING STOP PARAMETERS (your approach)\n",
    "# ============================================================\n",
    "ATR_WINDOW = 14\n",
    "ATR_MULT = 3.0\n",
    "\n",
    "SLOPE_WINDOW = 12\n",
    "SLOPE_METHOD = \"regression\"   # \"regression\" or \"polyfit\"\n",
    "USE_TIME_AWARE_SLOPE = True\n",
    "\n",
    "# slope_norm = slope / ATR\n",
    "SLOPE_UPPER = 0.30\n",
    "SLOPE_LOWER = 0.10\n",
    "\n",
    "ACCEL_FACTOR = 1.0\n",
    "MIN_MULT = 0.5\n",
    "\n",
    "# Since entry is at EOD, apply trailing from next day\n",
    "APPLY_FROM_NEXT_DAY = True\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS: normalization + direction parsing + mapping\n",
    "# ============================================================\n",
    "\n",
    "def normalize_ticker(x: str) -> str:\n",
    "    \"\"\"Uppercase alnum-only ticker key.\"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    x = str(x).strip().upper()\n",
    "    return \"\".join([ch for ch in x if ch.isalnum()])\n",
    "\n",
    "def trade_base_from_currency(currency: str) -> str:\n",
    "    \"\"\"\n",
    "    Trade file may have Currency like 'AAPL/USD' or just 'AAPL'.\n",
    "    We extract the base and normalize.\n",
    "    \"\"\"\n",
    "    if currency is None or (isinstance(currency, float) and np.isnan(currency)):\n",
    "        return \"\"\n",
    "    s = str(currency).strip().upper()\n",
    "    base = s.split(\"/\")[0].strip() if \"/\" in s else s\n",
    "    return normalize_ticker(base)\n",
    "\n",
    "def normalize_direction(x) -> str:\n",
    "    \"\"\"\n",
    "    Normalizes direction to 'LONG' or 'SHORT'.\n",
    "\n",
    "    Handles cases:\n",
    "      - 'LONG'/'SHORT'\n",
    "      - 'BUY'/'SELL'\n",
    "      - 1 / -1 (or '1', '-1', 1.0, -1.0)\n",
    "    \"\"\"\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    s = str(x).strip().upper()\n",
    "\n",
    "    if s in [\"LONG\", \"BUY\", \"B\", \"1\", \"1.0\", \"+1\", \"+1.0\"]:\n",
    "        return \"LONG\"\n",
    "    if s in [\"SHORT\", \"SELL\", \"S\", \"-1\", \"-1.0\"]:\n",
    "        return \"SHORT\"\n",
    "\n",
    "    # Try numeric parsing\n",
    "    try:\n",
    "        v = float(s)\n",
    "        if v > 0:\n",
    "            return \"LONG\"\n",
    "        if v < 0:\n",
    "            return \"SHORT\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def load_trade_to_m5_mapping(mapping_xlsx_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses Instrument Mapping.xlsx to map trade instrument -> M5 file ticker.\n",
    "    Expected columns: org_symbol (M5 ticker), Symbol (trade symbol like AAPL/USD).\n",
    "    \"\"\"\n",
    "    mp = pd.read_excel(mapping_xlsx_path)\n",
    "    mp.columns = [c.strip() for c in mp.columns]\n",
    "    if \"org_symbol\" not in mp.columns or \"Symbol\" not in mp.columns:\n",
    "        raise ValueError(\"Mapping file must contain columns: org_symbol, Symbol\")\n",
    "\n",
    "    mp[\"m5_ticker\"] = mp[\"org_symbol\"].apply(normalize_ticker)\n",
    "    mp[\"trade_base\"] = mp[\"Symbol\"].astype(str).str.upper().str.strip().str.split(\"/\").str[0].apply(normalize_ticker)\n",
    "    mp = mp[(mp[\"trade_base\"] != \"\") & (mp[\"m5_ticker\"] != \"\")]\n",
    "    mp = mp.drop_duplicates(subset=[\"trade_base\"], keep=\"first\")\n",
    "    return dict(zip(mp[\"trade_base\"], mp[\"m5_ticker\"]))\n",
    "\n",
    "TRADE_TO_M5 = load_trade_to_m5_mapping(MAPPING_XLSX_PATH)\n",
    "\n",
    "def map_trade_base_to_m5(trade_base: str) -> str:\n",
    "    \"\"\"Map trade base ticker to M5 ticker using mapping. If not found, fallback to itself.\"\"\"\n",
    "    t = normalize_ticker(trade_base)\n",
    "    return TRADE_TO_M5.get(t, t)\n",
    "\n",
    "# ============================================================\n",
    "# M5 FILE MAP\n",
    "# ============================================================\n",
    "\n",
    "def load_m5_file_map(m5_dir: str, file_list_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads m5_file_list.csv, expected columns: ticker, filename\n",
    "    and returns dict: ticker_norm -> full filepath\n",
    "    \"\"\"\n",
    "    fl = pd.read_csv(file_list_path)\n",
    "    fl[\"ticker_norm\"] = fl[\"ticker\"].apply(normalize_ticker)\n",
    "    fl[\"path\"] = fl[\"filename\"].apply(lambda f: os.path.join(m5_dir, f))\n",
    "    return dict(zip(fl[\"ticker_norm\"], fl[\"path\"]))\n",
    "\n",
    "M5_MAP = load_m5_file_map(M5_DIR, M5_FILE_LIST_PATH)\n",
    "M5_TICKERS = set(M5_MAP.keys())\n",
    "\n",
    "# ============================================================\n",
    "# INDICATORS: ATR + SLOPE\n",
    "# ============================================================\n",
    "\n",
    "def compute_atr(df: pd.DataFrame, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    ATR using simple rolling mean of True Range.\n",
    "    True Range = max(high-low, abs(high-prev_close), abs(low-prev_close))\n",
    "    \"\"\"\n",
    "    high = df[\"high\"]\n",
    "    low = df[\"low\"]\n",
    "    close = df[\"close\"]\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "    return tr.rolling(window, min_periods=window).mean()\n",
    "\n",
    "def rolling_slope(y: pd.Series, tmin: pd.Series | None, method: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes slope of y vs x where x is time (minutes) or simple index.\n",
    "    - regression: OLS slope\n",
    "    - polyfit: np.polyfit slope\n",
    "    \"\"\"\n",
    "    yy = y.values.astype(float)\n",
    "    if len(yy) < 2 or np.all(np.isnan(yy)):\n",
    "        return np.nan\n",
    "\n",
    "    if tmin is None:\n",
    "        xx = np.arange(len(yy), dtype=float)\n",
    "    else:\n",
    "        xx = tmin.values.astype(float)\n",
    "        xx = xx - xx[0]\n",
    "\n",
    "    mask = ~np.isnan(xx) & ~np.isnan(yy)\n",
    "    xx = xx[mask]; yy = yy[mask]\n",
    "    if len(yy) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    if method == \"polyfit\":\n",
    "        return np.polyfit(xx, yy, 1)[0]\n",
    "\n",
    "    xm = xx.mean(); ym = yy.mean()\n",
    "    denom = ((xx - xm) ** 2).sum()\n",
    "    if denom == 0:\n",
    "        return np.nan\n",
    "    return (((xx - xm) * (yy - ym)).sum()) / denom\n",
    "\n",
    "# ============================================================\n",
    "# MARKET HOURS FILTER\n",
    "# ============================================================\n",
    "\n",
    "def filter_market_hours(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep only rows with time between MARKET_START and MARKET_END (inclusive),\n",
    "    based on df['_dt'].\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    dfi = df.set_index(\"_dt\", drop=False).sort_index()\n",
    "    try:\n",
    "        dfi = dfi.between_time(MARKET_START, MARKET_END, inclusive=\"both\")\n",
    "    except TypeError:\n",
    "        # older pandas fallback\n",
    "        idx = dfi.index.indexer_between_time(\n",
    "            pd.to_datetime(MARKET_START).time(),\n",
    "            pd.to_datetime(MARKET_END).time(),\n",
    "            include_start=True,\n",
    "            include_end=True\n",
    "        )\n",
    "        dfi = dfi.iloc[idx]\n",
    "    return dfi.reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# TRADE EXTRACTION: 1 ROW PER TRADEID\n",
    "# ============================================================\n",
    "\n",
    "def build_trade_summary(raw: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    From the trade-level file (multiple rows per TradeID across days),\n",
    "    we extract:\n",
    "      - EntryDate: DayStatus == OPEN else earliest date\n",
    "      - ExitDate_Original: Closing Date (scheduled exit date)\n",
    "      - EntryPrice: last trade-file price on EntryDate\n",
    "      - ExitPrice_Original: last trade-file price on ExitDate\n",
    "      - Direction normalized to LONG/SHORT\n",
    "      - Currency -> TradeBase -> TickerNorm (using mapping)\n",
    "    \"\"\"\n",
    "    df = raw.copy()\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    required = [\"TradeID\", \"Currency\", \"Direction\", \"date\", \"price\", \"Closing Date\", \"DayStatus\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Trade file missing required columns: {missing}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"Closing Date\"] = pd.to_datetime(df[\"Closing Date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "    df[\"DayStatusU\"] = df[\"DayStatus\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "    df[\"TradeBase\"] = df[\"Currency\"].apply(trade_base_from_currency)\n",
    "    df[\"TickerNorm\"] = df[\"TradeBase\"].apply(map_trade_base_to_m5)\n",
    "    df[\"DirectionNorm\"] = df[\"Direction\"].apply(normalize_direction)\n",
    "\n",
    "    rows = []\n",
    "    reasons = []\n",
    "\n",
    "    for tid, g in df.groupby(\"TradeID\", dropna=True):\n",
    "        g = g.sort_values(\"date\")\n",
    "\n",
    "        currency = g[\"Currency\"].dropna().iloc[0] if g[\"Currency\"].notna().any() else np.nan\n",
    "        direction = g[\"DirectionNorm\"].dropna().iloc[0] if g[\"DirectionNorm\"].notna().any() else \"UNKNOWN\"\n",
    "        trade_base = g[\"TradeBase\"].dropna().iloc[0] if g[\"TradeBase\"].notna().any() else \"\"\n",
    "        ticker = g[\"TickerNorm\"].dropna().iloc[0] if g[\"TickerNorm\"].notna().any() else \"\"\n",
    "\n",
    "        open_mask = g[\"DayStatusU\"].eq(\"OPEN\")\n",
    "        entry_date = g.loc[open_mask, \"date\"].dropna().min() if open_mask.any() else g[\"date\"].dropna().min()\n",
    "        exit_date = g[\"Closing Date\"].dropna().max()\n",
    "\n",
    "        # Entry price: last trade-file price on entry date\n",
    "        entry_px = np.nan\n",
    "        if pd.notna(entry_date):\n",
    "            day = entry_date.normalize()\n",
    "            day_rows = g[g[\"date\"].dt.normalize() == day]\n",
    "            if day_rows[\"price\"].notna().any():\n",
    "                entry_px = float(day_rows[\"price\"].dropna().iloc[-1])\n",
    "\n",
    "        # Exit price: last trade-file price on exit date\n",
    "        exit_px = np.nan\n",
    "        if pd.notna(exit_date):\n",
    "            day = exit_date.normalize()\n",
    "            day_rows = g[g[\"date\"].dt.normalize() == day]\n",
    "            if day_rows[\"price\"].notna().any():\n",
    "                exit_px = float(day_rows[\"price\"].dropna().iloc[-1])\n",
    "\n",
    "        miss = []\n",
    "        if pd.isna(entry_date): miss.append(\"MISSING_ENTRY_DATE\")\n",
    "        if pd.isna(exit_date): miss.append(\"MISSING_EXIT_DATE\")\n",
    "        if pd.isna(entry_px): miss.append(\"MISSING_ENTRY_PRICE\")\n",
    "        if pd.isna(exit_px): miss.append(\"MISSING_EXIT_PRICE\")\n",
    "        if not ticker: miss.append(\"MISSING_TICKER\")\n",
    "        if ticker and (ticker not in M5_TICKERS): miss.append(\"TICKER_NOT_IN_M5_LIST\")\n",
    "        if direction == \"UNKNOWN\": miss.append(\"UNKNOWN_DIRECTION\")\n",
    "\n",
    "        if miss:\n",
    "            reasons.append({\n",
    "                \"TradeID\": tid, \"Currency\": currency, \"TradeBase\": trade_base, \"TickerNorm\": ticker,\n",
    "                \"DirectionNorm\": direction, \"MissingReasons\": \";\".join(miss)\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"TradeID\": tid,\n",
    "            \"Currency\": currency,\n",
    "            \"TradeBase\": trade_base,\n",
    "            \"TickerNorm\": ticker,\n",
    "            \"Direction\": direction,\n",
    "            \"EntryDate\": entry_date,\n",
    "            \"ExitDate_Original\": exit_date,\n",
    "            \"EntryPrice\": entry_px,\n",
    "            \"ExitPrice_Original\": exit_px,\n",
    "            \"HasAllCoreFields\": (len(miss) == 0),\n",
    "            \"MissingReasons\": \";\".join(miss) if miss else \"\"\n",
    "        })\n",
    "\n",
    "    trade_summary = pd.DataFrame(rows)\n",
    "    missing_df = pd.DataFrame(reasons)\n",
    "\n",
    "    coverage = pd.DataFrame([{\n",
    "        \"Raw_Unique_TradeIDs\": int(df[\"TradeID\"].nunique()),\n",
    "        \"Extracted_Unique_TradeIDs\": int(trade_summary[\"TradeID\"].nunique()),\n",
    "        \"Trades_With_All_Core_Fields\": int(trade_summary[\"HasAllCoreFields\"].sum()),\n",
    "        \"Trades_Missing_Something\": int((~trade_summary[\"HasAllCoreFields\"]).sum()),\n",
    "        \"Trades_Missing_M5\": int(trade_summary[\"MissingReasons\"].astype(str).str.contains(\"TICKER_NOT_IN_M5_LIST\", na=False).sum()),\n",
    "        \"Trades_Unknown_Direction\": int(trade_summary[\"MissingReasons\"].astype(str).str.contains(\"UNKNOWN_DIRECTION\", na=False).sum()),\n",
    "        \"Unique_M5_Tickers_Available\": int(len(M5_TICKERS)),\n",
    "        \"Unique_M5_Tickers_Used\": int(trade_summary[\"TickerNorm\"].nunique())\n",
    "    }])\n",
    "\n",
    "    return trade_summary, missing_df, coverage\n",
    "\n",
    "# ============================================================\n",
    "# TRAILING LOGIC (CAPPED)\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class TrailResult:\n",
    "    exit_time: pd.Timestamp\n",
    "    exit_price: float\n",
    "    exit_reason: str\n",
    "    best_favorable: float\n",
    "    max_high: float\n",
    "    min_low: float\n",
    "\n",
    "def direction_aware_improvement(direction: str, new_exit: float, old_exit: float) -> float:\n",
    "    \"\"\"\n",
    "    Returns positive if trailing is better.\n",
    "    LONG: new - old\n",
    "    SHORT: old - new\n",
    "    \"\"\"\n",
    "    d = normalize_direction(direction)\n",
    "    if pd.isna(new_exit) or pd.isna(old_exit):\n",
    "        return np.nan\n",
    "    return float(new_exit - old_exit) if d == \"LONG\" else float(old_exit - new_exit)\n",
    "\n",
    "def prepare_m5_with_indicators_market_only(ticker: str) -> tuple[pd.DataFrame | None, dict]:\n",
    "    \"\"\"\n",
    "    Loads one ticker M5 file, filters market hours only, and computes:\n",
    "      - ATR\n",
    "      - slope (rolling)\n",
    "      - slope_norm = slope / ATR\n",
    "\n",
    "    Returns (df, stats_dict)\n",
    "    \"\"\"\n",
    "    fp = M5_MAP.get(ticker)\n",
    "    if not fp or not os.path.exists(fp):\n",
    "        return None, {\"ticker\": ticker, \"status\": \"missing_file\"}\n",
    "\n",
    "    df = pd.read_csv(fp)\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    needed = [\"date\", \"open\", \"high\", \"low\", \"close\"]\n",
    "    if not all(c in df.columns for c in needed):\n",
    "        return None, {\"ticker\": ticker, \"status\": \"bad_columns\"}\n",
    "\n",
    "    df[\"_dt\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"_dt\"]).sort_values(\"_dt\").reset_index(drop=True)\n",
    "\n",
    "    for c in [\"open\", \"high\", \"low\", \"close\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"open\", \"high\", \"low\", \"close\"])\n",
    "\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Market hours only (remove pre/post market)\n",
    "    df = filter_market_hours(df)\n",
    "    kept_rows = len(df)\n",
    "\n",
    "    if kept_rows == 0:\n",
    "        return None, {\"ticker\": ticker, \"status\": \"no_rows_in_market_hours\", \"total_rows\": total_rows, \"kept_rows\": 0}\n",
    "\n",
    "    # Indicators on market-only data\n",
    "    df[\"ATR\"] = compute_atr(df, ATR_WINDOW)\n",
    "\n",
    "    if USE_TIME_AWARE_SLOPE:\n",
    "        df[\"_tmin\"] = (df[\"_dt\"] - df[\"_dt\"].iloc[0]).dt.total_seconds() / 60.0\n",
    "        df[\"slope\"] = df[\"close\"].rolling(SLOPE_WINDOW, min_periods=SLOPE_WINDOW).apply(\n",
    "            lambda s: rolling_slope(s, tmin=df.loc[s.index, \"_tmin\"], method=SLOPE_METHOD),\n",
    "            raw=False\n",
    "        )\n",
    "    else:\n",
    "        df[\"slope\"] = df[\"close\"].rolling(SLOPE_WINDOW, min_periods=SLOPE_WINDOW).apply(\n",
    "            lambda s: rolling_slope(s, tmin=None, method=SLOPE_METHOD),\n",
    "            raw=False\n",
    "        )\n",
    "\n",
    "    df[\"slope_norm\"] = df[\"slope\"] / df[\"ATR\"]\n",
    "\n",
    "    info = {\n",
    "        \"ticker\": ticker,\n",
    "        \"status\": \"ok\",\n",
    "        \"total_rows\": total_rows,\n",
    "        \"kept_rows\": kept_rows,\n",
    "        \"kept_pct\": (kept_rows / total_rows) if total_rows else np.nan\n",
    "    }\n",
    "    return df, info\n",
    "\n",
    "def simulate_trailing_capped(tr: pd.Series, m5: pd.DataFrame) -> TrailResult:\n",
    "    \"\"\"\n",
    "    Simulates trailing stop from (EntryDate+1 day) to ExitDate_Original (inclusive day).\n",
    "    If stop triggers: exit at stop price at that bar timestamp.\n",
    "    Else: exit at original scheduled exit price.\n",
    "\n",
    "    Stop logic:\n",
    "      - Start with trailing distance = ATR_MULT * ATR\n",
    "      - Track best favorable excursion:\n",
    "          LONG  -> running max of high\n",
    "          SHORT -> running min of low\n",
    "      - Accelerate (tighten) when slope_norm > SLOPE_UPPER by reducing ATR_MULT\n",
    "      - If slope_norm < SLOPE_LOWER, keep stop at baseline ATR_MULT\n",
    "    \"\"\"\n",
    "    entry_date = pd.to_datetime(tr[\"EntryDate\"])\n",
    "    exit_date = pd.to_datetime(tr[\"ExitDate_Original\"])\n",
    "    entry_price = float(tr[\"EntryPrice\"])\n",
    "    orig_exit_price = float(tr[\"ExitPrice_Original\"])\n",
    "\n",
    "    direction = normalize_direction(tr[\"Direction\"])\n",
    "    is_long = (direction == \"LONG\")\n",
    "\n",
    "    start_dt = (entry_date + timedelta(days=1)) if APPLY_FROM_NEXT_DAY else entry_date\n",
    "    end_dt = exit_date + timedelta(days=1)  # include exit date's session\n",
    "\n",
    "    window = m5[(m5[\"_dt\"] >= start_dt) & (m5[\"_dt\"] < end_dt)].copy()\n",
    "    if len(window) == 0:\n",
    "        return TrailResult(exit_date, orig_exit_price, \"NoM5DataInMarketHours\", entry_price, np.nan, np.nan)\n",
    "\n",
    "    max_high = float(window[\"high\"].max())\n",
    "    min_low = float(window[\"low\"].min())\n",
    "\n",
    "    best_fav = entry_price\n",
    "    stop = -np.inf if is_long else np.inf\n",
    "\n",
    "    for _, row in window.iterrows():\n",
    "        atr = row[\"ATR\"]\n",
    "        if pd.isna(atr) or atr <= 0:\n",
    "            continue\n",
    "\n",
    "        # Update favorable excursion\n",
    "        if is_long:\n",
    "            best_fav = max(best_fav, row[\"high\"])\n",
    "        else:\n",
    "            best_fav = min(best_fav, row[\"low\"])\n",
    "\n",
    "        slope_norm = row[\"slope_norm\"]\n",
    "        mult = ATR_MULT\n",
    "\n",
    "        # Acceleration / tightening logic\n",
    "        if not pd.isna(slope_norm) and slope_norm > SLOPE_UPPER:\n",
    "            mult = max(MIN_MULT, ATR_MULT - slope_norm * ACCEL_FACTOR)\n",
    "        elif not pd.isna(slope_norm) and slope_norm < SLOPE_LOWER:\n",
    "            mult = ATR_MULT\n",
    "\n",
    "        # Compute / update stop and check hit\n",
    "        if is_long:\n",
    "            stop = max(stop, best_fav - mult * atr)\n",
    "            if row[\"low\"] <= stop:\n",
    "                return TrailResult(row[\"_dt\"], float(stop), \"TrailingSL\", float(best_fav), max_high, min_low)\n",
    "        else:\n",
    "            stop = min(stop, best_fav + mult * atr)\n",
    "            if row[\"high\"] >= stop:\n",
    "                return TrailResult(row[\"_dt\"], float(stop), \"TrailingSL\", float(best_fav), max_high, min_low)\n",
    "\n",
    "    # If never triggered, exit at scheduled exit\n",
    "    return TrailResult(exit_date, orig_exit_price, \"OriginalExit\", float(best_fav), max_high, min_low)\n",
    "\n",
    "def summarize(out: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes:\n",
    "      - trigger rate\n",
    "      - improved vs worse vs unchanged\n",
    "      - sum/avg/median improvement\n",
    "    \"\"\"\n",
    "    df = out.copy()\n",
    "    df[\"TriggeredFlag\"] = df[\"ExitReason\"].astype(str).eq(\"TrailingSL\")\n",
    "    df[\"ImprovedFlag\"] = df[\"PriceImprovement\"] > 0\n",
    "    df[\"WorseFlag\"] = df[\"PriceImprovement\"] < 0\n",
    "    df[\"UnchangedFlag\"] = (df[\"PriceImprovement\"] == 0) | df[\"PriceImprovement\"].isna()\n",
    "\n",
    "    def agg(g: pd.DataFrame) -> dict:\n",
    "        n = len(g)\n",
    "        trig = int(g[\"TriggeredFlag\"].sum())\n",
    "        return {\n",
    "            \"Trades\": n,\n",
    "            \"TrailingTriggered\": trig,\n",
    "            \"TrailingNotTriggered\": int((~g[\"TriggeredFlag\"]).sum()),\n",
    "            \"PctTriggered\": trig / n if n else np.nan,\n",
    "            \"Improved\": int(g[\"ImprovedFlag\"].sum()),\n",
    "            \"Worse\": int(g[\"WorseFlag\"].sum()),\n",
    "            \"Unchanged\": int(g[\"UnchangedFlag\"].sum()),\n",
    "            \"SumPriceImprovement\": float(g[\"PriceImprovement\"].sum(skipna=True)),\n",
    "            \"AvgPriceImprovement\": float(g[\"PriceImprovement\"].mean(skipna=True)),\n",
    "            \"MedianPriceImprovement\": float(g[\"PriceImprovement\"].median(skipna=True)),\n",
    "            \"AvgPctImprovement\": float(g[\"PctImprovement_vs_OriginalExit\"].mean(skipna=True)),\n",
    "            \"MedianPctImprovement\": float(g[\"PctImprovement_vs_OriginalExit\"].median(skipna=True)),\n",
    "        }\n",
    "\n",
    "    rows = [{\"Group\": \"ALL\", **agg(df)}]\n",
    "    for d, g in df.groupby(df[\"Direction\"].astype(str).str.upper().str.strip()):\n",
    "        rows.append({\"Group\": f\"Direction={d}\", **agg(g)})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"Running: CAPPED ONLY + MARKET HOURS ONLY\")\n",
    "    print(\"Market session:\", MARKET_START, \"to\", MARKET_END)\n",
    "    print(\"Output:\", OUT_DIR)\n",
    "\n",
    "    raw = pd.read_excel(TRADE_XLSX_PATH)\n",
    "    trade_summary, missing_df, coverage_df = build_trade_summary(raw)\n",
    "\n",
    "    # Save audits / coverage\n",
    "    trade_summary.to_csv(os.path.join(OUT_DIR, \"trade_summary_extracted.csv\"), index=False)\n",
    "    missing_df.to_csv(os.path.join(OUT_DIR, \"trade_summary_missing_reasons.csv\"), index=False)\n",
    "    coverage_df.to_csv(os.path.join(OUT_DIR, \"coverage_report.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nCoverage:\")\n",
    "    print(coverage_df.to_string(index=False))\n",
    "\n",
    "    # Trade selection\n",
    "    if RUN_ALL_TRADES:\n",
    "        sample = trade_summary.copy()\n",
    "    else:\n",
    "        sample = trade_summary.sample(n=min(N_SAMPLE_TRADES, len(trade_summary)), random_state=RANDOM_SEED).copy()\n",
    "\n",
    "    # Use only fully valid trades\n",
    "    sample = sample[sample[\"HasAllCoreFields\"]].reset_index(drop=True)\n",
    "    print(\"\\nTrades to evaluate:\", len(sample))\n",
    "\n",
    "    # Speed: group by ticker -> load M5 once per ticker\n",
    "    out_rows = []\n",
    "    m5_stats = []\n",
    "    total = len(sample)\n",
    "\n",
    "    for tkr, grp in sample.groupby(\"TickerNorm\"):\n",
    "        if tkr not in M5_TICKERS:\n",
    "            continue\n",
    "\n",
    "        m5, info = prepare_m5_with_indicators_market_only(tkr)\n",
    "        m5_stats.append(info)\n",
    "\n",
    "        if m5 is None:\n",
    "            continue\n",
    "\n",
    "        grp = grp.sort_values(\"EntryDate\")\n",
    "        for _, tr in grp.iterrows():\n",
    "            res = simulate_trailing_capped(tr, m5)\n",
    "\n",
    "            old_exit = float(tr[\"ExitPrice_Original\"])\n",
    "            new_exit = float(res.exit_price)\n",
    "            impr = direction_aware_improvement(tr[\"Direction\"], new_exit, old_exit)\n",
    "\n",
    "            orig_exit_date = pd.to_datetime(tr[\"ExitDate_Original\"])\n",
    "            new_exit_time = pd.to_datetime(res.exit_time)\n",
    "            new_exit_date = new_exit_time.normalize() if pd.notna(new_exit_time) else pd.NaT\n",
    "\n",
    "            out_rows.append({\n",
    "                \"TradeID\": tr[\"TradeID\"],\n",
    "                \"Currency\": tr[\"Currency\"],\n",
    "                \"TradeBase\": tr[\"TradeBase\"],\n",
    "                \"TickerNorm\": tr[\"TickerNorm\"],\n",
    "                \"Direction\": tr[\"Direction\"],\n",
    "\n",
    "                \"EntryDate\": tr[\"EntryDate\"],\n",
    "                \"EntryPrice\": float(tr[\"EntryPrice\"]),\n",
    "\n",
    "                \"ExitDate_Original\": orig_exit_date,\n",
    "                \"ExitPrice_Original\": old_exit,\n",
    "\n",
    "                \"ExitTime_Trailing\": new_exit_time,\n",
    "                \"ExitDate_Trailing\": new_exit_date,\n",
    "                \"ExitPrice_Trailing\": new_exit,\n",
    "                \"ExitReason\": res.exit_reason,\n",
    "\n",
    "                \"MaxHigh_InWindow\": res.max_high,\n",
    "                \"MinLow_InWindow\": res.min_low,\n",
    "                \"BestFavorable_InWindow\": res.best_favorable,\n",
    "\n",
    "                \"PriceImprovement\": impr,\n",
    "                \"PctImprovement_vs_OriginalExit\": (impr / old_exit) if old_exit != 0 else np.nan,\n",
    "\n",
    "                \"ClosedEarlier\": (new_exit_date < orig_exit_date) if (pd.notna(new_exit_date) and pd.notna(orig_exit_date)) else np.nan\n",
    "            })\n",
    "\n",
    "        print(f\"Processed ticker {tkr} | trades: {len(grp)} | total done: {len(out_rows)}/{total}\")\n",
    "\n",
    "    out = pd.DataFrame(out_rows)\n",
    "    summ = summarize(out)\n",
    "\n",
    "    # Save outputs\n",
    "    out_path = os.path.join(OUT_DIR, \"trades_trailing_capped_market_only.csv\")\n",
    "    summ_path = os.path.join(OUT_DIR, \"summary_trailing_capped_market_only.csv\")\n",
    "    m5_stats_path = os.path.join(OUT_DIR, \"m5_market_hours_filter_stats.csv\")\n",
    "\n",
    "    out.to_csv(out_path, index=False)\n",
    "    summ.to_csv(summ_path, index=False)\n",
    "    pd.DataFrame(m5_stats).to_csv(m5_stats_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved:\")\n",
    "    print(out_path)\n",
    "    print(summ_path)\n",
    "    print(m5_stats_path)\n",
    "\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(summ.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3d5f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total_Trades</th>\n",
       "      <td>11578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Improved_Trades</th>\n",
       "      <td>5212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Worse_Trades</th>\n",
       "      <td>5386.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pct_Improved</th>\n",
       "      <td>0.450164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pct_Worse</th>\n",
       "      <td>0.465193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Pct_Impact_All_Trades</th>\n",
       "      <td>0.000850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Pct_Gain_Improved_Trades</th>\n",
       "      <td>0.012044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Pct_Loss_Worse_Trades</th>\n",
       "      <td>-0.009827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median_Pct_Impact_All_Trades</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payoff_Ratio_(AvgGain/AvgLoss)</th>\n",
       "      <td>1.225528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0\n",
       "Total_Trades                    11578.000000\n",
       "Improved_Trades                  5212.000000\n",
       "Worse_Trades                     5386.000000\n",
       "Pct_Improved                        0.450164\n",
       "Pct_Worse                           0.465193\n",
       "Avg_Pct_Impact_All_Trades           0.000850\n",
       "Avg_Pct_Gain_Improved_Trades        0.012044\n",
       "Avg_Pct_Loss_Worse_Trades          -0.009827\n",
       "Median_Pct_Impact_All_Trades        0.000000\n",
       "Payoff_Ratio_(AvgGain/AvgLoss)      1.225528"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FILE = r\"D:/work/Client/Maatra/Trade Level Data/TrailingSL_CAPPED_ONLY_MKT1430_2100_FIXEDDIR/trades_trailing_capped_market_only.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE)\n",
    "\n",
    "# keep valid percentage rows\n",
    "df = df[df[\"PctImprovement_vs_OriginalExit\"].notna()]\n",
    "\n",
    "improved = df[df[\"PctImprovement_vs_OriginalExit\"] > 0]\n",
    "worse    = df[df[\"PctImprovement_vs_OriginalExit\"] < 0]\n",
    "\n",
    "summary = {\n",
    "    \"Total_Trades\": len(df),\n",
    "\n",
    "    \"Improved_Trades\": len(improved),\n",
    "    \"Worse_Trades\": len(worse),\n",
    "\n",
    "    \"Pct_Improved\": len(improved) / len(df),\n",
    "    \"Pct_Worse\": len(worse) / len(df),\n",
    "\n",
    "    # headline number\n",
    "    \"Avg_Pct_Impact_All_Trades\": df[\"PctImprovement_vs_OriginalExit\"].mean(),\n",
    "\n",
    "    # payoff asymmetry\n",
    "    \"Avg_Pct_Gain_Improved_Trades\": improved[\"PctImprovement_vs_OriginalExit\"].mean(),\n",
    "    \"Avg_Pct_Loss_Worse_Trades\": worse[\"PctImprovement_vs_OriginalExit\"].mean(),\n",
    "\n",
    "    # robustness check\n",
    "    \"Median_Pct_Impact_All_Trades\": df[\"PctImprovement_vs_OriginalExit\"].median(),\n",
    "\n",
    "    \"Payoff_Ratio_(AvgGain/AvgLoss)\": (\n",
    "        improved[\"PctImprovement_vs_OriginalExit\"].mean()\n",
    "        / abs(worse[\"PctImprovement_vs_OriginalExit\"].mean())\n",
    "        if len(worse) > 0 else np.nan\n",
    "    )\n",
    "}\n",
    "\n",
    "pd.DataFrame([summary]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a69222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
